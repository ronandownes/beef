{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents (Clickable in sidebar)<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Modules-and--libraries--\" data-toc-modified-id=\"Modules-and--libraries---0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Modules and  libraries  <a id=\"libraries\" rel=\"nofollow\"></a></a></span></li><li><span><a href=\"#How-to-use\" data-toc-modified-id=\"How-to-use-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>How to use</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis-(EDA)\" data-toc-modified-id=\"Exploratory-Data-Analysis-(EDA)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Exploratory Data Analysis (EDA)</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Rain-Preparation\" data-toc-modified-id=\"Rain-Preparation-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Rain Preparation</a></span></li><li><span><a href=\"#Temp-°C--Preparation\" data-toc-modified-id=\"Temp-°C--Preparation-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Temp °C  Preparation</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Input/Output:\" data-toc-modified-id=\"Data-Input/Output:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Input/Output:</a></span></li><li><span><a href=\"#Data-Cleaning:\" data-toc-modified-id=\"Data-Cleaning:-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Cleaning:</a></span></li><li><span><a href=\"#Data-Exploration:\" data-toc-modified-id=\"Data-Exploration:-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Exploration:</a></span></li><li><span><a href=\"#Data-Transformation\" data-toc-modified-id=\"Data-Transformation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data Transformation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Split\" data-toc-modified-id=\"Split-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;</span>Split</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Combination:\" data-toc-modified-id=\"Data-Combination:-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Data Combination:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Pascal\" data-toc-modified-id=\"Pascal-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>Pascal</a></span></li><li><span><a href=\"#Total-Character\" data-toc-modified-id=\"Total-Character-6.0.2\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Total Character</a></span></li></ul></li><li><span><a href=\"#Scanners,--Readers-and-Writers\" data-toc-modified-id=\"Scanners,--Readers-and-Writers-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Scanners,  Readers and Writers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scanners\" data-toc-modified-id=\"Scanners-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Scanners</a></span></li><li><span><a href=\"#Readers\" data-toc-modified-id=\"Readers-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Readers</a></span></li><li><span><a href=\"#Splitters\" data-toc-modified-id=\"Splitters-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>Splitters</a></span></li><li><span><a href=\"#Delete\" data-toc-modified-id=\"Delete-6.1.4\"><span class=\"toc-item-num\">6.1.4&nbsp;&nbsp;</span>Delete</a></span></li><li><span><a href=\"#Rename\" data-toc-modified-id=\"Rename-6.1.5\"><span class=\"toc-item-num\">6.1.5&nbsp;&nbsp;</span>Rename</a></span></li><li><span><a href=\"#Replace\" data-toc-modified-id=\"Replace-6.1.6\"><span class=\"toc-item-num\">6.1.6&nbsp;&nbsp;</span>Replace</a></span></li></ul></li></ul></li><li><span><a href=\"#File-and-folder-operations\" data-toc-modified-id=\"File-and-folder-operations-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>File and folder operations</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#tabler\" data-toc-modified-id=\"tabler-7.0.1\"><span class=\"toc-item-num\">7.0.1&nbsp;&nbsp;</span>tabler</a></span></li></ul></li><li><span><a href=\"#Input/Output\" data-toc-modified-id=\"Input/Output-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Input/Output</a></span></li></ul></li><li><span><a href=\"#Data-Processing\" data-toc-modified-id=\"Data-Processing-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Data Processing</a></span></li><li><span><a href=\"#String-Manipulation\" data-toc-modified-id=\"String-Manipulation-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>String Manipulation</a></span></li><li><span><a href=\"#File-System-Operations\" data-toc-modified-id=\"File-System-Operations-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>File System Operations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filters\" data-toc-modified-id=\"Filters-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Filters</a></span><ul class=\"toc-item\"><li><span><a href=\"#EU-27\" data-toc-modified-id=\"EU-27-10.1.1\"><span class=\"toc-item-num\">10.1.1&nbsp;&nbsp;</span>EU 27</a></span></li><li><span><a href=\"#Now-or-Never!\" data-toc-modified-id=\"Now-or-Never!-10.1.2\"><span class=\"toc-item-num\">10.1.2&nbsp;&nbsp;</span>Now or Never!</a></span></li></ul></li></ul></li><li><span><a href=\"#Appendices\" data-toc-modified-id=\"Appendices-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Appendices</a></span><ul class=\"toc-item\"><li><span><a href=\"#FAOSTAT-Data-Domains\" data-toc-modified-id=\"FAOSTAT-Data-Domains-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>FAOSTAT Data Domains</a></span></li></ul></li><li><span><a href=\"#Climate--Data-Preparation\" data-toc-modified-id=\"Climate--Data-Preparation-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Climate  Data Preparation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#About-FAOSTAT\" data-toc-modified-id=\"About-FAOSTAT-12.0.1\"><span class=\"toc-item-num\">12.0.1&nbsp;&nbsp;</span>About FAOSTAT</a></span></li><li><span><a href=\"#Licencing\" data-toc-modified-id=\"Licencing-12.0.2\"><span class=\"toc-item-num\">12.0.2&nbsp;&nbsp;</span>Licencing</a></span></li><li><span><a href=\"#CCKP-Data--and-an-initial-bit-of-EDA\" data-toc-modified-id=\"CCKP-Data--and-an-initial-bit-of-EDA-12.0.3\"><span class=\"toc-item-num\">12.0.3&nbsp;&nbsp;</span>CCKP Data  and an initial bit of EDA</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and  comparison of Ireland's beef sector with other  EU contries\n",
    "\n",
    "![image1](../images/tu04.png)\n",
    "\n",
    "<span style=\"font-size: 24px;\">                 </span>\n",
    "\n",
    "### Modules and  libraries  <a id=\"libraries\"></a>\n",
    "\n",
    "This  code block imports various Python libraries  used for data analysis and visualization, including Pandas, Matplotlib, Seaborn, Scikit-Learn, and others. It also sets some options for data wrangling, visualization, and controlling warnings. Additionally, it imports a few specific functions and classes from some of these libraries, such as GridSearchCV from Scikit-Learn and ks_2samp from Scipy. Finally, it sets a color variable to be used in plotting. It's visibility can be toggled on and off woth the button underneath."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use\n",
    " How to use thisnotebook\n",
    "Function will need to be defined despite the fact that they are out at the end. \n",
    "Do a run all before exploring this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard library imports\n",
    "# Third-party imports\n",
    "# Local imports\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from functools import partial, reduce\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import fancyimpute\n",
    "import html\n",
    "import matplotlib.axes\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML, Image, display\n",
    "from countrygroups import EUROPEAN_UNION\n",
    "from countryinfo import CountryInfo\n",
    "from scipy.stats import ks_2samp, shapiro\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the folder '../data/raw':\n",
      "live_animale_cattle_stock_eu_1961_2021.csv        223 KB\n"
     ]
    }
   ],
   "source": [
    "scanr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain Code</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Area Code (M49)</th>\n",
       "      <th>Area</th>\n",
       "      <th>Element Code</th>\n",
       "      <th>Element</th>\n",
       "      <th>Item Code (CPC)</th>\n",
       "      <th>Item</th>\n",
       "      <th>Year Code</th>\n",
       "      <th>Year</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Value</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Flag Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>Head</td>\n",
       "      <td>2386761.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>Head</td>\n",
       "      <td>2456557.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1963</td>\n",
       "      <td>1963</td>\n",
       "      <td>Head</td>\n",
       "      <td>2437123.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1964</td>\n",
       "      <td>1964</td>\n",
       "      <td>Head</td>\n",
       "      <td>2310667.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>Head</td>\n",
       "      <td>2350269.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>752</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>Head</td>\n",
       "      <td>1448590.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>752</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>Head</td>\n",
       "      <td>1435450.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>752</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>Head</td>\n",
       "      <td>1404670.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>752</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>Head</td>\n",
       "      <td>1390960.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>752</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>Head</td>\n",
       "      <td>1389890.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1708 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Domain Code                        Domain  Area Code (M49)     Area  \\\n",
       "0            QCL  Crops and livestock products               40  Austria   \n",
       "1            QCL  Crops and livestock products               40  Austria   \n",
       "2            QCL  Crops and livestock products               40  Austria   \n",
       "3            QCL  Crops and livestock products               40  Austria   \n",
       "4            QCL  Crops and livestock products               40  Austria   \n",
       "...          ...                           ...              ...      ...   \n",
       "1703         QCL  Crops and livestock products              752   Sweden   \n",
       "1704         QCL  Crops and livestock products              752   Sweden   \n",
       "1705         QCL  Crops and livestock products              752   Sweden   \n",
       "1706         QCL  Crops and livestock products              752   Sweden   \n",
       "1707         QCL  Crops and livestock products              752   Sweden   \n",
       "\n",
       "      Element Code Element  Item Code (CPC)    Item  Year Code  Year  Unit  \\\n",
       "0             5111  Stocks             2111  Cattle       1961  1961  Head   \n",
       "1             5111  Stocks             2111  Cattle       1962  1962  Head   \n",
       "2             5111  Stocks             2111  Cattle       1963  1963  Head   \n",
       "3             5111  Stocks             2111  Cattle       1964  1964  Head   \n",
       "4             5111  Stocks             2111  Cattle       1965  1965  Head   \n",
       "...            ...     ...              ...     ...        ...   ...   ...   \n",
       "1703          5111  Stocks             2111  Cattle       2017  2017  Head   \n",
       "1704          5111  Stocks             2111  Cattle       2018  2018  Head   \n",
       "1705          5111  Stocks             2111  Cattle       2019  2019  Head   \n",
       "1706          5111  Stocks             2111  Cattle       2020  2020  Head   \n",
       "1707          5111  Stocks             2111  Cattle       2021  2021  Head   \n",
       "\n",
       "          Value Flag Flag Description  \n",
       "0     2386761.0    A  Official figure  \n",
       "1     2456557.0    A  Official figure  \n",
       "2     2437123.0    A  Official figure  \n",
       "3     2310667.0    A  Official figure  \n",
       "4     2350269.0    A  Official figure  \n",
       "...         ...  ...              ...  \n",
       "1703  1448590.0    A  Official figure  \n",
       "1704  1435450.0    A  Official figure  \n",
       "1705  1404670.0    A  Official figure  \n",
       "1706  1390960.0    A  Official figure  \n",
       "1707  1389890.0    A  Official figure  \n",
       "\n",
       "[1708 rows x 14 columns]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readr('live_animale_cattle_stock_eu_1961_2021', 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rain Preparation\n",
    "\n",
    "All 27 Rain CSVs are processed into rain.df and rain.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>Rain_mm/yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria1901</td>\n",
       "      <td>1052.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austria1902</td>\n",
       "      <td>1061.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria1903</td>\n",
       "      <td>1201.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austria1904</td>\n",
       "      <td>1146.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austria1905</td>\n",
       "      <td>1127.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           key  Rain_mm/yr\n",
       "0  Austria1901     1052.84\n",
       "1  Austria1902     1061.55\n",
       "2  Austria1903     1201.34\n",
       "3  Austria1904     1146.14\n",
       "4  Austria1905     1127.85"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the 'pr' data from the '../data' directory and create a dataframe 'rain_df'\n",
    "# with the 'Rain_mm/yr' data. \n",
    "rain_df = combine('../data', 'pr', 'Rain_mm/yr')\n",
    "\n",
    "# Display the first few rows of the 'rain_df' dataframe.\n",
    "rain_df.head()\n",
    "\n",
    "# Save the 'rain_df' dataframe to a CSV file in the '../data/processed' directory, \n",
    "# with the filename 'rain.csv', and exclude the index column from the output.\n",
    "rain_df.to_csv('../data/processed/rain.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the 'rain_df' dataframe again.\n",
    "rain_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temp °C  Preparation\n",
    "\n",
    "All 27 'pr_timeseries_annual_cru' and 27 'tas_timeseries_annual_cru' files will be processed into 'rain.df' &'rain.csv' and 'temp.df' & 'temp.csv' accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>Temp °C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria1901</td>\n",
       "      <td>5.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austria1902</td>\n",
       "      <td>5.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria1903</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austria1904</td>\n",
       "      <td>6.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austria1905</td>\n",
       "      <td>5.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           key  Temp °C\n",
       "0  Austria1901     5.41\n",
       "1  Austria1902     5.34\n",
       "2  Austria1903     5.88\n",
       "3  Austria1904     6.22\n",
       "4  Austria1905     5.79"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the 'tas' data from the '../data' directory and create a dataframe 'temp_df'\n",
    "# with the 'Temp °C' data. \n",
    "temp_df = combine('../data', 'tas', 'Temp \\u00B0C')\n",
    "\n",
    "# Display the first few rows of the 'temp_df' dataframe.\n",
    "temp_df.head()\n",
    "\n",
    "# Save the 'temp_df' dataframe to a CSV file in the '../data/processed' directory, \n",
    "# with the filename 'temp.csv', and exclude the index column from the output.\n",
    "temp_df.to_csv('../data/processed/temp.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the 'temp_df' dataframe again.\n",
    "temp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the folder '../data/processed':\n",
      "cover.csv                         2141160 2023-02-16 12:23\n",
      "main.csv                            56353 2023-02-16 03:53\n",
      "missing.csv                          3630 2023-02-16 04:55\n",
      "production.csv                    1173771 2023-02-17 15:10\n",
      "rain.csv                            65686 2023-02-17 19:35\n",
      "stocks.csv                        1438299 2023-02-17 15:10\n",
      "temp.csv                            59959 2023-02-17 19:35\n"
     ]
    }
   ],
   "source": [
    "scanp() # scans the processed folder in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 28px\">This all looks great!</h1>\n",
    "\n",
    "Merging the rain_df and temp_df dataframes with our main beef data based on the country year key will be performed later using the Pandas merge() method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input/Output:\n",
    "- readr()\n",
    "- scan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning:\n",
    "- camel()\n",
    "- pascal()\n",
    "- snake()\n",
    "- clean_df()\n",
    "- eu()\n",
    "- now2()\n",
    "- split_file_name()\n",
    "- splitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration:\n",
    "- filter_col_by_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "- eu()\n",
    "- transform\n",
    "- get_annual_aggregates()\n",
    "- get_dfs()\n",
    "- get_multi_index_df()\n",
    "- get_ranking_df()\n",
    "- pivot_table_aggregate()\n",
    "- prepare_wealth_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"  \n",
    "####  Transform \n",
    "\n",
    "The transform function reads in a CSV file, renames a column, \n",
    "and filters the resulting dataframe to only include the 'key' \n",
    "and specified column. It returns the resulting dataframe after\n",
    "renaming the specified column and filtering.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def transform(path: str, filename: str, col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads in a CSV file from the given directory, renames a column to the given column name, and\n",
    "    filters the resulting dataframe to only include the 'key' and specified column.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The relative path of the directory containing the data.\n",
    "        filename (str): The name of the CSV file to read in.\n",
    "        col_name (str): The name to assign to the specified column in the resulting dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The resulting dataframe after renaming the specified column and filtering\n",
    "        to only include the 'key' and specified column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = os.path.join(path, filename)\n",
    "        df = pd.read_csv(file_path, on_bad_lines='skip', skiprows=1)\n",
    "        df.rename(columns={'Unnamed: 0': 'Year'}, inplace=True)\n",
    "        df['key'] = df.columns[1] + df['Year'].astype(str)\n",
    "        df.rename(columns={df.columns[1]: col_name}, inplace=True)\n",
    "        df = df.filter(['key', col_name])\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading in CSV file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split\n",
    "The split function takes a pandas DataFrame df and a column name col as input, and returns a dictionary where each key corresponds to a unique value in the specified column, and each value is a DataFrame containing all rows with that unique value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "#### Split\n",
    "The split function takes a pandas DataFrame df and a column name col as input, and returns a dictionary where each key corresponds to a unique value in the specified column, and each value is a DataFrame containing all rows with that unique value.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def split(df: pd.DataFrame, col: str) -> dict:\n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to split.\n",
    "        col (str): The name of the column to group by.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where each key corresponds to a unique value in the specified column,\n",
    "        and each value is a DataFrame containing all rows with that unique value.\n",
    "    \"\"\"\n",
    "    dfs = dict(tuple(df.groupby(col)))\n",
    "    return dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now2(df):\n",
    "    \"\"\"\n",
    "    Filter to 'Year' >= 2000.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to filter.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    year_min = 2000\n",
    "    return df[df['Year'] >= year_min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Key\n",
    "def key(df, area_col='Area', year_col='Year', key_col='key'):\n",
    "    \"\"\"\n",
    "    Add a new column to a pandas DataFrame that concatenates the values in the 'area_col' and 'year_col' columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to add the new column to.\n",
    "    - area_col (str): The name of the column that contains the area values.\n",
    "    - year_col (str): The name of the column that contains the year values.\n",
    "    - key_col (str): The name of the new column to create.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    df[key_col] = df[area_col] + df[year_col].astype(str)\n",
    "    ____\n",
    "    \n",
    "    \n",
    "def add_key_column(df):\n",
    "    df['key'] = df['Area'] + df['Year'].astype(str)\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Combination:\n",
    "- merge_dfs()\n",
    "- combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "####  Combine Files\n",
    "\n",
    "The combine function is designed to read in all  the **precicipitation**\n",
    "or **mean-temperature** CSV files  from either the 'pr' or 'ts' folders,\n",
    "apply the transform function to each file to rename a column with the country \n",
    "the data relates to and filter at the superregional level. It then combines    \n",
    "all the the resulting dataframes together into a single dataframe for all the 27 EU counties.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def combine(path: str, subfolder: str, col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads in all CSV files in the given subfolder of the directory, applies the 'transform' function to each file, and\n",
    "    combines the resulting dataframes together into a single dataframe.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The relative path of the directory containing the data.\n",
    "        subfolder (str): The name of the subfolder within the directory to read the CSV files from.\n",
    "        col_name (str): The name to assign to the specified column in the resulting dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The resulting dataframe after combining data from all CSV files in the\n",
    "        specified subfolder.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        folder_path = os.path.join(path, subfolder)\n",
    "        csv_filenames = glob.glob(folder_path + \"/*.csv\")\n",
    "        processed_dfs = (transform(path, os.path.join(subfolder, os.path.basename(FileName)), col_name) for FileName in csv_filenames)\n",
    "        df = pd.concat(processed_dfs, ignore_index=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining CSV files in folder {folder_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Snake\n",
    "\n",
    "\n",
    "\n",
    "def snake(text, default='default'):\n",
    "    \"\"\"\n",
    "    Converts a given string to snake_case by replacing any whitespace characters with underscores,\n",
    "    converting to all lowercase, and removing any non-alphanumeric characters from the beginning and end.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The string to convert to snake_case.\n",
    "        default (str): The default value to return if the input text is empty.\n",
    "\n",
    "    Returns:\n",
    "        str: The resulting string in snake_case format, or the default value if the input text is empty.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return default\n",
    "    # Convert to string and replace any non-alphanumeric characters at the beginning and end with an empty string\n",
    "    text = re.sub(r'^\\W+|\\W+$', '', str(text))\n",
    "    # Replace any whitespace characters with underscores\n",
    "    text = re.sub(r'\\s+', '_', text)\n",
    "    # Convert to all lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "# snake('does snake Work')\n",
    "\n",
    "# snake('                  ')\n",
    "# snake('                  ')\n",
    "# snake('                  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pascal\n",
    "The 'pascal' naming function takes in a string as input and converts it to a PascalCase format. PascalCase is a naming convention where the first letter of each word is capitalized, and there are no spaces or separators between the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pascal(string):\n",
    "    \"\"\"\n",
    "    Convert a space- or snake-separated string to PascalCase.\n",
    "\n",
    "    Parameters:\n",
    "        string (str): The input string to convert to PascalCase.\n",
    "\n",
    "    Returns:\n",
    "        str: The input string in PascalCase format.\n",
    "\n",
    "    \"\"\"\n",
    "    # Replace any underscores with spaces\n",
    "    string = string.replace(\"_\", \" \")\n",
    "    # Capitalize the first letter of each word\n",
    "    words = string.title()\n",
    "    # Remove any remaining spaces\n",
    "    words = words.replace(\" \", \"\")\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#### Camel\n",
    "\n",
    "Camel case is a naming convention in which each word in a compound word is capitalized, except for the first word which is in lower case. It is commonly used in programming languages for naming variables and functions.\n",
    "\n",
    "handles both snake_case and space-separated strings\n",
    "\"\"\"\n",
    "def camel(string):\n",
    "    \"\"\"\n",
    "    Convert a space-separated or snake_case string to camelCase.\n",
    "\n",
    "    Parameters:\n",
    "        string (str): The string to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: The converted string in camelCase.\n",
    "    \"\"\"\n",
    "    # Replace underscores with spaces and split the string into a list of words\n",
    "    words = string.replace(\"_\", \" \").split()\n",
    "    # Convert the first word to lowercase and capitalize all subsequent words\n",
    "    camel_cased = [words[0].lower()] + [word.capitalize() for word in words[1:]]\n",
    "    # Concatenate the words together and return the resulting string\n",
    "    return \"    \" + ''.join(camel_cased) + \"    \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#### Title \n",
    "The 'title' naming convention is where each word starts\n",
    "with a capital letter, except for prepositions and conjunctions, which start with a lowercase letter. \n",
    "The function takes a string as input and converts it to title case, \n",
    "where the first letter of each non-conjunction/preposition word is capitalized, and all other letters are lowercase.\n",
    "It achieves this by splitting the input string into a list of words, identifying which words are prepositions or conjunctions based on a predefined list, and then capitalizing the first letter of all other words while converting prepositions and conjunctions to lowercase. The resulting list of processed words is then joined back into a single string with proper spacing and returned.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def title(sentence):\n",
    "    \"\"\"\n",
    "    Takes a string and converts it to title case, where the first letter of each\n",
    "    non-conjunction/preposition word is capitalized, and all other letters are lowercase.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): The string to convert to title case.\n",
    "        \n",
    "    Returns:\n",
    "        str: The input string converted to title case.\n",
    "    \"\"\"\n",
    "    # Define a list of common prepositions and conjunctions\n",
    "    prepositions_conjunctions = ['a', 'this', 'an', 'the', 'and', 'but', 'or', 'for', 'has', 'nor', 'on', 'at', 'to', 'from', 'by', 'over', 'under', 'in', 'out', 'of']\n",
    "    # Split the input string into a list of words\n",
    "    words = sentence.split()\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        # If the word is not a preposition or conjunction, capitalize the first letter and lowercase the rest\n",
    "        if word.lower() not in prepositions_conjunctions:\n",
    "            processed_words.append(word.capitalize())\n",
    "        # If the word is a preposition or conjunction, convert to lowercase\n",
    "        else:\n",
    "            processed_words.append(word.lower())\n",
    "    # Join the list of processed words into a single string, with proper spacing\n",
    "    output = \" \".join(processed_words)\n",
    "    # Remove any leading/trailing whitespace and add some padding\n",
    "    return \"     \" + re.sub('\\s+', ' ', output.strip()) + \"     \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Word count\n",
    "def word_count(text):\n",
    "    \"\"\"\n",
    "    Counts the number of words in a given text.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The text to be counted.\n",
    "        \n",
    "    Returns:\n",
    "        int: The number of words in the text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_count(string):\n",
    "    \"\"\"\n",
    "    Count the number of characters in a string.\n",
    "\n",
    "    Parameters:\n",
    "    - string (str): The string to count characters in.\n",
    "\n",
    "    Returns:\n",
    "    - int: The number of characters in the string.\n",
    "    \"\"\"\n",
    "    return len(string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scanners,  Readers and Writers\n",
    "#### Scanners\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scan(folder_path=\"../\"):\n",
    "    \"\"\"Scans the specified folder and prints contents information.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): The path to the folder to scan. Default is the root directory.\n",
    "\n",
    "    Returns:\n",
    "        Prints out contents information of the specified folder.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found: {}\".format(folder_path))\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path: {}\".format(folder_path))\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_path))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path) // 1024  # Convert to KB\n",
    "            print(\"{:30} {:10,} KB\".format(item, size))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))\n",
    "\n",
    "# scan('../../images')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scann():\n",
    "    \"\"\"Scans the raw data folder and prints contents information.\n",
    "\n",
    "    Parameters:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        Prints out contents information of the raw data folder.\n",
    "    \"\"\"\n",
    "    folder_path = \"../notebooks\"\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found\")\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path\")\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_path))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            print(\"{:30} {:10} {}\".format(item, size, modified_time))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanr():\n",
    "    \"\"\"Scans the raw data folder and prints contents information.\n",
    "\n",
    "    Parameters:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        Prints out contents information of the raw data folder.\n",
    "    \"\"\"\n",
    "    folder_name = '../data/raw'\n",
    "    current_dir = os.getcwd()\n",
    "    folder_path = os.path.join(current_dir, folder_name)\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found: {}\".format(folder_name))\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path: {}\".format(folder_path))\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_name))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path) // 1024  # Convert to KB\n",
    "            print(\"{:30} {:10,} KB\".format(item, size))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))\n",
    "\n",
    "#scanr()\n",
    "#scanr()\n",
    "#scanr()\n",
    "#scanr()\n",
    "#scanr()\n",
    "#scanr()\n",
    "#scanr()\n",
    "#scanr()\n",
    "#scanr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the folder '../data/raw':\n",
      "live_animale_cattle_stock_eu_1961_2021.csv        223 KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scanr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanp():\n",
    "    \"\"\"Scans the raw data folder and prints contents information.\n",
    "\n",
    "    Parameters:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        Prints out contents information of the raw data folder.\n",
    "    \"\"\" \n",
    "    folder_name = '../data/processed'\n",
    "    current_dir = os.getcwd()\n",
    "    folder_path = os.path.join(current_dir, folder_name)\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found\")\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path\")\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_name))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            print(\"{:30} {:10} {}\".format(item, size, modified_time))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))\n",
    "# scanr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Readers\n",
    "\n",
    "'read_csv' is a function in the Pandas library for reading in CSV (Comma Separated Values) files into a DataFrame. It is a flexible function that can handle a variety of input formats, including different delimiters, encodings, and line endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(filename, df_name, folder_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the specified folder and returns a pandas DataFrame\n",
    "    with the specified name.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The name of the CSV file to be read.\n",
    "        df_name (str): The name to be assigned to the resulting DataFrame.\n",
    "        folder_path (str): The path of the folder containing the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame created from the data in the CSV file with\n",
    "        the specified name.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(folder_path, f'{filename}.csv')\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.name = df_name\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readr(filename, df_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from  the raw data folder and returns a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        filename (str): base name of CSV file to be read.\n",
    "        df_name (str): The DataFrame name to be assigned \n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame created from the data in the CSV file with\n",
    "        the specified name.\n",
    "    \"\"\"\n",
    "    file_path = '../data/raw/' + filename + '.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.name = df_name\n",
    "    return df\n",
    "# readr('land_use','land_use_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readp(filename, df_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the specified file path in the processed folder and returns a pandas DataFrame\n",
    "    with the specified name.\n",
    "    \n",
    "    Parameters:\n",
    "        filename (str): The name of the CSV file to be read.\n",
    "        df_name (str): The name to be assigned to the resulting DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame created from the data in the CSV file with\n",
    "        the specified name.\n",
    "    \"\"\"\n",
    "    file_path = f'../data/processed/{filename}.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.name = df_name\n",
    "    return df\n",
    "# df=readp('missing','missing_df')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(file_name: str) -> Tuple[Dict[str, pd.DataFrame], List[str]]:\n",
    "    \"\"\"\n",
    "    Splits a CSV file into multiple dataframes based on the unique values in the 'Element' column.\n",
    "    \n",
    "    Parameters:\n",
    "        file_name (str): The name of the CSV file to read in.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[Dict[str, pd.DataFrame], List[str]]: A tuple containing a dictionary where each key corresponds\n",
    "        to a unique value in the 'Element' column, and each value is a dataframe containing all rows with that\n",
    "        unique value, and a list of unique values in the 'Element' column.\n",
    "    \"\"\"\n",
    "    if not file_name.endswith('.csv'):\n",
    "        raise ValueError('Input file must be a .csv file')\n",
    "    \n",
    "    # Load the specified dataframe\n",
    "    df = pd.read_csv(f'../data/raw/{file_name}')\n",
    "    \n",
    "    # Group the dataframe by the 'Element' column\n",
    "    dfs = dict(tuple(df.groupby('Element')))\n",
    "    \n",
    "    # Get the unique values in the 'Element' column\n",
    "    elements = list(df['Element'].unique())\n",
    "    \n",
    "    return dfs, elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_elementr(file_name: str) -> Tuple[Dict[str, pd.DataFrame], List[str]]:\n",
    "    \"\"\"\n",
    "    Splits a CSV file into multiple dataframes based on the unique values in the 'Element' column.\n",
    "    \n",
    "    Parameters:\n",
    "        file_name (str): The name of the CSV file to read in.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[Dict[str, pd.DataFrame], List[str]]: A tuple containing a dictionary where each key corresponds\n",
    "        to a unique value in the 'Element' column, and each value is a dataframe containing all rows with that\n",
    "        unique value, and a list of unique values in the 'Element' column.\n",
    "    \"\"\"\n",
    "    # Load the specified dataframe\n",
    "    df = pd.read_csv(f'../data/raw/{file_name}')\n",
    "    \n",
    "    # Group the dataframe by the 'Element' column\n",
    "    dfs = dict(tuple(df.groupby('Element')))\n",
    "    \n",
    "    # Get the unique values in the 'Element' column\n",
    "    elements = list(df['Element'].unique())\n",
    "    \n",
    "    return dfs, elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_element(df: pd.DataFrame) -> Tuple[Dict[str, pd.DataFrame], List[str]]:\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into multiple dataframes based on the unique values in the 'Element' column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to split.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[Dict[str, pd.DataFrame], List[str]]: A tuple containing a dictionary where each key corresponds\n",
    "        to a unique value in the 'Element' column, and each value is a dataframe containing all rows with that\n",
    "        unique value, and a list of unique values in the 'Element' column.\n",
    "    \"\"\"\n",
    "    # Group the dataframe by the 'Element' column\n",
    "    dfs = dict(tuple(df.groupby('Element')))\n",
    "    \n",
    "    # Get the unique values in the 'Element' column\n",
    "    elements = list(df['Element'].unique())\n",
    "    \n",
    "    return dfs, elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupdf(df, column_name):\n",
    "    # Group the dataframe by the specified column\n",
    "    dfs = dict(tuple(df.groupby(column_name)))\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Delete\n",
    "\n",
    "In this code, we use the os module to delete a file at the specified file path. We first check if the file exists using the os.path.exists() function. If the file exists, we delete it using the os.remove() function. If the file does not exist, we print a message indicating that the file was not found.\n",
    "\n",
    "Note that this code permanently deletes the file, so you should use it with caution. Once a file is deleted, it cannot be easily recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def delete(file_name):\n",
    "    file_path = os.path.join(\"..\", \"data\", \"raw\", file_name)\n",
    "    expected_ext = \".csv\"\n",
    "    if not file_path.endswith(expected_ext):\n",
    "        print(\"Error: Invalid file extension. File extension must be .csv.\")\n",
    "    elif os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"{file_name} deleted successfully\")\n",
    "    else:\n",
    "        print(f\"{file_name} not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename\n",
    "\n",
    "This function first constructs the file_path by joining the ../data/raw directory with the given file_name. It then sets the expected file extension to .csv. The function then checks if the file_path ends with the expected file extension. If it doesn't, the function prints an error message. If it does, the function checks if the file exists at file_path, and if it does, it removes it and prints a success message. If the file doesn't exist, the function prints a \"not found\" message. like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(old_filename, new_filename):\n",
    "    old_file_path = os.path.join(\"..\", \"data\", old_filename)\n",
    "    new_file_path = os.path.join(\"..\", \"data\", new_filename)\n",
    "    try:\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "        print(f\"{old_filename} renamed to {new_filename} successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{old_filename} not found\")\n",
    "    except FileExistsError:\n",
    "        print(f\"A file with the name {new_filename} already exists\")\n",
    "    except OSError:\n",
    "        print(\"Invalid file path or name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(input_str, find_str, replace_str):\n",
    "    output_str = re.sub(find_str, replace_str, input_str)\n",
    "    \n",
    "    if output_str == input_str:\n",
    "        warnings.warn(\"Replacement unsuccessful: '{}' not found in input string.\".format(find_str))\n",
    "    \n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File and folder operations\n",
    "- delete(file_name)\n",
    "- split(df, col)\n",
    "- snake(text)\n",
    "- scan(folder_path=\"../\")\n",
    "\n",
    "These functions are responsible for reading from and writing to files on the file system.\n",
    "readr(): Reads a CSV file and returns a Pandas DataFrame.\n",
    "delete(): Deletes a file from the file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  tabler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  tabler\n",
    "\n",
    "\"\"\"The code defines a function tabler that generates an HTML table with information on the files in a given folder and\n",
    "adds a section header that contains the directory path \n",
    "\"\"\"\n",
    "\n",
    "def tabler(folder_path):\n",
    "    \"\"\"\n",
    "    Generate an HTML table with information on files in a given folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path to the folder containing the files.\n",
    "\n",
    "    Returns:\n",
    "    - str: The HTML code for the table.\n",
    "    \"\"\"\n",
    "    # Get the contents of the folder\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        return \"Directory not found\"\n",
    "    except OSError:\n",
    "        return \"Invalid folder path\"\n",
    "\n",
    "    # Create the section header\n",
    "    header = f\"### {folder_path}\\n\\n\"\n",
    "\n",
    "    # Create the table header\n",
    "    table = '<table style=\"font-size:100%\"><thead><tr><th>File Name</th><th>Size</th><th>Modified Time</th></tr></thead><tbody>'\n",
    "\n",
    "    # Add a row for each file\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            # Get the file size and modified time\n",
    "            size_bytes = os.path.getsize(item_path)\n",
    "            size_kb = size_bytes / 1024\n",
    "            size_str = '{:,.2f} KB'.format(size_kb)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            # Add a row to the table\n",
    "            table += '<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(item, size_str, modified_time)\n",
    "\n",
    "    # Close the table\n",
    "    table += '</tbody></table>'\n",
    "\n",
    "    return header + table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input/Output\n",
    "\n",
    "These functions are responsible for reading from and writing to files on the file system.\n",
    "**readr()**: Reads a CSV file and returns a Pandas DataFrame.\n",
    "delete(): Deletes a file from the file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "- eu(df)\n",
    "- now2(df)\n",
    "- split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  String Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File System Operations\n",
    "\n",
    "\n",
    "These functions are responsible for performing various operations on the file system.\n",
    "scan(): Scans a specified folder and prints contents information.\n",
    "tabler(): Generates an HTML table with information on files in a given folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters\n",
    "Geographical and temporal filters are useful in exploratory data analysis (EDA) because they help to identify patterns and trends in data that may be difficult to discern otherwise. By limiting the data to a specific geographic or time period, it becomes easier to identify trends and patterns that might be obscured by noise in the larger dataset.\n",
    "\n",
    "####  EU 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eu(df):\n",
    "    \"\"\"\n",
    "    Filter a pandas DataFrame to include only the rows where the 'Area' column contains values that match the countries in the European Union.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to filter.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    country_list = ['Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czechia', 'Denmark', \n",
    "                    'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Ireland', 'Italy', \n",
    "                    'Latvia', 'Lithuania', 'Luxembourg', 'Malta', 'Netherlands', 'Poland', 'Portugal', 'Romania', \n",
    "                    'Slovakia', 'Slovenia', 'Spain', 'Sweden']\n",
    "    return df[df['Area'].isin(country_list)]\n",
    "\n",
    "# df=readr('meat','df')\n",
    "# df=eu(df)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Now or Never!\n",
    "df=readr('meat','df')\n",
    "df = now(df, 2015)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now(df, year_min):\n",
    "    \"\"\"\n",
    "    Filter to 'Year'>.\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to filter.\n",
    "    - year_min (int): The minimum year to include in the filtered DataFrame.\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    return df[df['Year'] >= year_min]\n",
    "\n",
    "# df=readr('meat','df')\n",
    "# df = now(df, 2015)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAOSTAT Data Domains \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../images/we16.png\" alt=\"image7\" width=\"80%\">\n",
    "\n",
    "[<span style=\"font-size: 18px;\">Figure 2:Data Domain Table view of FAOSTAT</span>\n",
    "](https://www.fao.org/faostat/en/#data/domains_table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The [FAOSTAT Data Domains](https://www.fao.org/faostat/en/#data/domains_table)  are organised as follows:\n",
    "\n",
    "  -  Production: Production of crops and livestock products, including production indices and the value of agricultural production.\n",
    "\n",
    "  - Food Security and Nutrition: Information on SDG indicators related to food security and nutrition and food balances.\n",
    "\n",
    "  - Trade: Including detailed trade matrices, trade indices, and updates on related data.\n",
    "\n",
    "  - Prices: Producer and   consumer price indices, deflators, and exchange rates.\n",
    "\n",
    "  - Land, Inputs and Sustainability:Land use, land cover, inputs, including fertilizers and manure and pesticides.\n",
    "  \n",
    "  - Population and Employment: Annual population including those specific to agriculture and rural areas.\n",
    "\n",
    "  - Investment: Government expenditure, credit to agriculture, foreign direct investment, and country investment statistics.\n",
    "\n",
    "  - Macro-Economic Indicators: Such as capital stock.\n",
    "\n",
    "  - Food Value Chain: This domain provides information on the value shares of the food industry and primary factors.\n",
    "\n",
    "  - Climate Change: Emissions, crop residues, forests, and other indicators related to climate change.\n",
    "\n",
    "  - Forestry: Forestry production and trade, as well as forestry trade flows.\n",
    "\n",
    "  - SDG Indicators: The Sustainable Development Goals (SDGs) are a set of 17 goals established by the United Nations in 2015.\n",
    "\n",
    "  - World Census of Agriculture: This domain provides structural data from agricultural censuses taken around the world\n",
    "\n",
    "  - Discontinued archives and data series: This includes data on indicators from surveys and research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climate  Data Preparation \n",
    "\n",
    "<img src=\"../images/tu03.png\" alt=\"image5\" width=\"50%\">\n",
    "<span style=\"font-size: 24px;\">                  </span>\n",
    " Only two attributes are taken from the CCKP database: Mean_Temperature and Precipitation.\n",
    "\n",
    "[The CCKP website](https://climateknowledgeportal.worldbank.org/)\n",
    " is a  resource for information on the impacts of climate change and the actions taken to address these impacts. While this is outside the remit of this project the CCKP also provides access to global data on a historical  basis for the **Mean_Temperature** and **Precipitation** at the  country-by-country level  on both monthly and yearly aggregates. While **humidity** is also a known influential predictor variable these two should have a statistically significant imapct on our predictive modelling. Spatial data is provided as a global NetCDF file, with Climatology, Timeseries and Heatplot data is provided as a CSV file.\n",
    "\n",
    "Climate conditions, such as average temperature and rainfall (precipitation ) can greatly affect the growth and health of cattle. Precipitation and temperature predictor variables were  retrieved  but they were   aggragated by country and this necessitated  the **cckp** and  **combine**  functions for data wrangling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  About FAOSTAT\n",
    "\n",
    "<img src=\"../images/tu02.png\" alt=\"image5\" width=\"50%\">\n",
    "\n",
    "<span style=\"font-size: 24px;\">                  </span>\n",
    "FAOSTAT is a comprehensive database maintained by the Food and Agriculture Organization of the United Nations (FAO), providing timely, reliable data on agriculture, food, and nutrition for over 200 countries. Its information is used to inform decision-making, policy formulation, and research in the field, covering topics such as production, trade, and fertilizer use. FAOSTAT is a valuable resource for governments, organizations, researchers, and the public, informing policy and interventions to enhance food security and reduce poverty.\n",
    "\n",
    "#### Licencing \n",
    "All datasets from the [FAOSTAT](https://www.fao.org/faostat/en/#home) are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO [(CC BY-NC-SA 3.0 IGO)](https://creativecommons.org/licenses/by-nc-sa/3.0/igo/). Source: FAOSTAT (2023). Time Series datasets.\n",
    "<img src=\"../images/tu06.png\" alt=\"image5\" width=\"50%\">\n",
    "<span style=\"font-size: 24px;\">                  </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "####   CCKP Data  and an initial bit of EDA\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <img src=\"../images/we28.png\" alt=\"image19\" style=\"width: 30%;\">\n",
    "  <img src=\"../images/we29.png\" alt=\"image20\" style=\"width: 30%;\">\n",
    "    <img src=\"../images/we30.png\" alt=\"image20\" style=\"width: 30%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size: 18px;\">Figure 3:Time Series Data downloaded for 27 EU countries from  CCKP site </span>\n",
    "\n",
    "\n",
    "To maintain consistency with the FAO data, annual and not monthly time series  aggregates were taken from the Climatic Research Unit (CRU) dataset for **precipitation** and   **mean-temperature**. These datasets are provided by the CRU TS 4.04 dataset, a gridded climate dataset produced by the Climatic Research Unit (CRU) at the University of East Anglia in the United Kingdom. In the statistics section, range, variance, and standard deviation of monthly data may be revisited for insights.\n",
    "\n",
    "The file names and folder names of the CCKP data used in this project are tabulated below.\n",
    "\n",
    "<span style=\"font-size: 24px;\">Table : Table shows Time Series data filenames and folders  for 27 EU countries from  CCKP site </span>\n",
    "\n",
    "\n",
    "\n",
    "| Country        | Code | TasAnnual (Folder with Tempature data)                              | PrAnnual   (Folder with Precipitation Data)                        |\n",
    "|:--------------:|:----:|:--------------------------------------:|:---------------------------------:|\n",
    "| Albania        | ALB  | tas_timeseries_annual_cru_1901-2021_ALB.csv | pr_timeseries_annual_cru_1901-2021_ALB.csv |\n",
    "| Andorra        | AND  | tas_timeseries_annual_cru_1901-2021_AND.csv | pr_timeseries_annual_cru_1901-2021_AND.csv |\n",
    "| Austria        | AUT  | tas_timeseries_annual_cru_1901-2021_AUT.csv | pr_timeseries_annual_cru_1901-2021_AUT.csv |\n",
    "| Belarus        | BLR  | tas_timeseries_annual_cru_1901-2021_BLR.csv | pr_timeseries_annual_cru_1901-2021_BLR.csv |\n",
    "| Belgium        | BEL  | tas_timeseries_annual_cru_1901-2021_BEL.csv | pr_timeseries_annual_cru_1901-2021_BEL.csv |\n",
    "| Bosnia and Herzegovina | BIH  | tas_timeseries_annual_cru_1901-2021_BIH.csv | pr_timeseries_annual_cru_1901-2021_BIH.csv |\n",
    "| Bulgaria       | BGR  | tas_timeseries_annual_cru_1901-2021_BGR.csv | pr_timeseries_annual_cru_1901-2021_BGR.csv |\n",
    "| Croatia        | HRV  | tas_timeseries_annual_cru_1901-2021_HRV.csv | pr_timeseries_annual_cru_1901-2021_HRV.csv |\n",
    "| Cyprus         | CYP  | tas_timeseries_annual_cru_1901-2021_CYP.csv | pr_timeseries_annual_cru_1901-2021_CYP.csv |\n",
    "| Czech Republic | CZE  | tas_timeseries_annual_cru_1901-2021_CZE.csv | pr_timeseries_annual_cru_1901-2021_CZE.csv |\n",
    "| Denmark        | DNK  | tas_timeseries_annual_cru_1901-2021_DNK.csv | pr_timeseries_annual_cru_1901-2021_DNK.csv |\n",
    "| Estonia        | EST  | tas_timeseries_annual_cru_1901-2021_EST.csv | pr_timeseries_annual_cru_1901-2021_EST.csv |\n",
    "| Finland        | FIN  | tas_timeseries_annual_cru_1901-2021_FIN.csv | pr_timeseries_annual_cru_1901-2021_FIN.csv |\n",
    "| France         | FRA  | tas_timeseries_annual_cru_1901-2021_FRA.csv | pr_timeseries_annual_cru_1901-2021_FRA.csv |\n",
    "| Germany        | DEU  | tas_timeseries_annual_cru_1901-2021_DEU.csv | pr_timeseries_annual_cru_1901-2021_DEU.csv |\n",
    "| Gibraltar      | GIB  | tas_timeseries_annual_cru_1901-2021_GIB.csv | pr_timeseries_annual_cru_1901-2021_GIB.csv |\n",
    "| Greece | GRC | tas_timeseries_annual_cru_1901-2021_GRC.csv | pr_timeseries_annual_cru_1901-2021_GRC.csv |\n",
    "| Croatia | HRV | tas_timeseries_annual_cru_1901-2021_HRV.csv | pr_timeseries_annual_cru_1901-2021_HRV.csv |\n",
    "| Hungary | HUN | tas_timeseries_annual_cru_1901-2021_HUN.csv | pr_timeseries_annual_cru_1901-2021_HUN.csv |\n",
    "| Ireland | IRL | tas_timeseries_annual_cru_1901-2021_IRL.csv | pr_timeseries_annual_cru_1901-2021_IRL.csv |\n",
    "| Italy | ITA | tas_timeseries_annual_cru_1901-2021_ITA.csv | pr_timeseries_annual_cru_1901-2021_ITA.csv |\n",
    "| Lithuania | LTU | tas_timeseries_annual_cru_1901-2021_LTU.csv | pr_timeseries_annual_cru_1901-2021_LTU.csv |\n",
    "| Luxembourg | LUX | tas_timeseries_annual_cru_1901-2021_LUX.csv | pr_timeseries_annual_cru_1901-2021_LUX.csv |\n",
    "| Latvia | LVA | tas_timeseries_annual_cru_1901-2021_LVA.csv | pr_timeseries_annual_cru_1901-2021_LVA.csv |\n",
    "| Malta | MLT | tas_timeseries_annual_cru_1901-2021_MLT.csv | pr_timeseries_annual_cru_1901-2021_MLT.csv |\n",
    "| Netherlands | NLD | tas_timeseries_annual_cru_1901-2021_NLD.csv | pr_timeseries_annual_cru_1901-2021_NLD.csv |\n",
    "| Poland | POL | tas_timeseries_annual_cru_1901-2021_POL.csv | pr_timeseries_annual_cru_1901-2021_POL.csv |\n",
    "| Portugal | PRT | tas_timeseries_annual_cru_1901-2021_PRT.csv | pr_timeseries_annual_cru_1901-2021_PRT.csv |\n",
    "| Romania | ROU | tas_timeseries_annual_cru_1901-2021_ROU.csv | pr_timeseries_annual_cru_1901-2021_ROU.csv |\n",
    "| Slovakia | SVK | tas_timeseries_annual_cru_1901-2021_SVK.csv | pr_timeseries_annual_cru_1901-2021_SVK.csv |\n",
    "| Slovenia | SVN | tas_timeseries_annual_cru_1901-2021_SVN.csv | pr_timeseries_annual_cru_1901-2021_SVN.csv |\n",
    "| Sweden | SWE | tas_timeseries_annual_cru_1901-2021_SWE.csv | pr_timeseries_annual_cru_1901-2021_SWE.csv |\n",
    "\n",
    "\n",
    "All datasets from the CCKP are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO (CC BY-NC-SA 3.0 IGO). \n",
    "Source: CCKP (2023). Time Series datasets. Retrieved from [https://climateknowledgeportal.worldbank.org/download-data]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents (Clickable in sidebar)",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "1166.7px",
    "left": "1103.01px",
    "top": "1396.2px",
    "width": "496.991px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "890563eb1401dd7c5eac482b2070a231034cb0eabe59bf1a3eb86f9e36919f52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
