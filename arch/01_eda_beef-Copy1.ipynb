{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis Irish Beef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries and modules\n",
    "### Data Manipulation and Analysis\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fancyimpute\n",
    "import missingno as msno\n",
    "from functools import partial, reduce\n",
    "\n",
    "### Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "### Statistical Analysis\n",
    "from scipy.stats import ks_2samp, shapiro\n",
    "\n",
    "### Machine Learning\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "### Text Processing\n",
    "import html\n",
    "import re\n",
    "\n",
    "### Country Information\n",
    "from countryinfo import CountryInfo\n",
    "import pycountry\n",
    "from countrygroups import EUROPEAN_UNION\n",
    "\n",
    "### File System and OS\n",
    "import glob\n",
    "import os\n",
    "\n",
    "### Date and Time\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "### Data Presentation\n",
    "from tabulate import tabulate\n",
    "from IPython.display import HTML, Image, display\n",
    "\n",
    "### Data Types\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housekeeping   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronan\\beef\n",
      "['.git', '.ipynb_checkpoints', '01_eda_beef.ipynb', '02_ml_beef.ipynb', 'arch', 'beef.pdf', 'belux.csv', 'benelux.csv', 'clean', 'country.csv', 'css', 'data', 'ignore', 'images', 'rain', 'README.md', 'temperature', 'Untitled Folder', 'Untitled.ipynb']\n",
      "['alive.csv', 'temperature.csv', 'temperature_change.csv', 'temperature_sd.csv']\n",
      "['benelux.csv', 'country.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd()) # working directory.\n",
    "print(os.listdir('.')) #List current directory\n",
    "print(os.listdir('data')) # Our source files from FAOSTAT are in 'data' folder\n",
    "print(os.listdir('clean')) # Our source files from FAOSTAT are in 'data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1708, 14)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/alive.csv')# loads the cattle stock  CSV file to pandas DataFrame n df\n",
    "print(df.shape) # Display dataframe dimensions. (1708, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply data exploration functions to livestock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain Code</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Area Code (M49)</th>\n",
       "      <th>Area</th>\n",
       "      <th>Element Code</th>\n",
       "      <th>Element</th>\n",
       "      <th>Item Code (CPC)</th>\n",
       "      <th>Item</th>\n",
       "      <th>Year Code</th>\n",
       "      <th>Year</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Value</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Flag Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>Head</td>\n",
       "      <td>2386761.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>Head</td>\n",
       "      <td>2456557.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1963</td>\n",
       "      <td>1963</td>\n",
       "      <td>Head</td>\n",
       "      <td>2437123.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1964</td>\n",
       "      <td>1964</td>\n",
       "      <td>Head</td>\n",
       "      <td>2310667.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>Head</td>\n",
       "      <td>2350269.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Domain Code                        Domain  Area Code (M49)     Area  \\\n",
       "0         QCL  Crops and livestock products               40  Austria   \n",
       "1         QCL  Crops and livestock products               40  Austria   \n",
       "2         QCL  Crops and livestock products               40  Austria   \n",
       "3         QCL  Crops and livestock products               40  Austria   \n",
       "4         QCL  Crops and livestock products               40  Austria   \n",
       "\n",
       "   Element Code Element  Item Code (CPC)    Item  Year Code  Year  Unit  \\\n",
       "0          5111  Stocks             2111  Cattle       1961  1961  Head   \n",
       "1          5111  Stocks             2111  Cattle       1962  1962  Head   \n",
       "2          5111  Stocks             2111  Cattle       1963  1963  Head   \n",
       "3          5111  Stocks             2111  Cattle       1964  1964  Head   \n",
       "4          5111  Stocks             2111  Cattle       1965  1965  Head   \n",
       "\n",
       "       Value Flag Flag Description  \n",
       "0  2386761.0    A  Official figure  \n",
       "1  2456557.0    A  Official figure  \n",
       "2  2437123.0    A  Official figure  \n",
       "3  2310667.0    A  Official figure  \n",
       "4  2350269.0    A  Official figure  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.head()#: returns the first few rows of the DataFrame indicating many fields may be invariant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Domain Code            1\n",
       "Domain                 1\n",
       "Area Code (M49)       28\n",
       "Area                  28\n",
       "Element Code           1\n",
       "Element                1\n",
       "Item Code (CPC)        1\n",
       "Item                   1\n",
       "Year Code             61\n",
       "Year                  61\n",
       "Unit                   1\n",
       "Value               1365\n",
       "Flag                   3\n",
       "Flag Description       3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique() # We have 28  country/ regions repoorting and 61 years of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Belgium and Luxembourg formed the BELUX economic union  and when Netherlands was also acting They were called BENELUX\"\"\"\n",
    "# Get unique countries\n",
    "country = df['Area'].unique()\n",
    "\n",
    "# Convert to a DataFrame\n",
    "country_df = pd.DataFrame(countries, columns=['Country'])\n",
    "\n",
    "# Write to CSV file\n",
    "country_df.to_csv('clean/country.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain Code</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Area Code (M49)</th>\n",
       "      <th>Area</th>\n",
       "      <th>Element Code</th>\n",
       "      <th>Element</th>\n",
       "      <th>Item Code (CPC)</th>\n",
       "      <th>Item</th>\n",
       "      <th>Year Code</th>\n",
       "      <th>Year</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Value</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Flag Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>442</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1990</td>\n",
       "      <td>1990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>528</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012</td>\n",
       "      <td>Head</td>\n",
       "      <td>3879250.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>528</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>Head</td>\n",
       "      <td>3750629.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>528</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1977</td>\n",
       "      <td>1977</td>\n",
       "      <td>Head</td>\n",
       "      <td>4528000.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>QCL</td>\n",
       "      <td>Crops and livestock products</td>\n",
       "      <td>58</td>\n",
       "      <td>Belgium-Luxembourg</td>\n",
       "      <td>5111</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>2111</td>\n",
       "      <td>Cattle</td>\n",
       "      <td>1975</td>\n",
       "      <td>1975</td>\n",
       "      <td>Head</td>\n",
       "      <td>3102000.0</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Domain Code                        Domain  Area Code (M49)  \\\n",
       "1127         QCL  Crops and livestock products              442   \n",
       "1271         QCL  Crops and livestock products              528   \n",
       "1224         QCL  Crops and livestock products              528   \n",
       "1236         QCL  Crops and livestock products              528   \n",
       "136          QCL  Crops and livestock products               58   \n",
       "\n",
       "                    Area  Element Code Element  Item Code (CPC)    Item  \\\n",
       "1127          Luxembourg          5111  Stocks             2111  Cattle   \n",
       "1271         Netherlands          5111  Stocks             2111  Cattle   \n",
       "1224         Netherlands          5111  Stocks             2111  Cattle   \n",
       "1236         Netherlands          5111  Stocks             2111  Cattle   \n",
       "136   Belgium-Luxembourg          5111  Stocks             2111  Cattle   \n",
       "\n",
       "      Year Code  Year  Unit      Value Flag Flag Description  \n",
       "1127       1990  1990   NaN        NaN  NaN              NaN  \n",
       "1271       2012  2012  Head  3879250.0    A  Official figure  \n",
       "1224       1965  1965  Head  3750629.0    A  Official figure  \n",
       "1236       1977  1977  Head  4528000.0    A  Official figure  \n",
       "136        1975  1975  Head  3102000.0    A  Official figure  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"There was an economic union known as the Benelux between Belgium, the Netherlands, and Luxembourg\n",
    "so we filter df to these and explore\"\"\"\n",
    "benelux  = df[df['Area'].isin(['Belgium-Luxembourg', 'Belgium', 'Luxembourg', 'Netherlands'])]\n",
    "benelux.sample(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Netherlands reported cattle stock independently, while Belgium \\nand Luxembourg reported collectively as the Benelux region. \\nThis changed in 2000 when they started reporting individually. A simple solution is to confine \\nhistorical data to >=2000 as data from so from before that is not rrepresentative of modern farming'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape the DataFrame with pivot()\n",
    "benelux_pivot = benelux.pivot(index='Year', columns='Area', values='Value')\n",
    "\n",
    "# Rename the columns of the pivot table\n",
    "benelux_pivot.columns = ['{}_stock'.format(col.replace(' ', '_')) for col in benelux_pivot.columns]\n",
    "\n",
    "# Display the resulting pivot table\n",
    "benelux_pivot\n",
    "\n",
    "\"\"\"  Netherlands reported cattle stock independently, while Belgium \n",
    "and Luxembourg reported collectively as the Benelux region. \n",
    "This changed in 2000 when they started reporting individually. A simple solution is to confine \n",
    "historical data to >=2000 as data from so from before that is not rrepresentative of modern farming\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the 'Area', 'Year'  and 'Value' columns\n",
    "benelux = benelux[['Area','Year', 'Value']]\n",
    "# Write to CSV file\n",
    "country_df.to_csv('clean/benelux.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['benelux.csv', 'country.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(os.listdir('clean'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Area</th>\n",
       "      <th>Belgium</th>\n",
       "      <th>Belgium-Luxembourg</th>\n",
       "      <th>Luxembourg</th>\n",
       "      <th>Netherlands</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2684120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3622588.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2798130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3816942.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2847478.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3695185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2641407.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3567379.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2685510.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3750629.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2782000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3968471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2770000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4029806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2784000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4116287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2847000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3694000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2899000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3679000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2901000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3465000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2829000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3748000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2942000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4111000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3104000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4666000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3102000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4714000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3010711.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4606000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3022000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4528000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3019000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4673000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3085000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4797000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3111000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5225857.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3116000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5191497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3072000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5240687.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3115000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5410889.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3184000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5516243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3210000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5247651.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3163000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5122950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3190000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4894841.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3159000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4546000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3174000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4772000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3257000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4926000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3360000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5062000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3311000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4920000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3303000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4797000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3289000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4716000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3369000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4654000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3363000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4557000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3280000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4411000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3184000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4283000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3395000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4206000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>3041560.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205072.0</td>\n",
       "      <td>4070000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>3037760.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205193.0</td>\n",
       "      <td>4047000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>2891260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>197257.0</td>\n",
       "      <td>3858000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>2778077.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189674.0</td>\n",
       "      <td>3759000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>2738648.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>186725.0</td>\n",
       "      <td>3767000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>2698649.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>185235.0</td>\n",
       "      <td>3799000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>2669076.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>183640.0</td>\n",
       "      <td>3749000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>2649392.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>191928.0</td>\n",
       "      <td>3763000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>2605532.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195661.0</td>\n",
       "      <td>3890000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>2600453.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>196470.0</td>\n",
       "      <td>3967600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>2593000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>198892.0</td>\n",
       "      <td>3975190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>2560319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>192535.0</td>\n",
       "      <td>3885350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>2484272.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188473.0</td>\n",
       "      <td>3879250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>2454704.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>193623.0</td>\n",
       "      <td>3999220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>2477236.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>198780.0</td>\n",
       "      <td>4169000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>2503262.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201036.0</td>\n",
       "      <td>4315000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>2501349.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201416.0</td>\n",
       "      <td>4294000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>2385988.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202281.0</td>\n",
       "      <td>4030000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>2398090.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194390.0</td>\n",
       "      <td>3690000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>2373100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>192100.0</td>\n",
       "      <td>3721000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>2335440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>190690.0</td>\n",
       "      <td>3691000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>2310440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187200.0</td>\n",
       "      <td>3705000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Area    Belgium  Belgium-Luxembourg  Luxembourg  Netherlands\n",
       "Year                                                        \n",
       "1961        NaN           2684120.0         NaN    3622588.0\n",
       "1962        NaN           2798130.0         NaN    3816942.0\n",
       "1963        NaN           2847478.0         NaN    3695185.0\n",
       "1964        NaN           2641407.0         NaN    3567379.0\n",
       "1965        NaN           2685510.0         NaN    3750629.0\n",
       "1966        NaN           2782000.0         NaN    3968471.0\n",
       "1967        NaN           2770000.0         NaN    4029806.0\n",
       "1968        NaN           2784000.0         NaN    4116287.0\n",
       "1969        NaN           2847000.0         NaN    3694000.0\n",
       "1970        NaN           2899000.0         NaN    3679000.0\n",
       "1971        NaN           2901000.0         NaN    3465000.0\n",
       "1972        NaN           2829000.0         NaN    3748000.0\n",
       "1973        NaN           2942000.0         NaN    4111000.0\n",
       "1974        NaN           3104000.0         NaN    4666000.0\n",
       "1975        NaN           3102000.0         NaN    4714000.0\n",
       "1976        NaN           3010711.0         NaN    4606000.0\n",
       "1977        NaN           3022000.0         NaN    4528000.0\n",
       "1978        NaN           3019000.0         NaN    4673000.0\n",
       "1979        NaN           3085000.0         NaN    4797000.0\n",
       "1980        NaN           3111000.0         NaN    5225857.0\n",
       "1981        NaN           3116000.0         NaN    5191497.0\n",
       "1982        NaN           3072000.0         NaN    5240687.0\n",
       "1983        NaN           3115000.0         NaN    5410889.0\n",
       "1984        NaN           3184000.0         NaN    5516243.0\n",
       "1985        NaN           3210000.0         NaN    5247651.0\n",
       "1986        NaN           3163000.0         NaN    5122950.0\n",
       "1987        NaN           3190000.0         NaN    4894841.0\n",
       "1988        NaN           3159000.0         NaN    4546000.0\n",
       "1989        NaN           3174000.0         NaN    4772000.0\n",
       "1990        NaN           3257000.0         NaN    4926000.0\n",
       "1991        NaN           3360000.0         NaN    5062000.0\n",
       "1992        NaN           3311000.0         NaN    4920000.0\n",
       "1993        NaN           3303000.0         NaN    4797000.0\n",
       "1994        NaN           3289000.0         NaN    4716000.0\n",
       "1995        NaN           3369000.0         NaN    4654000.0\n",
       "1996        NaN           3363000.0         NaN    4557000.0\n",
       "1997        NaN           3280000.0         NaN    4411000.0\n",
       "1998        NaN           3184000.0         NaN    4283000.0\n",
       "1999        NaN           3395000.0         NaN    4206000.0\n",
       "2000  3041560.0                 NaN    205072.0    4070000.0\n",
       "2001  3037760.0                 NaN    205193.0    4047000.0\n",
       "2002  2891260.0                 NaN    197257.0    3858000.0\n",
       "2003  2778077.0                 NaN    189674.0    3759000.0\n",
       "2004  2738648.0                 NaN    186725.0    3767000.0\n",
       "2005  2698649.0                 NaN    185235.0    3799000.0\n",
       "2006  2669076.0                 NaN    183640.0    3749000.0\n",
       "2007  2649392.0                 NaN    191928.0    3763000.0\n",
       "2008  2605532.0                 NaN    195661.0    3890000.0\n",
       "2009  2600453.0                 NaN    196470.0    3967600.0\n",
       "2010  2593000.0                 NaN    198892.0    3975190.0\n",
       "2011  2560319.0                 NaN    192535.0    3885350.0\n",
       "2012  2484272.0                 NaN    188473.0    3879250.0\n",
       "2013  2454704.0                 NaN    193623.0    3999220.0\n",
       "2014  2477236.0                 NaN    198780.0    4169000.0\n",
       "2015  2503262.0                 NaN    201036.0    4315000.0\n",
       "2016  2501349.0                 NaN    201416.0    4294000.0\n",
       "2017  2385988.0                 NaN    202281.0    4030000.0\n",
       "2018  2398090.0                 NaN    194390.0    3690000.0\n",
       "2019  2373100.0                 NaN    192100.0    3721000.0\n",
       "2020  2335440.0                 NaN    190690.0    3691000.0\n",
       "2021  2310440.0                 NaN    187200.0    3705000.0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Reshape the DataFrame with pivot()\n",
    "# benelux_pivot = benelux.pivot(index='Year', columns='Area', values='Value')\n",
    "# benelux_pivot \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'clean/belux.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10324\\1219728819.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the data into a DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clean/belux.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'clean/belux.csv'"
     ]
    }
   ],
   "source": [
    "# Load the data into a DataFrame\n",
    "bl = pd.read_csv('clean/belux.csv')\n",
    "print(os.listdir('clean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benelux union stopped  report beef stock in 1999\n",
    "At the end of 1999, the Benelux union ceased to report beef stock data as a single entity, as each member country began reporting its data individually. This change reflected the increasing economic development and growth of the individual countries within the union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Pivot the DataFrame\n",
    "pivoted_df = bl.pivot(index='Year', columns='Area', values='Value')\n",
    "# Rename the columns to match the desired format\n",
    "pivoted_df.columns = ['Value_BE', 'Value_BELUX', 'Value_LUX']\n",
    "pd.set_option('display.max_rows', None)  # Set max rows to None to display all rows\n",
    "pivoted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Set max rows to None to display all rows\n",
    "pivoted_df\n",
    "pivoted_df = pivoted_df.loc['1995':'2005']\n",
    "pivoted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Year column to the DataFrame\n",
    "pivoted_df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Set max rows to None to display all rows\n",
    "\n",
    "df = pd.read_csv('path/to/your/file.csv')  # Replace with your file path\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with Year and Value columns\n",
    "bl_pivot = df.pivot(index='Year', columns='Area', values='Value')\n",
    "\n",
    "# Rename the columns to match the desired format\n",
    "bl_pivot.columns = ['Value_BE', 'Value_BELUX', 'Value_LUX']\n",
    "\n",
    "# Add a Year column to the DataFrame\n",
    "bl_pivot['Year'] = new_df.index\n",
    "\n",
    "# Rearrange the columns so Year comes first\n",
    "bl_pivot = new_df[['Year', 'Value_BE', 'Value_LUX', 'Value_BELUX']]\n",
    "\n",
    "\n",
    "# Display the entire DataFrame\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "bl_pivot.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "pivoted_bl.columns = [col.replace('-', '_') + '_BL' for col in pivoted_bl.columns]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(pivoted_bl)\n",
    "\n",
    "# Rename the columns\n",
    "pivoted_df.columns = [col.replace('-', '_') for col in pivoted_df.columns]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(pivoted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl.to_csv('clean/belux.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()# returns the number of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape# returns the dimensions of the DataFrame as (1708, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()# provides a concise summary of the DataFrame, including column data types, non-null values, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()#computes various summary statistics, including count, mean, std, min, and max, for each numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts()#returns the count of unique values in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Area')# groups the DataFrame by one or more columns and applies a function to each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()# computes the correlation between each pair of numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Value'] = df['Value'].astype(int) # Recast stock to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot()# creates a scatterplot matrix of all the numeric variables in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap()# creates a heatmap to visualize the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "\"\"\"The output shows that there are missing values in the columns \n",
    "'Unit', 'Value', 'Flag', and 'Flag Description'. \n",
    "or each of these columns, there are 319 missing values.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info # DF summary with column data types, non-null values, and memory usage, to aid in memory optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.describe() output\n",
    "df.describe() outputs a statistical summary for 6 columns but the  \"Value\" output is the only one of concern.\n",
    "\n",
    "The df.describe() function provides various statistics for the 6 specified columns in the livestock dataframe, such as the mean, standard deviation, minimum, and maximum values.\n",
    "\n",
    "It can be useful for quickly understanding the distribution of data in the dataframe, and identifying potential outliers or extreme values.\n",
    "\n",
    "In this case, the lowest stock is 6731 head of cattle and the highest  is 24 million. The Median is 1.9 million and the mean is about 4 milliom which immediatly suggest that the data is skewed and not normally distributed.  We will return to these initial explorations  later in the analysis and interpretation of the data.\n",
    "\n",
    " The \"Value\" column has 319 missing values. That is an important point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  unique combination of values from df.value_counts()\n",
    "The output shows the number of occurrences of each unique combination of values in the columns \"Domain Code\", \"Domain\", \"Area Code (M49)\", \"Area\", \"Element Code\", \"Element\", \"Item Code (CPC)\", \"Item\", \"Year Code\", \"Year\", \"Unit\", \"Value\", \"Flag\" and \"Flag Description\" in the DataFrame df.\n",
    "\n",
    "It seems that the DataFrame contains 1389 unique rows, with each row corresponding to a particular combination of values in the aforementioned columns. For example, there is one row that has a Domain Code of QCL, a Domain of Crops and livestock products, an Area Code (M49) of 40, an Area of Austria, an Element Code of 5111, an Element of Stocks, an Item Code (CPC) of 2111, an Item of Cattle, a Year Code of 1961, a Year of 1961, a Unit of Head, a Value of 2386761.0, a Flag of A, and a Flag Description of Official figure.\n",
    "\n",
    "It's also worth noting that there are several rows that have the same combination of values in all the columns, indicating that there are duplicate entries in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cardinals = df.nunique()\n",
    "print(cardinals) #Print unique values count for each column in livestock dataframe\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.columns = ['Area', 'Element', 'Item', 'Year', 'Unit', 'Value', 'Flag', 'Flag Description']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"rename() method names and  removes special characters \"\"\"\n",
    "df = df.rename(columns={'Domain Code': 'Dom_Cd', 'Area Code (M49)': 'Area_Cd', 'Element Code': 'ECode', 'Item Code (CPC)': 'Item_Code', 'Year Code': 'Yr Code','Flag Description'\n",
    ":'Flag_dsc'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Shorten a constant string entry\"\"\"\n",
    "df['Domain'] = df['Domain'].str.replace('Crops and livestock products', 'LS ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 'Belgium-Luxembourg'  counted as  separate country to Belgium and Luxembourg\"\"\"\n",
    "df['Area'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Shortens occurences of Belgium-Luxembourg  which is an anomaly in the datat\"\"\"\n",
    "df[\"Area\"] = df[\"Area\"].replace(\"Belgium-Luxembourg\", \"BeLux\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Write the DataFrame and a sampled Dataframe  to 2 CSVs in the processed data folder\n",
    "# df.to_csv('../clean/livestock.csv', index=False)\n",
    "# df.sample(40).to_csv('../data/processed/livestock_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = df.columns.tolist()\n",
    "headers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanp():\n",
    "    \"\"\"Scans the raw data folder and prints contents information.\n",
    "\n",
    "    Parameters:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        Prints out contents information of the raw data folder.\n",
    "    \"\"\" \n",
    "    folder_name = '../data/processed'\n",
    "    current_dir = os.getcwd()\n",
    "    folder_path = os.path.join(current_dir, folder_name)\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found\")\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path\")\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_name))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            print(\"{:30} {:10} {}\".format(item, size, modified_time))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))\n",
    "scanp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipython-latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=df.head()\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get column names\n",
    "headers = df.head(5).columns.tolist()\n",
    "\n",
    "# set column width\n",
    "col_width = [1.5, 1, 1.5,1.5, 1, 1.5,1,1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "# create a list of lists from the dataframe\n",
    "table = df.values.tolist()\n",
    "\n",
    "# create the LaTeX table using tabulate\n",
    "latex_table = tabulate(table, headers, tablefmt='latex_booktabs')\n",
    "\n",
    "# add the column width specifier to the LaTeX table\n",
    "latex_table = latex_table.replace('\\\\begin{tabular}', '\\\\begin{tabular}{%s}' % '|'.join(['p{%.2fcm}' % w  col_width]))\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "…or push an existing repository from the command line\n",
    "git remote add origin https://github.com/ronandownes/irl_beed_analysis.git\n",
    "git branch -M main\n",
    "git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$$\n",
    "\\begin{tabular}{p{1.50cm}|p{1.00cm}|p{1.50cm}}{lrl}\n",
    "\\toprule\n",
    " Name   &   Age & City     \\\\\n",
    "\\midrule\n",
    " John   &    25 & New York \\\\\n",
    " Alice  &    30 & Paris    \\\\\n",
    " Bob    &    35 & London   \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert dataframe to list of lists\n",
    "table = ds.values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "# use tabulate to create a table\n",
    "print(tabulate(table, headers, tablefmt='latex_booktabs'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Pandas DataFrame,'df' in latex table out\n",
    "table = tabulate(df.sample(40), headers='keys', tablefmt='latex', floatfmt=\".2f\")\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_rotated_table(df: pd.DataFrame, caption: str, label: str) -> str:\n",
    "    # Start the LaTeX tabular environment\n",
    "    latex = \"\\\\begin{table}[h]\\n\"\n",
    "    latex += \"\\\\centering\\n\"\n",
    "    latex += \"\\\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}\\n\"\n",
    "    latex += \"\\\\hline\\n\"\n",
    "    \n",
    "    # Add the table headers\n",
    "    headers = list(df.columns)\n",
    "    for header in headers:\n",
    "        latex += \"\\\\rotatebox{90}{\" + header + \"} & \"\n",
    "    latex = latex[:-2] + \"\\\\\\\\\\n\"\n",
    "    latex += \"\\\\hline\\n\"\n",
    "    \n",
    "    # Add the table rows\n",
    "    for row in df.iterrows():\n",
    "        latex += \" & \".join([str(item) for item in row[1]]) + \" \\\\\\\\\\n\"\n",
    "    \n",
    "    # End the LaTeX tabular environment and add the caption and label\n",
    "    latex += \"\\\\hline\\n\"\n",
    "    latex += \"\\\\end{tabular}\\n\"\n",
    "    latex += \"\\\\caption{\" + caption + \"}\\n\"\n",
    "    latex += \"\\\\label{\" + label + \"}\\n\"\n",
    "    latex += \"\\\\end{table}\"\n",
    "    \n",
    "    return latex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume your data is in a pandas DataFrame called 'df'\n",
    "table = tabulate(livestock, headers='keys', tablefmt='latex', floatfmt=\".2f\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_rotated_table(df.sample(40),'Sample of Livestock data before EDA','tab:livestock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert df to a LaTeX table\n",
    "table = tabulate(df, headers='keys', tablefmt='latex')\n",
    "# Print the LaTeX table\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop Belgium-Luxembourg\n",
    "# Drop \"Production\" column\n",
    "df = df.drop(df[df['Area'] == 'Belgium-Luxembourg'].index)\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the DataFrame\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of unique countries from the \"Area\" column\n",
    "countries = df['Area'].unique()\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# count the number of missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# print the result\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of areas with missing values\n",
    "num_missing_areas = df['Area'][df.isna().any(axis=1)].nunique()\n",
    "\n",
    "# Print the result\n",
    "print(\"The number of areas with missing values is:\", num_missing_areas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "31900/1708 # 20% data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake('Distribution of Missing Values by Area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake('msno.matrix(df)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a bar chart of the missing value counts by year\n",
    "plt.figure(figsize=(20,8))  # increase the figure size for better readability\n",
    "ax = year_counts.plot(kind='bar')\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Number of Missing Values', fontsize=14)\n",
    "plt.title('Distribution of Missing Values by Year', fontsize=18)\n",
    "plt.xticks(rotation=0, fontsize=12)  # rotate x-axis labels to 0 degrees\n",
    "ax.tick_params(axis='y', labelsize=12)  # adjust y-axis label size\n",
    "ax.tick_params(axis='x', pad=10)  # adjust x-axis tick padding\n",
    "\n",
    "# add value labels to the bars\n",
    "for i, v in enumerate(year_counts):\n",
    "    ax.text(i, v, str(v), ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "# set year labels vertically\n",
    "ax.set_xticklabels(year_counts.index, rotation=90)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "A new dataframe with counts of missing values \n",
    "for each country was sorted in descending order\n",
    "revealing  the top 10 countries\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create a new dataframe to hold the counts of missing values by country\n",
    "country_counts = df.isnull().sum(axis=1).groupby(df.Area).sum().sort_values(ascending=False)\n",
    "\n",
    "# get the top 10 countries with the most missing values\n",
    "top_10_countries = country_counts.head(10)\n",
    "\n",
    "# print the list of top 10 countries\n",
    "print(top_10_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe to hold the counts of missing values by country\n",
    "country_counts = df.isnull().sum(axis=1).groupby(df.Area).sum().sort_values(ascending=True)\n",
    "\n",
    "# get the list of countries with any missing values\n",
    "missing_countries = country_counts[country_counts > 0].index\n",
    "\n",
    "# print the list of countries with missing values\n",
    "print(\"Countries with missing values:\\n\", missing_countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe to hold the counts of missing values by country\n",
    "country_counts = df.isnull().sum(axis=1).groupby(df.Area).sum().sort_values(ascending=True)\n",
    "\n",
    "# get the list of countries with any missing values\n",
    "missing_countries = country_counts[country_counts > 0].index\n",
    "\n",
    "# print the list of countries with missing values\n",
    "print(\"Countries with missing values:\\n\", missing_countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe to hold the counts of missing values by country\n",
    "country_counts = df.isnull().sum(axis=1).groupby(df.Area).sum().sort_values(ascending=True)\n",
    "\n",
    "# get the list of countries with any missing values\n",
    "missing_countries = country_counts[country_counts > 0]\n",
    "\n",
    "# create a new dataframe with the missing value counts and the total number of observations for each country\n",
    "mv_counts = pd.concat([missing_countries, df.groupby('Area').size()], axis=1)\n",
    "mv_counts.columns = ['Missing Values', 'Total Observations']\n",
    "\n",
    "# calculate the proportion of missing values for each country\n",
    "mv_counts['% Missing'] = mv_counts['Missing Values'] / mv_counts['Total Observations'] * 100\n",
    "\n",
    "# sort the dataframe by the proportion of missing values in descending order\n",
    "mv_counts = mv_counts.sort_values('% Missing', ascending=False)\n",
    "\n",
    "# display the table\n",
    "print(mv_counts.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# group the data by Area and compute the total count of missing values for each group\n",
    "area_counts = df.isnull().sum(axis=1).groupby(df.Area).sum()\n",
    "\n",
    "# create a bar chart of the missing value counts by Area\n",
    "plt.figure(figsize=(10,5))\n",
    "ax = area_counts.plot(kind='bar')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Number of Missing Values')\n",
    "plt.title('Distribution of Missing Values by Area')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# add value labels to the bars\n",
    "for i, v in enumerate(area_counts):\n",
    "    ax.text(i, v, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_describtions = df['Flag Description'].unique()\n",
    "print(unique_describtions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[df['Year'] > 1999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents (Clickable in sidebar)<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Housekeeping\" data-toc-modified-id=\"Housekeeping-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Housekeeping</a></span></li><li><span><a href=\"#Apply-data-exploration-functions-to-livestock-data\" data-toc-modified-id=\"Apply-data-exploration-functions-to-livestock-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Apply data exploration functions to livestock data</a></span><ul class=\"toc-item\"><li><span><a href=\"#df.describe()-output\" data-toc-modified-id=\"df.describe()-output-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>df.describe() output</a></span></li><li><span><a href=\"#unique-combination-of-values-from-df.value_counts()\" data-toc-modified-id=\"unique-combination-of-values-from-df.value_counts()-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>unique combination of values from df.value_counts()</a></span></li></ul></li><li><span><a href=\"#Clean\" data-toc-modified-id=\"Clean-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Clean</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Input/Output:\" data-toc-modified-id=\"Data-Input/Output:-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Data Input/Output:</a></span></li><li><span><a href=\"#Data-Cleaning:\" data-toc-modified-id=\"Data-Cleaning:-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Data Cleaning:</a></span></li><li><span><a href=\"#Data-Exploration:\" data-toc-modified-id=\"Data-Exploration:-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Data Exploration:</a></span></li><li><span><a href=\"#Data-Transformation\" data-toc-modified-id=\"Data-Transformation-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Data Transformation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split\" data-toc-modified-id=\"Split-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Split</a></span></li></ul></li><li><span><a href=\"#Data-Combination:\" data-toc-modified-id=\"Data-Combination:-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Data Combination:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pascal\" data-toc-modified-id=\"Pascal-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>Pascal</a></span></li><li><span><a href=\"#Total-Character\" data-toc-modified-id=\"Total-Character-4.5.2\"><span class=\"toc-item-num\">4.5.2&nbsp;&nbsp;</span>Total Character</a></span></li></ul></li><li><span><a href=\"#Scanners,--Readers-and-Writers\" data-toc-modified-id=\"Scanners,--Readers-and-Writers-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Scanners,  Readers and Writers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scanners\" data-toc-modified-id=\"Scanners-4.6.1\"><span class=\"toc-item-num\">4.6.1&nbsp;&nbsp;</span>Scanners</a></span></li><li><span><a href=\"#Readers\" data-toc-modified-id=\"Readers-4.6.2\"><span class=\"toc-item-num\">4.6.2&nbsp;&nbsp;</span>Readers</a></span></li><li><span><a href=\"#Splitters\" data-toc-modified-id=\"Splitters-4.6.3\"><span class=\"toc-item-num\">4.6.3&nbsp;&nbsp;</span>Splitters</a></span></li><li><span><a href=\"#Delete\" data-toc-modified-id=\"Delete-4.6.4\"><span class=\"toc-item-num\">4.6.4&nbsp;&nbsp;</span>Delete</a></span></li><li><span><a href=\"#Rename\" data-toc-modified-id=\"Rename-4.6.5\"><span class=\"toc-item-num\">4.6.5&nbsp;&nbsp;</span>Rename</a></span></li><li><span><a href=\"#Replace\" data-toc-modified-id=\"Replace-4.6.6\"><span class=\"toc-item-num\">4.6.6&nbsp;&nbsp;</span>Replace</a></span></li></ul></li><li><span><a href=\"#Data-Processing\" data-toc-modified-id=\"Data-Processing-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Data Processing</a></span></li><li><span><a href=\"#String-Manipulation\" data-toc-modified-id=\"String-Manipulation-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>String Manipulation</a></span></li><li><span><a href=\"#File-System-Operations\" data-toc-modified-id=\"File-System-Operations-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;</span>File System Operations</a></span></li></ul></li><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Modules-Functions-Libraries-and-How-to-Use\" data-toc-modified-id=\"Modules-Functions-Libraries-and-How-to-Use-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Modules Functions Libraries and How to Use</a></span></li><li><span><a href=\"#Appendices\" data-toc-modified-id=\"Appendices-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Appendices</a></span><ul class=\"toc-item\"><li><span><a href=\"#FAOSTAT-Data-Domains\" data-toc-modified-id=\"FAOSTAT-Data-Domains-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>FAOSTAT Data Domains</a></span></li></ul></li><li><span><a href=\"#Climate--Data-Preparation\" data-toc-modified-id=\"Climate--Data-Preparation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Climate  Data Preparation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#About-FAOSTAT\" data-toc-modified-id=\"About-FAOSTAT-8.0.1\"><span class=\"toc-item-num\">8.0.1&nbsp;&nbsp;</span>About FAOSTAT</a></span></li><li><span><a href=\"#Licencing\" data-toc-modified-id=\"Licencing-8.0.2\"><span class=\"toc-item-num\">8.0.2&nbsp;&nbsp;</span>Licencing</a></span></li><li><span><a href=\"#CCKP-Data--and-an-initial-bit-of-EDA\" data-toc-modified-id=\"CCKP-Data--and-an-initial-bit-of-EDA-8.0.3\"><span class=\"toc-item-num\">8.0.3&nbsp;&nbsp;</span>CCKP Data  and an initial bit of EDA</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Key'] = df['Area'] + '_' + df['Year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sample(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_values = df['Flag Description'].unique()\n",
    "print(flag_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Domain Code', 'Domain', 'Area Code (M49)', 'Element Code', 'Element', 'Item Code (CPC)', 'Year Code', 'Year', 'Unit', 'Flag Description'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# count the frequency of NaN and 'Official figure' values in the 'Flag Description' column\n",
    "flag_counts = df['Flag Description'].value_counts(dropna=False)\n",
    "\n",
    "# plot a pie chart of the flag counts\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(flag_counts, labels=flag_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Flag Description Frequencies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_countries = df[df.isna().any(axis=1)]['Area'].unique()\n",
    "\n",
    "print(nan_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_areas = df['Area'].unique()\n",
    "print(unique_areas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_data = df[df['Area'] == 'Belgium-Luxembourg']\n",
    "bl_data.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luxembourg_data = df[df[\"Area\"] == \"Luxembourg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Area\"] == \"Luxembourg\"].head(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Area'] == 'Belgium'].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_areas = df['Area'].nunique()\n",
    "print('Number of unique areas:', num_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows for Belgium-Luxembourg\n",
    "df = df.drop(index=df[df[\"Area\"] == \"Belgium-Luxembourg\"].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "removeing extraneous columns\n",
    "\"\"\"\n",
    "\n",
    "df = df.drop(['flag','Domain Code', 'Domain', 'Area Code (M49)', 'Element Code', 'Element', 'Item Code (CPC)', 'Year Code', 'Year', 'Unit', 'Flag Description'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"  \n",
    "####  Transform \n",
    "\n",
    "The transform function reads in a CSV file, renames a column, \n",
    "and filters the resulting dataframe to only include the 'key' \n",
    "and specified column. It returns the resulting dataframe after\n",
    "renaming the specified column and filtering.\n",
    "\"\"\"\n",
    "\n",
    "def transform(path: str, filename: str, col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads in a CSV file from the given directory, renames a column to the given column name, and\n",
    "    filters the resulting dataframe to only include the 'key' and specified column.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The relative path of the directory containing the data.\n",
    "        filename (str): The name of the CSV file to read in.\n",
    "        col_name (str): The name to assign to the specified column in the resulting dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The resulting dataframe after renaming the specified column and filtering\n",
    "        to only include the 'key' and specified column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = os.path.join(path, filename)\n",
    "        df = pd.read_csv(file_path, on_bad_lines='skip', skiprows=1)\n",
    "        df.rename(columns={'Unnamed: 0': 'Year'}, inplace=True)\n",
    "        df['key'] = df.columns[1] + df['Year'].astype(str)\n",
    "        df.rename(columns={df.columns[1]: col_name}, inplace=True)\n",
    "        df = df.filter(['key', col_name])\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading in CSV file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "####  Combine Files\n",
    "\n",
    "The combine function is designed to read in all  the **precicipitation**\n",
    "or **mean-temperature** CSV files  from either the 'pr' or 'ts' folders,\n",
    "apply the transform function to each file to rename a column with the country \n",
    "the data relates to and filter at the superregional level. It then combines    \n",
    "all the the resulting dataframes together into a single dataframe for all the 27 EU counties.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def combine(path: str, subfolder: str, col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads in all CSV files in the given subfolder of the directory, applies the 'transform' function to each file, and\n",
    "    combines the resulting dataframes together into a single dataframe.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The relative path of the directory containing the data.\n",
    "        subfolder (str): The name of the subfolder within the directory to read the CSV files from.\n",
    "        col_name (str): The name to assign to the specified column in the resulting dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The resulting dataframe after combining data from all CSV files in the\n",
    "        specified subfolder.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        folder_path = os.path.join(path, subfolder)\n",
    "        csv_filenames = glob.glob(folder_path + \"/*.csv\")\n",
    "        processed_dfs = (transform(path, os.path.join(subfolder, os.path.basename(FileName)), col_name) for FileName in csv_filenames)\n",
    "        df = pd.concat(processed_dfs, ignore_index=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining CSV files in folder {folder_path}: {e}\")\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#### Temp °C  Preparation\n",
    "\n",
    "All 27 'pr_timeseries_annual_cru' and 27 'tas_timeseries_annual_cru' \n",
    "files will be processed into 'rain.df' &'rain.csv' and 'temp.df' & 'temp.csv' accordingly.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load the 'tas' data from the '../data' directory and create a dataframe 'temp_df'\n",
    "# with the 'Temp °C' data. \n",
    "temp_df = combine('../data', 'tas', 'Temp \\u00B0C')\n",
    "\n",
    "# Display the first few rows of the 'temp_df' dataframe.\n",
    "temp_df.head()\n",
    "\n",
    "# Save the 'temp_df' dataframe to a CSV file in the '../data/processed' directory, \n",
    "# with the filename 'temp.csv', and exclude the index column from the output.\n",
    "temp_df.to_csv('../data/processed/temp.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the 'temp_df' dataframe again.\n",
    "temp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#### Rain Preparation\n",
    "\n",
    "All 27 Rain CSVs are processed into rain.df and rain.csv.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load the 'pr' data from the '../data' directory and create a dataframe 'rain_df'\n",
    "# with the 'Rain_mm/yr' data. \n",
    "rain_df = combine('../data', 'pr', 'Rain_mm/yr')\n",
    "\n",
    "# Display the first few rows of the 'rain_df' dataframe.\n",
    "rain_df.head()\n",
    "\n",
    "# Save the 'rain_df' dataframe to a CSV file in the '../data/processed' directory, \n",
    "# with the filename 'rain.csv', and exclude the index column from the output.\n",
    "rain_df.to_csv('../data/processed/rain.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the 'rain_df' dataframe again.\n",
    "rain_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only data from 2000 and later\n",
    "beef_df = beef_df[beef_df['Year'] >= 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=beef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column titles\n",
    "cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanp() # scans the processed folder in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Input/Output:\n",
    "- readr()\n",
    "- scan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning:\n",
    "- camel()\n",
    "- pascal()\n",
    "- snake()\n",
    "- clean_df()\n",
    "- eu()\n",
    "- now2()\n",
    "- split_file_name()\n",
    "- splitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration:\n",
    "- filter_col_by_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "- eu()\n",
    "- transform\n",
    "- get_annual_aggregates()\n",
    "- get_dfs()\n",
    "- get_multi_index_df()\n",
    "- get_ranking_df()\n",
    "- pivot_table_aggregate()\n",
    "- prepare_wealth_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split\n",
    "The split function takes a pandas DataFrame df and a column name col as input, and returns a dictionary where each key corresponds to a unique value in the specified column, and each value is a DataFrame containing all rows with that unique value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "#### Split\n",
    "The split function takes a pandas DataFrame df and a column name col as input, and returns a dictionary where each key corresponds to a unique value in the specified column, and each value is a DataFrame containing all rows with that unique value.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def split(df: pd.DataFrame, col: str) -> dict:\n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to split.\n",
    "        col (str): The name of the column to group by.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where each key corresponds to a unique value in the specified column,\n",
    "        and each value is a DataFrame containing all rows with that unique value.\n",
    "    \"\"\"\n",
    "    dfs = dict(tuple(df.groupby(col)))\n",
    "    return dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now2(df):\n",
    "    \"\"\"\n",
    "    Filter to 'Year' >= 2000.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to filter.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    year_min = 2000\n",
    "    return df[df['Year'] >= year_min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Combination:\n",
    "- merge_dfs()\n",
    "- combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Snake\n",
    "\n",
    "\n",
    "def snake(text, default='default'):\n",
    "    \"\"\"\n",
    "    Converts a given string to snake_case by replacing any whitespace characters with underscores,\n",
    "    converting to all lowercase, and removing any non-alphanumeric characters from the beginning and end.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The string to convert to snake_case.\n",
    "        default (str): The default value to return if the input text is empty.\n",
    "\n",
    "    Returns:\n",
    "        str: The resulting string in snake_case format, or the default value if the input text is empty.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return default\n",
    "    # Convert to string and replace any non-alphanumeric characters at the beginning and end with an empty string\n",
    "    text = re.sub(r'^\\W+|\\W+$', '', str(text))\n",
    "    # Replace any period symbols with underscores\n",
    "    text = text.replace('.', '')\n",
    "    # Replace any other non-alphanumeric characters with empty strings\n",
    "    text = re.sub(r'\\W+', '_', text)\n",
    "    # Convert to all lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "text = \"This is a text with periods. (And other characters.)\"\n",
    "result = snake(text)\n",
    "print(result)  # Output: \"this_is_a_text_with_periods_and_other_characters\"\n",
    "\n",
    "\n",
    "# snake('does snake Work')\n",
    "\n",
    "# snake('                  ')\n",
    "# snake('                  ')\n",
    "# snake('                  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake('\"This is a text with periods. (And other characters.)\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pascal\n",
    "The 'pascal' naming function takes in a string as input and converts it to a PascalCase format. PascalCase is a naming convention where the first letter of each word is capitalized, and there are no spaces or separators between the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pascal(string):\n",
    "    \"\"\"\n",
    "    Convert a space- or snake-separated string to PascalCase.\n",
    "\n",
    "    Parameters:\n",
    "        string (str): The input string to convert to PascalCase.\n",
    "\n",
    "    Returns:\n",
    "        str: The input string in PascalCase format.\n",
    "\n",
    "    \"\"\"\n",
    "    # Replace any underscores with spaces\n",
    "    string = string.replace(\"_\", \" \")\n",
    "    # Capitalize the first letter of each word\n",
    "    words = string.title()\n",
    "    # Remove any remaining spaces\n",
    "    words = words.replace(\" \", \"\")\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#### Camel\n",
    "\n",
    "Camel case is a naming convention in which each word in a compound word is capitalized, except for the first word which is in lower case. It is commonly used in programming languages for naming variables and functions.\n",
    "\n",
    "handles both snake_case and space-separated strings\n",
    "\"\"\"\n",
    "def camel(string):\n",
    "    \"\"\"\n",
    "    Convert a space-separated or snake_case string to camelCase.\n",
    "\n",
    "    Parameters:\n",
    "        string (str): The string to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: The converted string in camelCase.\n",
    "    \"\"\"\n",
    "    # Replace underscores with spaces and split the string into a list of words\n",
    "    words = string.replace(\"_\", \" \").split()\n",
    "    # Convert the first word to lowercase and capitalize all subsequent words\n",
    "    camel_cased = [words[0].lower()] + [word.capitalize() for word in words[1:]]\n",
    "    # Concatenate the words together and return the resulting string\n",
    "    return \"    \" + ''.join(camel_cased) + \"    \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#### Title \n",
    "The 'title' naming convention is where each word starts\n",
    "with a capital letter, except for prepositions and conjunctions, which start with a lowercase letter. \n",
    "The function takes a string as input and converts it to title case, \n",
    "where the first letter of each non-conjunction/preposition word is capitalized, and all other letters are lowercase.\n",
    "It achieves this by splitting the input string into a list of words, identifying which words are prepositions or conjunctions based on a predefined list, and then capitalizing the first letter of all other words while converting prepositions and conjunctions to lowercase. The resulting list of processed words is then joined back into a single string with proper spacing and returned.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def title(sentence):\n",
    "    \"\"\"\n",
    "    Takes a string and converts it to title case, where the first letter of each\n",
    "    non-conjunction/preposition word is capitalized, and all other letters are lowercase.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): The string to convert to title case.\n",
    "        \n",
    "    Returns:\n",
    "        str: The input string converted to title case.\n",
    "    \"\"\"\n",
    "    # Define a list of common prepositions and conjunctions\n",
    "    prepositions_conjunctions = ['a', 'this', 'an', 'the', 'and', 'but', 'or', 'for', 'has', 'nor', 'on', 'at', 'to', 'from', 'by', 'over', 'under', 'in', 'out', 'of']\n",
    "    # Split the input string into a list of words\n",
    "    words = sentence.split()\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        # If the word is not a preposition or conjunction, capitalize the first letter and lowercase the rest\n",
    "        if word.lower() not in prepositions_conjunctions:\n",
    "            processed_words.append(word.capitalize())\n",
    "        # If the word is a preposition or conjunction, convert to lowercase\n",
    "        else:\n",
    "            processed_words.append(word.lower())\n",
    "    # Join the list of processed words into a single string, with proper spacing\n",
    "    output = \" \".join(processed_words)\n",
    "    # Remove any leading/trailing whitespace and add some padding\n",
    "    return \"     \" + re.sub('\\s+', ' ', output.strip()) + \"     \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_count(string):\n",
    "    \"\"\"\n",
    "    Count the number of characters in a string.\n",
    "\n",
    "    Parameters:\n",
    "    - string (str): The string to count characters in.\n",
    "\n",
    "    Returns:\n",
    "    - int: The number of characters in the string.\n",
    "    \"\"\"\n",
    "    return len(string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scanners,  Readers and Writers\n",
    "#### Scanners\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scan(folder_path=\"../\"):\n",
    "    \"\"\"Scans the specified folder and prints contents information.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): The path to the folder to scan. Default is the root directory.\n",
    "\n",
    "    Returns:\n",
    "        Prints out contents information of the specified folder.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found: {}\".format(folder_path))\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path: {}\".format(folder_path))\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_path))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path) // 1024  # Convert to KB\n",
    "            print(\"{:30} {:10,} KB\".format(item, size))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))\n",
    "\n",
    "# scan('../../images')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scann():\n",
    "    \"\"\"Scans the raw data folder and prints contents information.\n",
    "\n",
    "    Parameters:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        Prints out contents information of the raw data folder.\n",
    "    \"\"\"\n",
    "    folder_path = \"../notebooks\"\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found\")\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path\")\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_path))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            print(\"{:30} {:10} {}\".format(item, size, modified_time))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scanr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Readers\n",
    "\n",
    "'read_csv' is a function in the Pandas library for reading in CSV (Comma Separated Values) files into a DataFrame. It is a flexible function that can handle a variety of input formats, including different delimiters, encodings, and line endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(filename, df_name, folder_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the specified folder and returns a pandas DataFrame\n",
    "    with the specified name.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The name of the CSV file to be read.\n",
    "        df_name (str): The name to be assigned to the resulting DataFrame.\n",
    "        folder_path (str): The path of the folder containing the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame created from the data in the CSV file with\n",
    "        the specified name.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(folder_path, f'{filename}.csv')\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.name = df_name\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readr(filename, df_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from  the raw data folder and returns a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        filename (str): base name of CSV file to be read.\n",
    "        df_name (str): The DataFrame name to be assigned \n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame created from the data in the CSV file with\n",
    "        the specified name.\n",
    "    \"\"\"\n",
    "    file_path = '../data/raw/' + filename + '.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.name = df_name\n",
    "    return df\n",
    "# readr('land_use','land_use_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readp(filename, df_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the specified file path in the processed folder and returns a pandas DataFrame\n",
    "    with the specified name.\n",
    "    \n",
    "    Parameters:\n",
    "        filename (str): The name of the CSV file to be read.\n",
    "        df_name (str): The name to be assigned to the resulting DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame created from the data in the CSV file with\n",
    "        the specified name.\n",
    "    \"\"\"\n",
    "    file_path = f'../data/processed/{filename}.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.name = df_name\n",
    "    return df\n",
    "# df=readp('missing','missing_df')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(file_name: str) -> Tuple[Dict[str, pd.DataFrame], List[str]]:\n",
    "    \"\"\"\n",
    "    Splits a CSV file into multiple dataframes based on the unique values in the 'Element' column.\n",
    "    \n",
    "    Parameters:\n",
    "        file_name (str): The name of the CSV file to read in.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[Dict[str, pd.DataFrame], List[str]]: A tuple containing a dictionary where each key corresponds\n",
    "        to a unique value in the 'Element' column, and each value is a dataframe containing all rows with that\n",
    "        unique value, and a list of unique values in the 'Element' column.\n",
    "    \"\"\"\n",
    "    if not file_name.endswith('.csv'):\n",
    "        raise ValueError('Input file must be a .csv file')\n",
    "    \n",
    "    # Load the specified dataframe\n",
    "    df = pd.read_csv(f'../data/raw/{file_name}')\n",
    "    \n",
    "    # Group the dataframe by the 'Element' column\n",
    "    dfs = dict(tuple(df.groupby('Element')))\n",
    "    \n",
    "    # Get the unique values in the 'Element' column\n",
    "    elements = list(df['Element'].unique())\n",
    "    \n",
    "    return dfs, elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_elementr(file_name: str) -> Tuple[Dict[str, pd.DataFrame], List[str]]:\n",
    "    \"\"\"\n",
    "    Splits a CSV file into multiple dataframes based on the unique values in the 'Element' column.\n",
    "    \n",
    "    Parameters:\n",
    "        file_name (str): The name of the CSV file to read in.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[Dict[str, pd.DataFrame], List[str]]: A tuple containing a dictionary where each key corresponds\n",
    "        to a unique value in the 'Element' column, and each value is a dataframe containing all rows with that\n",
    "        unique value, and a list of unique values in the 'Element' column.\n",
    "    \"\"\"\n",
    "    # Load the specified dataframe\n",
    "    df = pd.read_csv(f'../data/raw/{file_name}')\n",
    "    \n",
    "    # Group the dataframe by the 'Element' column\n",
    "    dfs = dict(tuple(df.groupby('Element')))\n",
    "    \n",
    "    # Get the unique values in the 'Element' column\n",
    "    elements = list(df['Element'].unique())\n",
    "    \n",
    "    return dfs, elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_element(df: pd.DataFrame) -> Tuple[Dict[str, pd.DataFrame], List[str]]:\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into multiple dataframes based on the unique values in the 'Element' column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to split.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[Dict[str, pd.DataFrame], List[str]]: A tuple containing a dictionary where each key corresponds\n",
    "        to a unique value in the 'Element' column, and each value is a dataframe containing all rows with that\n",
    "        unique value, and a list of unique values in the 'Element' column.\n",
    "    \"\"\"\n",
    "    # Group the dataframe by the 'Element' column\n",
    "    dfs = dict(tuple(df.groupby('Element')))\n",
    "    \n",
    "    # Get the unique values in the 'Element' column\n",
    "    elements = list(df['Element'].unique())\n",
    "    \n",
    "    return dfs, elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupdf(df, column_name):\n",
    "    # Group the dataframe by the specified column\n",
    "    dfs = dict(tuple(df.groupby(column_name)))\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Delete\n",
    "\n",
    "In this code, we use the os module to delete a file at the specified file path. We first check if the file exists using the os.path.exists() function. If the file exists, we delete it using the os.remove() function. If the file does not exist, we print a message indicating that the file was not found.\n",
    "\n",
    "Note that this code permanently deletes the file, so you should use it with caution. Once a file is deleted, it cannot be easily recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def delete(file_name):\n",
    "    file_path = os.path.join(\"..\", \"data\", \"raw\", file_name)\n",
    "    expected_ext = \".csv\"\n",
    "    if not file_path.endswith(expected_ext):\n",
    "        print(\"Error: Invalid file extension. File extension must be .csv.\")\n",
    "    elif os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"{file_name} deleted successfully\")\n",
    "    else:\n",
    "        print(f\"{file_name} not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename\n",
    "\n",
    "This function first constructs the file_path by joining the ../data/raw directory with the given file_name. It then sets the expected file extension to .csv. The function then checks if the file_path ends with the expected file extension. If it doesn't, the function prints an error message. If it does, the function checks if the file exists at file_path, and if it does, it removes it and prints a success message. If the file doesn't exist, the function prints a \"not found\" message. like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(old_filename, new_filename):\n",
    "    old_file_path = os.path.join(\"..\", \"data\", old_filename)\n",
    "    new_file_path = os.path.join(\"..\", \"data\", new_filename)\n",
    "    try:\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "        print(f\"{old_filename} renamed to {new_filename} successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{old_filename} not found\")\n",
    "    except FileExistsError:\n",
    "        print(f\"A file with the name {new_filename} already exists\")\n",
    "    except OSError:\n",
    "        print(\"Invalid file path or name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(input_str, find_str, replace_str):\n",
    "    output_str = re.sub(find_str, replace_str, input_str)\n",
    "    \n",
    "    if output_str == input_str:\n",
    "        warnings.warn(\"Replacement unsuccessful: '{}' not found in input string.\".format(find_str))\n",
    "    \n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  tabler\n",
    "\n",
    "\"\"\"The code defines a function tabler that generates an HTML table with information on the files in a given folder and\n",
    "adds a section header that contains the directory path \n",
    "\"\"\"\n",
    "\n",
    "def tabler(folder_path):\n",
    "    \"\"\"\n",
    "    Generate an HTML table with information on files in a given folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path to the folder containing the files.\n",
    "\n",
    "    Returns:\n",
    "    - str: The HTML code for the table.\n",
    "    \"\"\"\n",
    "    # Get the contents of the folder\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        return \"Directory not found\"\n",
    "    except OSError:\n",
    "        return \"Invalid folder path\"\n",
    "\n",
    "    # Create the section header\n",
    "    header = f\"### {folder_path}\\n\\n\"\n",
    "\n",
    "    # Create the table header\n",
    "    table = '<table style=\"font-size:100%\"><thead><tr><th>File Name</th><th>Size</th><th>Modified Time</th></tr></thead><tbody>'\n",
    "\n",
    "    # Add a row for each file\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            # Get the file size and modified time\n",
    "            size_bytes = os.path.getsize(item_path)\n",
    "            size_kb = size_bytes / 1024\n",
    "            size_str = '{:,.2f} KB'.format(size_kb)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            # Add a row to the table\n",
    "            table += '<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(item, size_str, modified_time)\n",
    "\n",
    "    # Close the table\n",
    "    table += '</tbody></table>'\n",
    "\n",
    "    return header + table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beef(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and returns a Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): The path to the CSV file to read.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The resulting DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "- eu(df)\n",
    "- now2(df)\n",
    "- split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  String Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File System Operations\n",
    "\n",
    "\n",
    "These functions are responsible for performing various operations on the file system.\n",
    "scan(): Scans a specified folder and prints contents information.\n",
    "tabler(): Generates an HTML table with information on files in a given folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### eu\n",
    "def eu(df):\n",
    "    \"\"\"\n",
    "    Filter a pandas DataFrame to include only the rows where the 'Area' column contains values that match the countries in the European Union.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to filter.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    country_list = ['Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czechia', 'Denmark', \n",
    "                    'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Ireland', 'Italy', \n",
    "                    'Latvia', 'Lithuania', 'Luxembourg', 'Malta', 'Netherlands', 'Poland', 'Portugal', 'Romania', \n",
    "                    'Slovakia', 'Slovenia', 'Spain', 'Sweden']\n",
    "    return df[df['Area'].isin(country_list)]\n",
    "\n",
    "# df=readr('meat','df')\n",
    "# df=eu(df)\n",
    "# df\n",
    "++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now(df, year_min):\n",
    "    \"\"\"\n",
    "    Filter to 'Year'>.\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to filter.\n",
    "    - year_min (int): The minimum year to include in the filtered DataFrame.\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    return df[df['Year'] >= year_min]\n",
    "\n",
    "# df=readr('meat','df')\n",
    "# df = now(df, 2015)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and  comparison of Ireland's beef sector with other  EU contries\n",
    "\n",
    "![image1](../images/tu04.png)\n",
    "\n",
    "<span style=\"font-size: 24px;\">                 </span>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we carry out the first 6 stages in the followning list@\n",
    "\n",
    "1. Reading - export, import, trade imbalance, arable production, animal stock\n",
    "2. Cleaning\n",
    "3. Transformation\n",
    "4. Splitting\n",
    "5. Aggregation\n",
    "6. Analysis\n",
    "7. Visualization\n",
    "8. Modeling and Machine Learning\n",
    "9. Forecasting\n",
    "10. Sentiment Analysis\n",
    "11. Evidence Based Recommendations\n",
    "12. Process Rationale\n",
    "13. Ireland as your baseline.\n",
    "\n",
    "\n",
    "## Modules Functions Libraries and How to Use\n",
    "\n",
    "Before starting the exploratory data analysis (EDA), make sure to execute all necessary module imports, libraries, and functions. This will ensure all  required dependencies and tools to perform analysis are operational.\n",
    "To execute all necessary module imports, libraries, and functions before starting the exploratory data analysis (EDA) \n",
    "**Restart & run all** as convention of dependancy order was broken for organisational purposes! On my part that is! Sorry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Reading\n",
    "2. Data Cleaning\n",
    "3. Data Transformation\n",
    "4. Data Splitting\n",
    "5. Data Aggregation\n",
    "6. Data Analysis\n",
    "7. Data Visualization\n",
    "8. Modeling and Machine Learning\n",
    "9. Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAOSTAT Data Domains \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../images/we16.png\" alt=\"image7\" width=\"80%\">\n",
    "\n",
    "[<span style=\"font-size: 18px;\">Figure 2:Data Domain Table view of FAOSTAT</span>\n",
    "](https://www.fao.org/faostat/en/#data/domains_table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The [FAOSTAT Data Domains](https://www.fao.org/faostat/en/#data/domains_table)  are organised as follows:\n",
    "\n",
    "  -  Production: Production of crops and livestock products, including production indices and the value of agricultural production.\n",
    "\n",
    "  - Food Security and Nutrition: Information on SDG indicators related to food security and nutrition and food balances.\n",
    "\n",
    "  - Trade: Including detailed trade matrices, trade indices, and updates on related data.\n",
    "\n",
    "  - Prices: Producer and   consumer price indices, deflators, and exchange rates.\n",
    "\n",
    "  - Land, Inputs and Sustainability:Land use, land cover, inputs, including fertilizers and manure and pesticides.\n",
    "  \n",
    "  - Population and Employment: Annual population including those specific to agriculture and rural areas.\n",
    "\n",
    "  - Investment: Government expenditure, credit to agriculture, foreign direct investment, and country investment statistics.\n",
    "\n",
    "  - Macro-Economic Indicators: Such as capital stock.\n",
    "\n",
    "  - Food Value Chain: This domain provides information on the value shares of the food industry and primary factors.\n",
    "\n",
    "  - Climate Change: Emissions, crop residues, forests, and other indicators related to climate change.\n",
    "\n",
    "  - Forestry: Forestry production and trade, as well as forestry trade flows.\n",
    "\n",
    "  - SDG Indicators: The Sustainable Development Goals (SDGs) are a set of 17 goals established by the United Nations in 2015.\n",
    "\n",
    "  - World Census of Agriculture: This domain provides structural data from agricultural censuses taken around the world\n",
    "\n",
    "  - Discontinued archives and data series: This includes data on indicators from surveys and research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climate  Data Preparation \n",
    "\n",
    "<img src=\"../images/tu03.png\" alt=\"image5\" width=\"50%\">\n",
    "<span style=\"font-size: 24px;\">                  </span>\n",
    " Only two attributes are taken from the CCKP database: Mean_Temperature and Precipitation.\n",
    "\n",
    "[The CCKP website](https://climateknowledgeportal.worldbank.org/)\n",
    " is a  resource for information on the impacts of climate change and the actions taken to address these impacts. While this is outside the remit of this project the CCKP also provides access to global data on a historical  basis for the **Mean_Temperature** and **Precipitation** at the  country-by-country level  on both monthly and yearly aggregates. While **humidity** is also a known influential predictor variable these two should have a statistically significant imapct on our predictive modelling. Spatial data is provided as a global NetCDF file, with Climatology, Timeseries and Heatplot data is provided as a CSV file.\n",
    "\n",
    "Climate conditions, such as average temperature and rainfall (precipitation ) can greatly affect the growth and health of cattle. Precipitation and temperature predictor variables were  retrieved  but they were   aggragated by country and this necessitated  the **cckp** and  **combine**  functions for data wrangling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  About FAOSTAT\n",
    "\n",
    "<img src=\"../images/tu02.png\" alt=\"image5\" width=\"50%\">\n",
    "\n",
    "<span style=\"font-size: 24px;\">                  </span>\n",
    "FAOSTAT is a comprehensive database maintained by the Food and Agriculture Organization of the United Nations (FAO), providing timely, reliable data on agriculture, food, and nutrition for over 200 countries. Its information is used to inform decision-making, policy formulation, and research in the field, covering topics such as production, trade, and fertilizer use. FAOSTAT is a valuable resource for governments, organizations, researchers, and the public, informing policy and interventions to enhance food security and reduce poverty.\n",
    "\n",
    "#### Licencing \n",
    "All datasets from the [FAOSTAT](https://www.fao.org/faostat/en/#home) are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO [(CC BY-NC-SA 3.0 IGO)](https://creativecommons.org/licenses/by-nc-sa/3.0/igo/). Source: FAOSTAT (2023). Time Series datasets.\n",
    "<img src=\"../images/tu06.png\" alt=\"image5\" width=\"50%\">\n",
    "<span style=\"font-size: 24px;\">                  </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "####   CCKP Data  and an initial bit of EDA\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <img src=\"../images/we28.png\" alt=\"image19\" style=\"width: 30%;\">\n",
    "  <img src=\"../images/we29.png\" alt=\"image20\" style=\"width: 30%;\">\n",
    "    <img src=\"../images/we30.png\" alt=\"image20\" style=\"width: 30%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size: 18px;\">Figure 3:Time Series Data downloaded for 27 EU countries from  CCKP site </span>\n",
    "\n",
    "\n",
    "To maintain consistency with the FAO data, annual and not monthly time series  aggregates were taken from the Climatic Research Unit (CRU) dataset for **precipitation** and   **mean-temperature**. These datasets are provided by the CRU TS 4.04 dataset, a gridded climate dataset produced by the Climatic Research Unit (CRU) at the University of East Anglia in the United Kingdom. In the statistics section, range, variance, and standard deviation of monthly data may be revisited for insights.\n",
    "\n",
    "The file names and folder names of the CCKP data used in this project are tabulated below.\n",
    "\n",
    "<span style=\"font-size: 24px;\">Table : Table shows Time Series data filenames and folders  for 27 EU countries from  CCKP site </span>\n",
    "\n",
    "\n",
    "\n",
    "| Country        | Code | TasAnnual (Folder with Tempature data)                              | PrAnnual   (Folder with Precipitation Data)                        |\n",
    "|:--------------:|:----:|:--------------------------------------:|:---------------------------------:|\n",
    "| Albania        | ALB  | tas_timeseries_annual_cru_1901-2021_ALB.csv | pr_timeseries_annual_cru_1901-2021_ALB.csv |\n",
    "| Andorra        | AND  | tas_timeseries_annual_cru_1901-2021_AND.csv | pr_timeseries_annual_cru_1901-2021_AND.csv |\n",
    "| Austria        | AUT  | tas_timeseries_annual_cru_1901-2021_AUT.csv | pr_timeseries_annual_cru_1901-2021_AUT.csv |\n",
    "| Belarus        | BLR  | tas_timeseries_annual_cru_1901-2021_BLR.csv | pr_timeseries_annual_cru_1901-2021_BLR.csv |\n",
    "| Belgium        | BEL  | tas_timeseries_annual_cru_1901-2021_BEL.csv | pr_timeseries_annual_cru_1901-2021_BEL.csv |\n",
    "| Bosnia and Herzegovina | BIH  | tas_timeseries_annual_cru_1901-2021_BIH.csv | pr_timeseries_annual_cru_1901-2021_BIH.csv |\n",
    "| Bulgaria       | BGR  | tas_timeseries_annual_cru_1901-2021_BGR.csv | pr_timeseries_annual_cru_1901-2021_BGR.csv |\n",
    "| Croatia        | HRV  | tas_timeseries_annual_cru_1901-2021_HRV.csv | pr_timeseries_annual_cru_1901-2021_HRV.csv |\n",
    "| Cyprus         | CYP  | tas_timeseries_annual_cru_1901-2021_CYP.csv | pr_timeseries_annual_cru_1901-2021_CYP.csv |\n",
    "| Czech Republic | CZE  | tas_timeseries_annual_cru_1901-2021_CZE.csv | pr_timeseries_annual_cru_1901-2021_CZE.csv |\n",
    "| Denmark        | DNK  | tas_timeseries_annual_cru_1901-2021_DNK.csv | pr_timeseries_annual_cru_1901-2021_DNK.csv |\n",
    "| Estonia        | EST  | tas_timeseries_annual_cru_1901-2021_EST.csv | pr_timeseries_annual_cru_1901-2021_EST.csv |\n",
    "| Finland        | FIN  | tas_timeseries_annual_cru_1901-2021_FIN.csv | pr_timeseries_annual_cru_1901-2021_FIN.csv |\n",
    "| France         | FRA  | tas_timeseries_annual_cru_1901-2021_FRA.csv | pr_timeseries_annual_cru_1901-2021_FRA.csv |\n",
    "| Germany        | DEU  | tas_timeseries_annual_cru_1901-2021_DEU.csv | pr_timeseries_annual_cru_1901-2021_DEU.csv |\n",
    "| Gibraltar      | GIB  | tas_timeseries_annual_cru_1901-2021_GIB.csv | pr_timeseries_annual_cru_1901-2021_GIB.csv |\n",
    "| Greece | GRC | tas_timeseries_annual_cru_1901-2021_GRC.csv | pr_timeseries_annual_cru_1901-2021_GRC.csv |\n",
    "| Croatia | HRV | tas_timeseries_annual_cru_1901-2021_HRV.csv | pr_timeseries_annual_cru_1901-2021_HRV.csv |\n",
    "| Hungary | HUN | tas_timeseries_annual_cru_1901-2021_HUN.csv | pr_timeseries_annual_cru_1901-2021_HUN.csv |\n",
    "| Ireland | IRL | tas_timeseries_annual_cru_1901-2021_IRL.csv | pr_timeseries_annual_cru_1901-2021_IRL.csv |\n",
    "| Italy | ITA | tas_timeseries_annual_cru_1901-2021_ITA.csv | pr_timeseries_annual_cru_1901-2021_ITA.csv |\n",
    "| Lithuania | LTU | tas_timeseries_annual_cru_1901-2021_LTU.csv | pr_timeseries_annual_cru_1901-2021_LTU.csv |\n",
    "| Luxembourg | LUX | tas_timeseries_annual_cru_1901-2021_LUX.csv | pr_timeseries_annual_cru_1901-2021_LUX.csv |\n",
    "| Latvia | LVA | tas_timeseries_annual_cru_1901-2021_LVA.csv | pr_timeseries_annual_cru_1901-2021_LVA.csv |\n",
    "| Malta | MLT | tas_timeseries_annual_cru_1901-2021_MLT.csv | pr_timeseries_annual_cru_1901-2021_MLT.csv |\n",
    "| Netherlands | NLD | tas_timeseries_annual_cru_1901-2021_NLD.csv | pr_timeseries_annual_cru_1901-2021_NLD.csv |\n",
    "| Poland | POL | tas_timeseries_annual_cru_1901-2021_POL.csv | pr_timeseries_annual_cru_1901-2021_POL.csv |\n",
    "| Portugal | PRT | tas_timeseries_annual_cru_1901-2021_PRT.csv | pr_timeseries_annual_cru_1901-2021_PRT.csv |\n",
    "| Romania | ROU | tas_timeseries_annual_cru_1901-2021_ROU.csv | pr_timeseries_annual_cru_1901-2021_ROU.csv |\n",
    "| Slovakia | SVK | tas_timeseries_annual_cru_1901-2021_SVK.csv | pr_timeseries_annual_cru_1901-2021_SVK.csv |\n",
    "| Slovenia | SVN | tas_timeseries_annual_cru_1901-2021_SVN.csv | pr_timeseries_annual_cru_1901-2021_SVN.csv |\n",
    "| Sweden | SWE | tas_timeseries_annual_cru_1901-2021_SWE.csv | pr_timeseries_annual_cru_1901-2021_SWE.csv |\n",
    "\n",
    "\n",
    "All datasets from the CCKP are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO (CC BY-NC-SA 3.0 IGO). \n",
    "Source: CCKP (2023). Time Series datasets. Retrieved from [https://climateknowledgeportal.worldbank.org/download-data]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readcattle():\n",
    "    \"\"\"\n",
    "    Reads in the 'live_animale_cattle_stock_eu_1961_2021.csv' file from the '../data/raw/' directory and returns a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    data_path = '../data/raw/live_animale_cattle_stock_eu_1961_2021.csv'\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readcattle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions and techniques:\n",
    "\n",
    "Data Cleaning:\n",
    "\n",
    "df.dropna()\n",
    "df.fillna()\n",
    "df.clip()\n",
    "df.replace()\n",
    "df.drop_duplicates()\n",
    "df.drop_duplicates()\n",
    "df.fillna()\n",
    "df.merge()\n",
    "df.pivot()\n",
    "df.rename()\n",
    "df.query()\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df['column_name'].interpolate(inplace=True)\n",
    "os.listdir('data')\n",
    "Data Wrangling:\n",
    "\n",
    "df.melt()\n",
    "df.pivot()\n",
    "df.stack()\n",
    "df.unstack()\n",
    "df.astype()\n",
    "df.get()\n",
    "df.apply()\n",
    "pd.crosstab()\n",
    "pd.concat()\n",
    "df.rename()\n",
    "df.replace()\n",
    "df.sort_values()\n",
    "df.transform()\n",
    "Data Exploration:\n",
    "\n",
    "df.info()\n",
    "df.describe()\n",
    "df.value_counts()\n",
    "Data Visualization:\n",
    "\n",
    "plotly.plot()\n",
    "seaborn.plot()\n",
    "bokeh.plot()\n",
    "ggplot.plot()\n",
    "matplotlib.plot()\n",
    "plot.plot()\n",
    "holoviews.plot()\n",
    "Data Transformation:\n",
    "\n",
    "df.log()\n",
    "df.exp()\n",
    "df.normalize()\n",
    "df.scale()\n",
    "df.power()\n",
    "df.abs()\n",
    "df.rescale()\n",
    "df.subtract()\n",
    "df.divide()\n",
    "df.add()\n",
    "df.expit()\n",
    "df.cumsum()\n",
    "Feature Engineering:\n",
    "\n",
    "df.discretize()\n",
    "df.binarize()\n",
    "df.rank()\n",
    "df.dummies()\n",
    "df.group_enc()\n",
    "df.interactions()\n",
    "df.frequency()\n",
    "df.scaling/discretize()\n",
    "df.non-linear_trans()\n",
    "Time Series Analysis:\n",
    "\n",
    "df.pct()\n",
    "df.shift()\n",
    "df.diff()\n",
    "df.pct_change()\n",
    "df.autocorr()/df.pacf()\n",
    "df.rolling()\n",
    "df.forecast()\n",
    "df.rolling_window()\n",
    "Statistical Analysis:\n",
    "\n",
    "df.describe()\n",
    "df.skew()\n",
    "df.kurtosis()\n",
    "df.corr()\n",
    "stats.ttest_ind()\n",
    "statsmodels.stats.ztest()\n",
    "stats.chi2_contingency()\n",
    "stats.f_oneway()\n",
    "statsmodels.formula.api.ols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Data Cleaning           |\n",
    "| -----------------------|\n",
    "| df.dropna()            |\n",
    "| df.fillna()            |\n",
    "| df.clip()              |\n",
    "| df.replace()           |\n",
    "| pd.cut()               |\n",
    "| df.drop_duplicates()   |\n",
    "| df.drop_duplicates()   |\n",
    "| df.fillna()            |\n",
    "| df.merge()             |\n",
    "| df.pivot()             |\n",
    "| df.rename()            |\n",
    "| df.query()             |\n",
    "| df.dropna(axis=0, inplace=True)  |\n",
    "| df['column_name'].interpolate(inplace=True) |\n",
    "| os.listdir('data')     |\n",
    "| N/A                    |\n",
    "\n",
    "| Data Wrangling          |\n",
    "| ------------------------|\n",
    "| df.melt()               |\n",
    "| df.pivot()              |\n",
    "| df.stack()              |\n",
    "| df.unstack()            |\n",
    "| df.astype()             |\n",
    "| df.apply()              |\n",
    "| df.groupby()            |\n",
    "| pd.crosstab()           |\n",
    "| pd.concat()             |\n",
    "| df.replace()            |\n",
    "| df.rename()             |\n",
    "| df.sort_values()        |\n",
    "| df.transform()          |\n",
    "\n",
    "| Data Exploration         |\n",
    "| -------------------------|\n",
    "| df.info()                |\n",
    "| df.describe()            |\n",
    "| df.describe()            |\n",
    "| df.value_counts()        |\n",
    "| N/A                      |\n",
    "| N/A                      |\n",
    "| N/A                      |\n",
    "| N/A                      |\n",
    "| N/A                      |\n",
    "| N/A                      |\n",
    "| N/A                      |\n",
    "| N/A                      |\n",
    "| N/A                      |\n",
    "| N/A                      |\n",
    "| N/A                      |\n",
    "\n",
    "| Data Visualization       |\n",
    "| -------------------------|\n",
    "| plotly.plot()            |\n",
    "| seaborn.plot()           |\n",
    "| ggplot.plot()            |\n",
    "| matplotlib.plot()        |\n",
    "| plot.plot()              |\n",
    "| holoviews.plot()         |\n",
    "| bokeh.plot()             |\n",
    "| plotly.plot()            |\n",
    "| seaborn.plot()           |\n",
    "| N/A                      |\n",
    "| N/A                      |\n",
    "| N/A                      |\n",
    "\n",
    "| Data Transformation       |\n",
    "| --------------------------|\n",
    "| df.log()                  |\n",
    "| df.exp()                  |\n",
    "| df.scale()                |\n",
    "| df.normalize()            |\n",
    "| df.sqrt()                 |\n",
    "| df.abs()                  |\n",
    "| df.abs()                  |\n",
    "| df.divide()               |\n",
    "| df.subtract()             |\n",
    "| df.multiply()             |\n",
    "| df.add()                  |\n",
    "| df.expit()                |\n",
    "| df.cumsum()               |\n",
    "| N/A                       |\n",
    "\n",
    "| Feature Engineering       |\n",
    "| --------------------------|\n",
    "| df.discretize()           |\n",
    "| df.binarize()             |\n",
    "| df.dummies()              |\n",
    "| df.group_enc()            |\n",
    "| df.interactions()         |\n",
    "| df.pca()/df.ica()         |\n",
    "| df.non-linear_trans()     |\n",
    "| df.scaling/discretize()   |\n",
    "| df.rolling_window()       |\n",
    "| N/A                       |\n",
    "\n",
    "| Time Series Analysis       |\n",
    "| --------------------------|\n",
    "| df.pct()                  |\n",
    "| df.shift()                |\n",
    "| df.count()                |\n",
    "| df.rank()                 |\n",
    "| df.zscore()               |\n",
    "| df.frequency()            |\n",
    "| df.rolling()              |\n",
    "| df.autocorr()/df.pacf()   |\n",
    "| N/A                       |\n",
    "| N/A                       |\n",
    "| df.pct_change()           |\n",
    "| df.forecast()             |\n",
    "| N/A                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning:\n",
    "df.dropna()\n",
    "df.fillna()\n",
    "df.clip()\n",
    "df.replace()\n",
    "df.drop_duplicates()\n",
    "df.drop_duplicates()\n",
    "df.fillna()\n",
    "df.merge()\n",
    "df.pivot()\n",
    "df.rename()\n",
    "df.query()\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df['column_name'].interpolate(inplace=True)\n",
    "os.listdir('data')\n",
    "\n",
    "Data Wrangling:\n",
    "df.melt()\n",
    "df.pivot()\n",
    "df.stack()\n",
    "df.unstack()\n",
    "df.astype()\n",
    "df.get()\n",
    "df.apply()\n",
    "pd.crosstab()\n",
    "pd.concat()\n",
    "df.rename()\n",
    "df.replace()\n",
    "df.sort_values()\n",
    "df.transform()\n",
    "\n",
    "Data Exploration:\n",
    "df.info()\n",
    "df.describe()\n",
    "df.value_counts()\n",
    "\n",
    "Data Visualization:\n",
    "plotly.plot()\n",
    "seaborn.plot()\n",
    "bokeh.plot()\n",
    "ggplot.plot()\n",
    "matplotlib.plot()\n",
    "plot.plot()\n",
    "holoviews.plot()\n",
    "\n",
    "Data Transformation:\n",
    "df.log()\n",
    "df.exp()\n",
    "df.normalize()\n",
    "df.scale()\n",
    "df.power()\n",
    "df.abs()\n",
    "df.rescale()\n",
    "df.subtract()\n",
    "df.divide()\n",
    "df.add()\n",
    "df.expit()\n",
    "df.cumsum()\n",
    "\n",
    "Feature Engineering:\n",
    "df.discretize()\n",
    "df.binarize()\n",
    "df.rank()\n",
    "df.dummies()\n",
    "df.group_enc()\n",
    "df.interactions()\n",
    "df.frequency()\n",
    "df.scaling/discretize()\n",
    "df.non-linear_trans()\n",
    "\n",
    "Time Series Analysis:\n",
    "df.pct()\n",
    "df.shift()\n",
    "df.diff()\n",
    "df.pct_change()\n",
    "df.autocorr()/df.pacf()\n",
    "df.rolling()\n",
    "df.forecast()\n",
    "df.rolling_window()\n",
    "\n",
    "Statistical Analysis:\n",
    "df.describe()\n",
    "df.skew()\n",
    "df.kurtosis()\n",
    "df.corr()\n",
    "stats.ttest_ind()\n",
    "statsmodels.stats.ztest()\n",
    "stats.chi2_contingency()\n",
    "stats.f_oneway()\n",
    "statsmodels.formula.api.ols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning:\n",
    "df.dropna()\n",
    "df.fillna()\n",
    "df.clip()\n",
    "df.replace()\n",
    "df.drop_duplicates()\n",
    "df.drop_duplicates()\n",
    "df.fillna()\n",
    "df.merge()\n",
    "df.pivot()\n",
    "df.rename()\n",
    "df.query()\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df['column_name'].interpolate(inplace=True)\n",
    "os.listdir('data')\n",
    "\n",
    "Data Wrangling:\n",
    "df.melt()\n",
    "df.pivot()\n",
    "df.stack()\n",
    "df.unstack()\n",
    "df.astype()\n",
    "df.get()\n",
    "df.apply()\n",
    "pd.crosstab()\n",
    "pd.concat()\n",
    "df.rename()\n",
    "df.replace()\n",
    "df.sort_values()\n",
    "df.transform()\n",
    "\n",
    "Data Exploration:\n",
    "df.info()\n",
    "df.describe()\n",
    "df.value_counts()\n",
    "\n",
    "Data Visualization:\n",
    "plotly.plot()\n",
    "seaborn.plot()\n",
    "bokeh.plot()\n",
    "ggplot.plot()\n",
    "matplotlib.plot()\n",
    "plot.plot()\n",
    "holoviews.plot()\n",
    "\n",
    "Data Transformation:\n",
    "df.log()\n",
    "df.exp()\n",
    "df.normalize()\n",
    "df.scale()\n",
    "df.power()\n",
    "df.abs()\n",
    "df.rescale()\n",
    "df.subtract()\n",
    "df.divide()\n",
    "df.add()\n",
    "df.expit()\n",
    "df.cumsum()\n",
    "\n",
    "Feature Engineering:\n",
    "df.discretize()\n",
    "df.binarize()\n",
    "df.rank()\n",
    "df.dummies()\n",
    "df.group_enc()\n",
    "df.interactions()\n",
    "df.frequency()\n",
    "df.scaling/discretize()\n",
    "df.non-linear_trans()\n",
    "\n",
    "Time Series Analysis:\n",
    "df.pct()\n",
    "df.shift()\n",
    "df.diff()\n",
    "df.pct_change()\n",
    "df.autocorr()/df.pacf()\n",
    "df.rolling()\n",
    "df.forecast()\n",
    "df.rolling_window()\n",
    "\n",
    "Statistical Analysis:\n",
    "df.describe()\n",
    "df.skew()\n",
    "df.kurtosis()\n",
    "df.corr()\n",
    "stats.ttest_ind()\n",
    "statsmodels.stats.ztest()\n",
    "stats.chi2_contingency()\n",
    "stats.f_oneway()\n",
    "statsmodels.formula.api.ols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions and techniques:\n",
    "Data Cleaning:\n",
    "df.dropna()\n",
    "df.fillna()\n",
    "df.clip()\n",
    "df.replace()\n",
    "df.drop_duplicates()\n",
    "df.drop_duplicates()\n",
    "df.fillna()\n",
    "df.merge()\n",
    "df.pivot()\n",
    "df.rename()\n",
    "df.query()\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df['column_name'].interpolate(inplace=True)\n",
    "os.listdir('data')\n",
    "Data Wrangling:\n",
    "df.melt()\n",
    "df.pivot()\n",
    "df.stack()\n",
    "df.unstack()\n",
    "df.astype()\n",
    "df.get()\n",
    "df.apply()\n",
    "pd.crosstab()\n",
    "pd.concat()\n",
    "df.rename()\n",
    "df.replace()\n",
    "df.sort_values()\n",
    "df.transform()\n",
    "Data Exploration:\n",
    "df.info()\n",
    "df.describe()\n",
    "df.value_counts()\n",
    "Data Visualization:\n",
    "plotly.plot()\n",
    "seaborn.plot()\n",
    "bokeh.plot()\n",
    "ggplot.plot()\n",
    "matplotlib.plot()\n",
    "plot.plot()\n",
    "holoviews.plot()\n",
    "Data Transformation:\n",
    "df.log()\n",
    "df.exp()\n",
    "df.normalize()\n",
    "df.scale()\n",
    "df.power()\n",
    "df.abs()\n",
    "df.rescale()\n",
    "df.subtract()\n",
    "df.divide()\n",
    "df.add()\n",
    "df.expit()\n",
    "df.cumsum()\n",
    "Feature Engineering:\n",
    "df.discretize()\n",
    "df.binarize()\n",
    "df.rank()\n",
    "df.dummies()\n",
    "df.group_enc()\n",
    "df.interactions()\n",
    "df.frequency()\n",
    "df.scaling/discretize()\n",
    "df.non-linear_trans()\n",
    "Time Series Analysis:\n",
    "df.pct()\n",
    "df.shift()\n",
    "df.diff()\n",
    "df.pct_change()\n",
    "df.autocorr()/df.pacf()\n",
    "df.rolling()\n",
    "df.forecast()\n",
    "df.rolling_window()\n",
    "Statistical Analysis:\n",
    "df.describe()\n",
    "df.skew()\n",
    "df.kurtosis()\n",
    "df.corr()\n",
    "stats.ttest_ind()\n",
    "statsmodels.stats.ztest()\n",
    "stats.chi2_contingency()\n",
    "stats.f_oneway()\n",
    "statsmodels.formula.api.ols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"The os module in Python provides a way of interacting with the operating \n",
    "system. It provides various functions to work with file systems,\n",
    "directories, processes, and environment variables. Here are some \n",
    "of the commonly used functions in the os module#\n",
    "\"\"\"\n",
    "os.name\n",
    "os.getcwd()\n",
    "os.chdir(path)\n",
    "os.mkdir(path)\n",
    "os.makedirs(path)\n",
    "os.rmdir(path)\n",
    "os.removedirs(path)\n",
    "os.listdir(path)\n",
    "os.path.join(path, *paths)\n",
    "os.path.exists(path)\n",
    "os.path.isfile(path)\n",
    "os.path.isdir(path)\n",
    "os.path.basename(path)\n",
    "os.path.dirname(path)\n",
    "os.path.splitext(path)\n",
    "os.rename(src, dst)\n",
    "os.remove(path)\n",
    "os.system(command)\n",
    "os.name# Returns os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "1012.78px",
    "width": "507.775px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents (Clickable in sidebar)",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "1166.7px",
    "left": "1576px",
    "top": "1389.19px",
    "width": "497px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "890563eb1401dd7c5eac482b2070a231034cb0eabe59bf1a3eb86f9e36919f52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
