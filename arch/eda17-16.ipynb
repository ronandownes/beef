{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents (Clickable in sidebar)<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Modules-and--libraries--\" data-toc-modified-id=\"Modules-and--libraries---1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Modules and  libraries  <a id=\"libraries\" rel=\"nofollow\"></a></a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis-(EDA)\" data-toc-modified-id=\"Exploratory-Data-Analysis-(EDA)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Exploratory Data Analysis (EDA)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Aggregate-Concatenate-and--Split\" data-toc-modified-id=\"Aggregate-Concatenate-and--Split-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Aggregate Concatenate and  Split</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transform\" data-toc-modified-id=\"Transform-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Transform</a></span></li><li><span><a href=\"#Combine-Files\" data-toc-modified-id=\"Combine-Files-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Combine Files</a></span></li><li><span><a href=\"#CCKP-Data--and-an-initial-bit-of-EDA\" data-toc-modified-id=\"CCKP-Data--and-an-initial-bit-of-EDA-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>CCKP Data  and an initial bit of EDA</a></span></li><li><span><a href=\"#Rain-Preparation\" data-toc-modified-id=\"Rain-Preparation-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Rain Preparation</a></span></li><li><span><a href=\"#Temp-°C--Preparation\" data-toc-modified-id=\"Temp-°C--Preparation-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Temp °C  Preparation</a></span></li></ul></li></ul></li><li><span><a href=\"#Supplementary-Functions\" data-toc-modified-id=\"Supplementary-Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Supplementary Functions</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Split\" data-toc-modified-id=\"Split-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Split</a></span></li><li><span><a href=\"#Key\" data-toc-modified-id=\"Key-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Key</a></span></li></ul></li><li><span><a href=\"#Text-Styles-and-counts\" data-toc-modified-id=\"Text-Styles-and-counts-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Text Styles and counts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Snake\" data-toc-modified-id=\"Snake-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Snake</a></span></li><li><span><a href=\"#Pascal\" data-toc-modified-id=\"Pascal-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Pascal</a></span></li><li><span><a href=\"#Camel\" data-toc-modified-id=\"Camel-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Camel</a></span></li><li><span><a href=\"#Title\" data-toc-modified-id=\"Title-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Title</a></span></li><li><span><a href=\"#Word-count\" data-toc-modified-id=\"Word-count-3.1.5\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span>Word count</a></span></li><li><span><a href=\"#Total-Character\" data-toc-modified-id=\"Total-Character-3.1.6\"><span class=\"toc-item-num\">3.1.6&nbsp;&nbsp;</span>Total Character</a></span></li></ul></li><li><span><a href=\"#Scanners,--Readers-and-Writers\" data-toc-modified-id=\"Scanners,--Readers-and-Writers-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Scanners,  Readers and Writers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scanners\" data-toc-modified-id=\"Scanners-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Scanners</a></span></li><li><span><a href=\"#Readers\" data-toc-modified-id=\"Readers-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Readers</a></span></li><li><span><a href=\"#Splitters\" data-toc-modified-id=\"Splitters-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Splitters</a></span></li><li><span><a href=\"#Delete\" data-toc-modified-id=\"Delete-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Delete</a></span></li><li><span><a href=\"#Rename\" data-toc-modified-id=\"Rename-3.2.5\"><span class=\"toc-item-num\">3.2.5&nbsp;&nbsp;</span>Rename</a></span></li><li><span><a href=\"#Replace\" data-toc-modified-id=\"Replace-3.2.6\"><span class=\"toc-item-num\">3.2.6&nbsp;&nbsp;</span>Replace</a></span></li><li><span><a href=\"#tabler\" data-toc-modified-id=\"tabler-3.2.7\"><span class=\"toc-item-num\">3.2.7&nbsp;&nbsp;</span>tabler</a></span></li></ul></li><li><span><a href=\"#Filters\" data-toc-modified-id=\"Filters-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Filters</a></span><ul class=\"toc-item\"><li><span><a href=\"#EU-27\" data-toc-modified-id=\"EU-27-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>EU 27</a></span></li><li><span><a href=\"#Now-or-Never!\" data-toc-modified-id=\"Now-or-Never!-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Now or Never!</a></span></li></ul></li></ul></li><li><span><a href=\"#Appendices\" data-toc-modified-id=\"Appendices-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Appendices</a></span><ul class=\"toc-item\"><li><span><a href=\"#FAOSTAT-Data-Domains\" data-toc-modified-id=\"FAOSTAT-Data-Domains-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>FAOSTAT Data Domains</a></span></li></ul></li><li><span><a href=\"#Climate--Data-Preparation\" data-toc-modified-id=\"Climate--Data-Preparation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Climate  Data Preparation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#About-FAOSTAT\" data-toc-modified-id=\"About-FAOSTAT-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;</span>About FAOSTAT</a></span></li><li><span><a href=\"#Licencing\" data-toc-modified-id=\"Licencing-5.0.2\"><span class=\"toc-item-num\">5.0.2&nbsp;&nbsp;</span>Licencing</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and  comparison of Ireland's beef sector with other  EU contries\n",
    "\n",
    "Final name 1_Data_Exploration\n",
    "\n",
    "\n",
    "## Introduction \n",
    "\n",
    "![image1](../images/tu04.png)\n",
    "\n",
    "<span style=\"font-size: 24px;\">                 </span>\n",
    "\n",
    "Between €28.5bn and €32.6bn of the European Union's subsidies for farmers supports the livestock sector, [Greenpeace said in a new report published Tuesday (12 February)]( https://storage.googleapis.com/planet4-eu-unit-stateless/2019/02/83254ee1-190212-feeding-the-problem-dangerous-intensification-of-animal-farming-in-europe.pdf).\n",
    "That estimate amounts to between 18 and 20 percent of the entire EU budget. \n",
    "\n",
    "Beef production analysis and comparison using machine learning (ML) techniques then provides valuable insights into the industry here in Ireland. Forecasting prices and fertilizer demand, as well as understanding the sentiment of key stakeholders are all provided  as well as an  exposition and justification for the methodologies used throughout.\n",
    "\n",
    "The Department of Agriculture, Food and the Marine is responsible for implementing The Common Agricultural Policy (CAP) and managing the various programs and initiatives available to Irish beef farmers. For this reason the departments [The CAP Strategic Plan 2023 -2027](https://www.gov.ie/en/publication/76026-common-agricultural-policy-cap-post-2020/#irelands-cap-strategic-plan-2023-2027) documents were used as motivation to discover data with the most significant statistical impact.\n",
    "\n",
    "\n",
    "\n",
    "### Modules and  libraries  <a id=\"libraries\"></a>\n",
    "\n",
    "This  code block imports various Python libraries  used for data analysis and visualization, including Pandas, Matplotlib, Seaborn, Scikit-Learn, and others. It also sets some options for data wrangling, visualization, and controlling warnings. Additionally, it imports a few specific functions and classes from some of these libraries, such as GridSearchCV from Scikit-Learn and ks_2samp from Scipy. Finally, it sets a color variable to be used in plotting. It's visibility can be toggled on and off woth the button underneath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard library imports\n",
    "# Third-party imports\n",
    "# Local imports\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from functools import partial, reduce\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import fancyimpute\n",
    "import html\n",
    "import matplotlib.axes\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML, Image, display\n",
    "from countrygroups import EUROPEAN_UNION\n",
    "from countryinfo import CountryInfo\n",
    "from scipy.stats import ks_2samp, shapiro\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the folder '../data/raw':\n",
      "cattle_stocks_EU27_1961_1922.csv     193128 2023-02-17 15:06\n"
     ]
    }
   ],
   "source": [
    "scanr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/raw/cattle_stocks_EU27_1961_1922.csv .csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9104\\3134231871.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreadr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cattle_stocks_EU27_1961_1922.csv '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'df'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9104\\3134819734.py\u001b[0m in \u001b[0;36mreadr\u001b[1;34m(filename, df_name)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \"\"\"\n\u001b[0;32m     13\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../data/raw/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/raw/cattle_stocks_EU27_1961_1922.csv .csv'"
     ]
    }
   ],
   "source": [
    "readr('cattle_stocks_EU27_1961_1922.csv ','df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Concatenate and  Split \n",
    "\n",
    "####  Transform \n",
    "\n",
    "The transform function reads in a CSV file, renames a column, and filters the resulting dataframe to only include the 'key' and specified column. It returns the resulting dataframe after renaming the specified column and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(path: str, filename: str, col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads in a CSV file from the given directory, renames a column to the given column name, and\n",
    "    filters the resulting dataframe to only include the 'key' and specified column.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The relative path of the directory containing the data.\n",
    "        filename (str): The name of the CSV file to read in.\n",
    "        col_name (str): The name to assign to the specified column in the resulting dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The resulting dataframe after renaming the specified column and filtering\n",
    "        to only include the 'key' and specified column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = os.path.join(path, filename)\n",
    "        df = pd.read_csv(file_path, on_bad_lines='skip', skiprows=1)\n",
    "        df.rename(columns={'Unnamed: 0': 'Year'}, inplace=True)\n",
    "        df['key'] = df.columns[1] + df['Year'].astype(str)\n",
    "        df.rename(columns={df.columns[1]: col_name}, inplace=True)\n",
    "        df = df.filter(['key', col_name])\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading in CSV file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  Combine Files\n",
    "\n",
    "The combine function is designed to read in all  the **precicipitation** or **mean-temperature** CSV files  from either the 'pr' or 'ts' folders, apply the transform function to each file to rename a column with the country the data relates to and filter at the superregional level. It then combines    all the the resulting dataframes together into a single dataframe for all the 27 EU counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(path: str, subfolder: str, col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads in all CSV files in the given subfolder of the directory, applies the 'transform' function to each file, and\n",
    "    combines the resulting dataframes together into a single dataframe.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The relative path of the directory containing the data.\n",
    "        subfolder (str): The name of the subfolder within the directory to read the CSV files from.\n",
    "        col_name (str): The name to assign to the specified column in the resulting dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The resulting dataframe after combining data from all CSV files in the\n",
    "        specified subfolder.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        folder_path = os.path.join(path, subfolder)\n",
    "        csv_filenames = glob.glob(folder_path + \"/*.csv\")\n",
    "        processed_dfs = (transform(path, os.path.join(subfolder, os.path.basename(FileName)), col_name) for FileName in csv_filenames)\n",
    "        df = pd.concat(processed_dfs, ignore_index=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining CSV files in folder {folder_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "####   CCKP Data  and an initial bit of EDA\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <img src=\"../images/we28.png\" alt=\"image19\" style=\"width: 30%;\">\n",
    "  <img src=\"../images/we29.png\" alt=\"image20\" style=\"width: 30%;\">\n",
    "    <img src=\"../images/we30.png\" alt=\"image20\" style=\"width: 30%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size: 18px;\">Figure 3:Time Series Data downloaded for 27 EU countries from  CCKP site </span>\n",
    "\n",
    "\n",
    "To maintain consistency with the FAO data, annual and not monthly time series  aggregates were taken from the Climatic Research Unit (CRU) dataset for **precipitation** and   **mean-temperature**. These datasets are provided by the CRU TS 4.04 dataset, a gridded climate dataset produced by the Climatic Research Unit (CRU) at the University of East Anglia in the United Kingdom. In the statistics section, range, variance, and standard deviation of monthly data may be revisited for insights.\n",
    "\n",
    "The file names and folder names of the CCKP data used in this project are tabulated below.\n",
    "\n",
    "<span style=\"font-size: 24px;\">Table : Table shows Time Series data filenames and folders  for 27 EU countries from  CCKP site </span>\n",
    "\n",
    "\n",
    "\n",
    "| Country        | Code | TasAnnual (Folder with Tempature data)                              | PrAnnual   (Folder with Precipitation Data)                        |\n",
    "|:--------------:|:----:|:--------------------------------------:|:---------------------------------:|\n",
    "| Albania        | ALB  | tas_timeseries_annual_cru_1901-2021_ALB.csv | pr_timeseries_annual_cru_1901-2021_ALB.csv |\n",
    "| Andorra        | AND  | tas_timeseries_annual_cru_1901-2021_AND.csv | pr_timeseries_annual_cru_1901-2021_AND.csv |\n",
    "| Austria        | AUT  | tas_timeseries_annual_cru_1901-2021_AUT.csv | pr_timeseries_annual_cru_1901-2021_AUT.csv |\n",
    "| Belarus        | BLR  | tas_timeseries_annual_cru_1901-2021_BLR.csv | pr_timeseries_annual_cru_1901-2021_BLR.csv |\n",
    "| Belgium        | BEL  | tas_timeseries_annual_cru_1901-2021_BEL.csv | pr_timeseries_annual_cru_1901-2021_BEL.csv |\n",
    "| Bosnia and Herzegovina | BIH  | tas_timeseries_annual_cru_1901-2021_BIH.csv | pr_timeseries_annual_cru_1901-2021_BIH.csv |\n",
    "| Bulgaria       | BGR  | tas_timeseries_annual_cru_1901-2021_BGR.csv | pr_timeseries_annual_cru_1901-2021_BGR.csv |\n",
    "| Croatia        | HRV  | tas_timeseries_annual_cru_1901-2021_HRV.csv | pr_timeseries_annual_cru_1901-2021_HRV.csv |\n",
    "| Cyprus         | CYP  | tas_timeseries_annual_cru_1901-2021_CYP.csv | pr_timeseries_annual_cru_1901-2021_CYP.csv |\n",
    "| Czech Republic | CZE  | tas_timeseries_annual_cru_1901-2021_CZE.csv | pr_timeseries_annual_cru_1901-2021_CZE.csv |\n",
    "| Denmark        | DNK  | tas_timeseries_annual_cru_1901-2021_DNK.csv | pr_timeseries_annual_cru_1901-2021_DNK.csv |\n",
    "| Estonia        | EST  | tas_timeseries_annual_cru_1901-2021_EST.csv | pr_timeseries_annual_cru_1901-2021_EST.csv |\n",
    "| Finland        | FIN  | tas_timeseries_annual_cru_1901-2021_FIN.csv | pr_timeseries_annual_cru_1901-2021_FIN.csv |\n",
    "| France         | FRA  | tas_timeseries_annual_cru_1901-2021_FRA.csv | pr_timeseries_annual_cru_1901-2021_FRA.csv |\n",
    "| Germany        | DEU  | tas_timeseries_annual_cru_1901-2021_DEU.csv | pr_timeseries_annual_cru_1901-2021_DEU.csv |\n",
    "| Gibraltar      | GIB  | tas_timeseries_annual_cru_1901-2021_GIB.csv | pr_timeseries_annual_cru_1901-2021_GIB.csv |\n",
    "| Greece | GRC | tas_timeseries_annual_cru_1901-2021_GRC.csv | pr_timeseries_annual_cru_1901-2021_GRC.csv |\n",
    "| Croatia | HRV | tas_timeseries_annual_cru_1901-2021_HRV.csv | pr_timeseries_annual_cru_1901-2021_HRV.csv |\n",
    "| Hungary | HUN | tas_timeseries_annual_cru_1901-2021_HUN.csv | pr_timeseries_annual_cru_1901-2021_HUN.csv |\n",
    "| Ireland | IRL | tas_timeseries_annual_cru_1901-2021_IRL.csv | pr_timeseries_annual_cru_1901-2021_IRL.csv |\n",
    "| Italy | ITA | tas_timeseries_annual_cru_1901-2021_ITA.csv | pr_timeseries_annual_cru_1901-2021_ITA.csv |\n",
    "| Lithuania | LTU | tas_timeseries_annual_cru_1901-2021_LTU.csv | pr_timeseries_annual_cru_1901-2021_LTU.csv |\n",
    "| Luxembourg | LUX | tas_timeseries_annual_cru_1901-2021_LUX.csv | pr_timeseries_annual_cru_1901-2021_LUX.csv |\n",
    "| Latvia | LVA | tas_timeseries_annual_cru_1901-2021_LVA.csv | pr_timeseries_annual_cru_1901-2021_LVA.csv |\n",
    "| Malta | MLT | tas_timeseries_annual_cru_1901-2021_MLT.csv | pr_timeseries_annual_cru_1901-2021_MLT.csv |\n",
    "| Netherlands | NLD | tas_timeseries_annual_cru_1901-2021_NLD.csv | pr_timeseries_annual_cru_1901-2021_NLD.csv |\n",
    "| Poland | POL | tas_timeseries_annual_cru_1901-2021_POL.csv | pr_timeseries_annual_cru_1901-2021_POL.csv |\n",
    "| Portugal | PRT | tas_timeseries_annual_cru_1901-2021_PRT.csv | pr_timeseries_annual_cru_1901-2021_PRT.csv |\n",
    "| Romania | ROU | tas_timeseries_annual_cru_1901-2021_ROU.csv | pr_timeseries_annual_cru_1901-2021_ROU.csv |\n",
    "| Slovakia | SVK | tas_timeseries_annual_cru_1901-2021_SVK.csv | pr_timeseries_annual_cru_1901-2021_SVK.csv |\n",
    "| Slovenia | SVN | tas_timeseries_annual_cru_1901-2021_SVN.csv | pr_timeseries_annual_cru_1901-2021_SVN.csv |\n",
    "| Sweden | SWE | tas_timeseries_annual_cru_1901-2021_SWE.csv | pr_timeseries_annual_cru_1901-2021_SWE.csv |\n",
    "\n",
    "\n",
    "All datasets from the CCKP are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO (CC BY-NC-SA 3.0 IGO). \n",
    "Source: CCKP (2023). Time Series datasets. Retrieved from [https://climateknowledgeportal.worldbank.org/download-data]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rain Preparation\n",
    "\n",
    "All 27 Rain CSVs are processed into rain.df and rain.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>Rain_mm/yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria1901</td>\n",
       "      <td>1052.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austria1902</td>\n",
       "      <td>1061.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria1903</td>\n",
       "      <td>1201.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austria1904</td>\n",
       "      <td>1146.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austria1905</td>\n",
       "      <td>1127.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           key  Rain_mm/yr\n",
       "0  Austria1901     1052.84\n",
       "1  Austria1902     1061.55\n",
       "2  Austria1903     1201.34\n",
       "3  Austria1904     1146.14\n",
       "4  Austria1905     1127.85"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the 'pr' data from the '../data' directory and create a dataframe 'rain_df'\n",
    "# with the 'Rain_mm/yr' data. \n",
    "rain_df = combine('../data', 'pr', 'Rain_mm/yr')\n",
    "\n",
    "# Display the first few rows of the 'rain_df' dataframe.\n",
    "rain_df.head()\n",
    "\n",
    "# Save the 'rain_df' dataframe to a CSV file in the '../data/processed' directory, \n",
    "# with the filename 'rain.csv', and exclude the index column from the output.\n",
    "rain_df.to_csv('../data/processed/rain.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the 'rain_df' dataframe again.\n",
    "rain_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temp °C  Preparation\n",
    "\n",
    "All 27 'pr_timeseries_annual_cru' and 27 'tas_timeseries_annual_cru' files will be processed into 'rain.df' &'rain.csv' and 'temp.df' & 'temp.csv' accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>Temp °C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria1901</td>\n",
       "      <td>5.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austria1902</td>\n",
       "      <td>5.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria1903</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austria1904</td>\n",
       "      <td>6.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austria1905</td>\n",
       "      <td>5.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           key  Temp °C\n",
       "0  Austria1901     5.41\n",
       "1  Austria1902     5.34\n",
       "2  Austria1903     5.88\n",
       "3  Austria1904     6.22\n",
       "4  Austria1905     5.79"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the 'tas' data from the '../data' directory and create a dataframe 'temp_df'\n",
    "# with the 'Temp °C' data. \n",
    "temp_df = combine('../data', 'tas', 'Temp \\u00B0C')\n",
    "\n",
    "# Display the first few rows of the 'temp_df' dataframe.\n",
    "temp_df.head()\n",
    "\n",
    "# Save the 'temp_df' dataframe to a CSV file in the '../data/processed' directory, \n",
    "# with the filename 'temp.csv', and exclude the index column from the output.\n",
    "temp_df.to_csv('../data/processed/temp.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the 'temp_df' dataframe again.\n",
    "temp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the folder '../data/processed':\n",
      "cover.csv                         2141160 2023-02-16 12:23\n",
      "main.csv                            56353 2023-02-16 03:53\n",
      "missing.csv                          3630 2023-02-16 04:55\n",
      "production.csv                    1173771 2023-02-17 15:10\n",
      "rain.csv                            65686 2023-02-17 15:11\n",
      "stocks.csv                        1438299 2023-02-17 15:10\n",
      "temp.csv                            59959 2023-02-17 15:11\n"
     ]
    }
   ],
   "source": [
    "scanp() # scans the processed folder in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 28px\">This all looks great!</h1>\n",
    "\n",
    "Merging the rain_df and temp_df dataframes with our main beef data based on the country year key will be performed later using the Pandas merge() method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Functions\n",
    "\n",
    "We write functions for operations that are commonly repeated or have a well-defined purpose. It took me a lot of time but this modular strategy is more organized, easier to read, and less prone to errors overall. On the other hand, they introduce an added administrative burden and can leave code harder to follow and disjointed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### Split\n",
    "The split function takes a pandas DataFrame df and a column name col as input, and returns a dictionary where each key corresponds to a unique value in the specified column, and each value is a DataFrame containing all rows with that unique value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df: pd.DataFrame, col: str) -> dict:\n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to split.\n",
    "        col (str): The name of the column to group by.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where each key corresponds to a unique value in the specified column,\n",
    "        and each value is a DataFrame containing all rows with that unique value.\n",
    "    \"\"\"\n",
    "    dfs = dict(tuple(df.groupby(col)))\n",
    "    return dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def key(df, area_col='Area', year_col='Year', key_col='key'):\n",
    "    \"\"\"\n",
    "    Add a new column to a pandas DataFrame that concatenates the values in the 'area_col' and 'year_col' columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to add the new column to.\n",
    "    - area_col (str): The name of the column that contains the area values.\n",
    "    - year_col (str): The name of the column that contains the year values.\n",
    "    - key_col (str): The name of the new column to create.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    df[key_col] = df[area_col] + df[year_col].astype(str)\n",
    "    \n",
    "    \n",
    "    \n",
    "def add_key_column(df):\n",
    "    df['key'] = df['Area'] + df['Year'].astype(str)\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Styles and counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def snake(text):\n",
    "    \"\"\"\n",
    "    Converts a given string to snake_case by replacing any whitespace characters with underscores,\n",
    "    converting to all lowercase, and adding an underscore to the beginning if it doesn't already start with one.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The string to convert to snake_case.\n",
    "\n",
    "    Returns:\n",
    "        str: The resulting string in snake_case format.\n",
    "    \"\"\"\n",
    "    # Convert to string and strip leading/trailing whitespace\n",
    "    text = str(text).strip()\n",
    "    # Replace any whitespace characters with underscores\n",
    "    text = re.sub(r'\\s+', '_', text)\n",
    "    # Convert to all lowercase\n",
    "    text = text.lower()\n",
    "    # Add an underscore to the beginning if it doesn't already start with one\n",
    "    if not text.startswith('_'):\n",
    "        text = '_' + text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git config --global user.email \"ronandownes@gmail.com\"\n",
    "git config --global user.name \"Ronan Downes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pascal\n",
    "The 'pascal' naming function takes in a string as input and converts it to a PascalCase format. PascalCase is a naming convention where the first letter of each word is capitalized, and there are no spaces or separators between the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pascal(string):\n",
    "    \"\"\"\n",
    "    Convert a space- or snake-separated string to PascalCase.\n",
    "\n",
    "    Parameters:\n",
    "        string (str): The input string to convert to PascalCase.\n",
    "\n",
    "    Returns:\n",
    "        str: The input string in PascalCase format.\n",
    "\n",
    "    \"\"\"\n",
    "    # Replace any underscores with spaces\n",
    "    string = string.replace(\"_\", \" \")\n",
    "    # Capitalize the first letter of each word\n",
    "    words = string.title()\n",
    "    # Remove any remaining spaces\n",
    "    words = words.replace(\" \", \"\")\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Camel\n",
    "\n",
    "Camel case is a naming convention in which each word in a compound word is capitalized, except for the first word which is in lower case. It is commonly used in programming languages for naming variables and functions.\n",
    "\n",
    "handles both snake_case and space-separated strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel(string):\n",
    "    \"\"\"\n",
    "    Convert a space-separated or snake_case string to camelCase.\n",
    "\n",
    "    Parameters:\n",
    "        string (str): The string to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: The converted string in camelCase.\n",
    "    \"\"\"\n",
    "    # Replace underscores with spaces and split the string into a list of words\n",
    "    words = string.replace(\"_\", \" \").split()\n",
    "    # Convert the first word to lowercase and capitalize all subsequent words\n",
    "    camel_cased = [words[0].lower()] + [word.capitalize() for word in words[1:]]\n",
    "    # Concatenate the words together and return the resulting string\n",
    "    return \"    \" + ''.join(camel_cased) + \"    \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_live_animals_cattle_stocks'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snake('live animals cattle stocks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title \n",
    "The 'title' naming convention is where each word starts with a capital letter, except for prepositions and conjunctions, which start with a lowercase letter. \n",
    "The function takes a string as input and converts it to title case, where the first letter of each non-conjunction/preposition word is capitalized, and all other letters are lowercase. It achieves this by splitting the input string into a list of words, identifying which words are prepositions or conjunctions based on a predefined list, and then capitalizing the first letter of all other words while converting prepositions and conjunctions to lowercase. The resulting list of processed words is then joined back into a single string with proper spacing and returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title(sentence):\n",
    "    \"\"\"\n",
    "    Takes a string and converts it to title case, where the first letter of each\n",
    "    non-conjunction/preposition word is capitalized, and all other letters are lowercase.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): The string to convert to title case.\n",
    "        \n",
    "    Returns:\n",
    "        str: The input string converted to title case.\n",
    "    \"\"\"\n",
    "    # Define a list of common prepositions and conjunctions\n",
    "    prepositions_conjunctions = ['a', 'this', 'an', 'the', 'and', 'but', 'or', 'for', 'has', 'nor', 'on', 'at', 'to', 'from', 'by', 'over', 'under', 'in', 'out', 'of']\n",
    "    # Split the input string into a list of words\n",
    "    words = sentence.split()\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        # If the word is not a preposition or conjunction, capitalize the first letter and lowercase the rest\n",
    "        if word.lower() not in prepositions_conjunctions:\n",
    "            processed_words.append(word.capitalize())\n",
    "        # If the word is a preposition or conjunction, convert to lowercase\n",
    "        else:\n",
    "            processed_words.append(word.lower())\n",
    "    # Join the list of processed words into a single string, with proper spacing\n",
    "    output = \" \".join(processed_words)\n",
    "    # Remove any leading/trailing whitespace and add some padding\n",
    "    return \"     \" + re.sub('\\s+', ' ', output.strip()) + \"     \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    \"\"\"\n",
    "    Counts the number of words in a given text.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The text to be counted.\n",
    "        \n",
    "    Returns:\n",
    "        int: The number of words in the text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_count(string):\n",
    "    \"\"\"\n",
    "    Count the number of characters in a string.\n",
    "\n",
    "    Parameters:\n",
    "    - string (str): The string to count characters in.\n",
    "\n",
    "    Returns:\n",
    "    - int: The number of characters in the string.\n",
    "    \"\"\"\n",
    "    return len(string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scanners,  Readers and Writers\n",
    "#### Scanners\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scan(folder_name):\n",
    "    \"\"\"Scans the specified folder and prints information about its contents.\n",
    "\n",
    "    Parameters:\n",
    "        folder_name (str): The name of the folder to scan.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    folder_path = os.path.join(current_dir, folder_name)\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found\")\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path\")\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_path))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            print(\"{:30} {:10} {}\".format(item, size, modified_time))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))\n",
    "# scan('../../images')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scann():\n",
    "    \"\"\"Scans the raw data folder and prints contents information.\n",
    "\n",
    "    Parameters:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        Prints out contents information of the raw data folder.\n",
    "    \"\"\"\n",
    "    folder_path = \"../notebooks\"\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found\")\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path\")\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_path))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            print(\"{:30} {:10} {}\".format(item, size, modified_time))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))\n",
    "# scann()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanr():\n",
    "    \"\"\"Scans the raw data folder and prints contents information.\n",
    "\n",
    "    Parameters:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        Prints out contents information of the raw data folder.\n",
    "    \"\"\" \n",
    "    folder_name = '../data/raw'\n",
    "    current_dir = os.getcwd()\n",
    "    folder_path = os.path.join(current_dir, folder_name)\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found\")\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path\")\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_name))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            print(\"{:30} {:10} {}\".format(item, size, modified_time))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))\n",
    "# scanr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanp():\n",
    "    \"\"\"Scans the raw data folder and prints contents information.\n",
    "\n",
    "    Parameters:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        Prints out contents information of the raw data folder.\n",
    "    \"\"\" \n",
    "    folder_name = '../data/processed'\n",
    "    current_dir = os.getcwd()\n",
    "    folder_path = os.path.join(current_dir, folder_name)\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found\")\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path\")\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_name))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            print(\"{:30} {:10} {}\".format(item, size, modified_time))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))\n",
    "# scanr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Readers\n",
    "\n",
    "'read_csv' is a function in the Pandas library for reading in CSV (Comma Separated Values) files into a DataFrame. It is a flexible function that can handle a variety of input formats, including different delimiters, encodings, and line endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(filename, df_name, folder_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the specified folder and returns a pandas DataFrame\n",
    "    with the specified name.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The name of the CSV file to be read.\n",
    "        df_name (str): The name to be assigned to the resulting DataFrame.\n",
    "        folder_path (str): The path of the folder containing the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame created from the data in the CSV file with\n",
    "        the specified name.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(folder_path, f'{filename}.csv')\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.name = df_name\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readr(filename, df_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from  the raw data folder and returns a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        filename (str): base name of CSV file to be read.\n",
    "        df_name (str): The DataFrame name to be assigned \n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame created from the data in the CSV file with\n",
    "        the specified name.\n",
    "    \"\"\"\n",
    "    file_path = '../data/raw/' + filename + '.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.name = df_name\n",
    "    return df\n",
    "# readr('land_use','land_use_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readp(filename, df_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the specified file path in the processed folder and returns a pandas DataFrame\n",
    "    with the specified name.\n",
    "    \n",
    "    Parameters:\n",
    "        filename (str): The name of the CSV file to be read.\n",
    "        df_name (str): The name to be assigned to the resulting DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame created from the data in the CSV file with\n",
    "        the specified name.\n",
    "    \"\"\"\n",
    "    file_path = f'../data/processed/{filename}.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.name = df_name\n",
    "    return df\n",
    "# df=readp('missing','missing_df')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_elementr(file_name: str) -> Tuple[Dict[str, pd.DataFrame], List[str]]:\n",
    "    \"\"\"\n",
    "    Splits a CSV file into multiple dataframes based on the unique values in the 'Element' column.\n",
    "    \n",
    "    Parameters:\n",
    "        file_name (str): The name of the CSV file to read in.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[Dict[str, pd.DataFrame], List[str]]: A tuple containing a dictionary where each key corresponds\n",
    "        to a unique value in the 'Element' column, and each value is a dataframe containing all rows with that\n",
    "        unique value, and a list of unique values in the 'Element' column.\n",
    "    \"\"\"\n",
    "    if not file_name.endswith('.csv'):\n",
    "        raise ValueError('Input file must be a .csv file')\n",
    "    \n",
    "    # Load the specified dataframe\n",
    "    df = pd.read_csv(f'../data/raw/{file_name}')\n",
    "    \n",
    "    # Group the dataframe by the 'Element' column\n",
    "    dfs = dict(tuple(df.groupby('Element')))\n",
    "    \n",
    "    # Get the unique values in the 'Element' column\n",
    "    elements = list(df['Element'].unique())\n",
    "    \n",
    "    return dfs, elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_elementr(file_name: str) -> Tuple[Dict[str, pd.DataFrame], List[str]]:\n",
    "    \"\"\"\n",
    "    Splits a CSV file into multiple dataframes based on the unique values in the 'Element' column.\n",
    "    \n",
    "    Parameters:\n",
    "        file_name (str): The name of the CSV file to read in.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[Dict[str, pd.DataFrame], List[str]]: A tuple containing a dictionary where each key corresponds\n",
    "        to a unique value in the 'Element' column, and each value is a dataframe containing all rows with that\n",
    "        unique value, and a list of unique values in the 'Element' column.\n",
    "    \"\"\"\n",
    "    # Load the specified dataframe\n",
    "    df = pd.read_csv(f'../data/raw/{file_name}')\n",
    "    \n",
    "    # Group the dataframe by the 'Element' column\n",
    "    dfs = dict(tuple(df.groupby('Element')))\n",
    "    \n",
    "    # Get the unique values in the 'Element' column\n",
    "    elements = list(df['Element'].unique())\n",
    "    \n",
    "    return dfs, elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_element(df: pd.DataFrame) -> Tuple[Dict[str, pd.DataFrame], List[str]]:\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into multiple dataframes based on the unique values in the 'Element' column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to split.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[Dict[str, pd.DataFrame], List[str]]: A tuple containing a dictionary where each key corresponds\n",
    "        to a unique value in the 'Element' column, and each value is a dataframe containing all rows with that\n",
    "        unique value, and a list of unique values in the 'Element' column.\n",
    "    \"\"\"\n",
    "    # Group the dataframe by the 'Element' column\n",
    "    dfs = dict(tuple(df.groupby('Element')))\n",
    "    \n",
    "    # Get the unique values in the 'Element' column\n",
    "    elements = list(df['Element'].unique())\n",
    "    \n",
    "    return dfs, elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupdf(df, column_name):\n",
    "    # Group the dataframe by the specified column\n",
    "    dfs = dict(tuple(df.groupby(column_name)))\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Delete\n",
    "\n",
    "In this code, we use the os module to delete a file at the specified file path. We first check if the file exists using the os.path.exists() function. If the file exists, we delete it using the os.remove() function. If the file does not exist, we print a message indicating that the file was not found.\n",
    "\n",
    "Note that this code permanently deletes the file, so you should use it with caution. Once a file is deleted, it cannot be easily recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def delete(file_name):\n",
    "    file_path = os.path.join(\"..\", \"data\", \"raw\", file_name)\n",
    "    expected_ext = \".csv\"\n",
    "    if not file_path.endswith(expected_ext):\n",
    "        print(\"Error: Invalid file extension. File extension must be .csv.\")\n",
    "    elif os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"{file_name} deleted successfully\")\n",
    "    else:\n",
    "        print(f\"{file_name} not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename\n",
    "\n",
    "This function first constructs the file_path by joining the ../data/raw directory with the given file_name. It then sets the expected file extension to .csv. The function then checks if the file_path ends with the expected file extension. If it doesn't, the function prints an error message. If it does, the function checks if the file exists at file_path, and if it does, it removes it and prints a success message. If the file doesn't exist, the function prints a \"not found\" message. like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(old_filename, new_filename):\n",
    "    old_file_path = os.path.join(\"..\", \"data\", old_filename)\n",
    "    new_file_path = os.path.join(\"..\", \"data\", new_filename)\n",
    "    try:\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "        print(f\"{old_filename} renamed to {new_filename} successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{old_filename} not found\")\n",
    "    except FileExistsError:\n",
    "        print(f\"A file with the name {new_filename} already exists\")\n",
    "    except OSError:\n",
    "        print(\"Invalid file path or name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(input_str, find_str, replace_str):\n",
    "    output_str = re.sub(find_str, replace_str, input_str)\n",
    "    \n",
    "    if output_str == input_str:\n",
    "        warnings.warn(\"Replacement unsuccessful: '{}' not found in input string.\".format(find_str))\n",
    "    \n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  tabler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  tabler\n",
    "\n",
    "\"\"\"The code defines a function tabler that generates an HTML table with information on the files in a given folder and\n",
    "adds a section header that contains the directory path \n",
    "\"\"\"\n",
    "\n",
    "def tabler(folder_path):\n",
    "    \"\"\"\n",
    "    Generate an HTML table with information on files in a given folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path to the folder containing the files.\n",
    "\n",
    "    Returns:\n",
    "    - str: The HTML code for the table.\n",
    "    \"\"\"\n",
    "    # Get the contents of the folder\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        return \"Directory not found\"\n",
    "    except OSError:\n",
    "        return \"Invalid folder path\"\n",
    "\n",
    "    # Create the section header\n",
    "    header = f\"### {folder_path}\\n\\n\"\n",
    "\n",
    "    # Create the table header\n",
    "    table = '<table style=\"font-size:100%\"><thead><tr><th>File Name</th><th>Size</th><th>Modified Time</th></tr></thead><tbody>'\n",
    "\n",
    "    # Add a row for each file\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            # Get the file size and modified time\n",
    "            size_bytes = os.path.getsize(item_path)\n",
    "            size_kb = size_bytes / 1024\n",
    "            size_str = '{:,.2f} KB'.format(size_kb)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            # Add a row to the table\n",
    "            table += '<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(item, size_str, modified_time)\n",
    "\n",
    "    # Close the table\n",
    "    table += '</tbody></table>'\n",
    "\n",
    "    return header + table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters\n",
    "Geographical and temporal filters are useful in exploratory data analysis (EDA) because they help to identify patterns and trends in data that may be difficult to discern otherwise. By limiting the data to a specific geographic or time period, it becomes easier to identify trends and patterns that might be obscured by noise in the larger dataset.\n",
    "\n",
    "####  EU 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eu(df):\n",
    "    \"\"\"\n",
    "    Filter a pandas DataFrame to include only the rows where the 'Area' column contains values that match the countries in the European Union.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to filter.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    country_list = ['Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czechia', 'Denmark', \n",
    "                    'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Ireland', 'Italy', \n",
    "                    'Latvia', 'Lithuania', 'Luxembourg', 'Malta', 'Netherlands', 'Poland', 'Portugal', 'Romania', \n",
    "                    'Slovakia', 'Slovenia', 'Spain', 'Sweden']\n",
    "    return df[df['Area'].isin(country_list)]\n",
    "\n",
    "# df=readr('meat','df')\n",
    "# df=eu(df)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Now or Never!\n",
    "df=readr('meat','df')\n",
    "df = now(df, 2015)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now(df, year_min):\n",
    "    \"\"\"\n",
    "    Filter to 'Year'>.\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to filter.\n",
    "    - year_min (int): The minimum year to include in the filtered DataFrame.\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    return df[df['Year'] >= year_min]\n",
    "\n",
    "# df=readr('meat','df')\n",
    "# df = now(df, 2015)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now2(df):\n",
    "    \"\"\"\n",
    "    Filter to 'Year' >= 2000.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to filter.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    year_min = 2000\n",
    "    return df[df['Year'] >= year_min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAOSTAT Data Domains \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../images/we16.png\" alt=\"image7\" width=\"80%\">\n",
    "\n",
    "[<span style=\"font-size: 18px;\">Figure 2:Data Domain Table view of FAOSTAT</span>\n",
    "](https://www.fao.org/faostat/en/#data/domains_table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The [FAOSTAT Data Domains](https://www.fao.org/faostat/en/#data/domains_table)  are organised as follows:\n",
    "\n",
    "  -  Production: Production of crops and livestock products, including production indices and the value of agricultural production.\n",
    "\n",
    "  - Food Security and Nutrition: Information on SDG indicators related to food security and nutrition and food balances.\n",
    "\n",
    "  - Trade: Including detailed trade matrices, trade indices, and updates on related data.\n",
    "\n",
    "  - Prices: Producer and   consumer price indices, deflators, and exchange rates.\n",
    "\n",
    "  - Land, Inputs and Sustainability:Land use, land cover, inputs, including fertilizers and manure and pesticides.\n",
    "  \n",
    "  - Population and Employment: Annual population including those specific to agriculture and rural areas.\n",
    "\n",
    "  - Investment: Government expenditure, credit to agriculture, foreign direct investment, and country investment statistics.\n",
    "\n",
    "  - Macro-Economic Indicators: Such as capital stock.\n",
    "\n",
    "  - Food Value Chain: This domain provides information on the value shares of the food industry and primary factors.\n",
    "\n",
    "  - Climate Change: Emissions, crop residues, forests, and other indicators related to climate change.\n",
    "\n",
    "  - Forestry: Forestry production and trade, as well as forestry trade flows.\n",
    "\n",
    "  - SDG Indicators: The Sustainable Development Goals (SDGs) are a set of 17 goals established by the United Nations in 2015.\n",
    "\n",
    "  - World Census of Agriculture: This domain provides structural data from agricultural censuses taken around the world\n",
    "\n",
    "  - Discontinued archives and data series: This includes data on indicators from surveys and research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climate  Data Preparation \n",
    "\n",
    "<img src=\"../images/tu03.png\" alt=\"image5\" width=\"50%\">\n",
    "<span style=\"font-size: 24px;\">                  </span>\n",
    " Only two attributes are taken from the CCKP database: Mean_Temperature and Precipitation.\n",
    "\n",
    "[The CCKP website](https://climateknowledgeportal.worldbank.org/)\n",
    " is a  resource for information on the impacts of climate change and the actions taken to address these impacts. While this is outside the remit of this project the CCKP also provides access to global data on a historical  basis for the **Mean_Temperature** and **Precipitation** at the  country-by-country level  on both monthly and yearly aggregates. While **humidity** is also a known influential predictor variable these two should have a statistically significant imapct on our predictive modelling. Spatial data is provided as a global NetCDF file, with Climatology, Timeseries and Heatplot data is provided as a CSV file.\n",
    "\n",
    "Climate conditions, such as average temperature and rainfall (precipitation ) can greatly affect the growth and health of cattle. Precipitation and temperature predictor variables were  retrieved  but they were   aggragated by country and this necessitated  the **cckp** and  **combine**  functions for data wrangling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  About FAOSTAT\n",
    "\n",
    "<img src=\"../images/tu02.png\" alt=\"image5\" width=\"50%\">\n",
    "\n",
    "<span style=\"font-size: 24px;\">                  </span>\n",
    "FAOSTAT is a comprehensive database maintained by the Food and Agriculture Organization of the United Nations (FAO), providing timely, reliable data on agriculture, food, and nutrition for over 200 countries. Its information is used to inform decision-making, policy formulation, and research in the field, covering topics such as production, trade, and fertilizer use. FAOSTAT is a valuable resource for governments, organizations, researchers, and the public, informing policy and interventions to enhance food security and reduce poverty.\n",
    "\n",
    "#### Licencing \n",
    "All datasets from the [FAOSTAT](https://www.fao.org/faostat/en/#home) are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO [(CC BY-NC-SA 3.0 IGO)](https://creativecommons.org/licenses/by-nc-sa/3.0/igo/). Source: FAOSTAT (2023). Time Series datasets.\n",
    "<img src=\"../images/tu06.png\" alt=\"image5\" width=\"50%\">\n",
    "<span style=\"font-size: 24px;\">                  </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents (Clickable in sidebar)",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "1983.67px",
    "left": "329px",
    "top": "186.219px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "890563eb1401dd7c5eac482b2070a231034cb0eabe59bf1a3eb86f9e36919f52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
