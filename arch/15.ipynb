{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents (Clickable in sidebar)<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Intoduction\" data-toc-modified-id=\"Intoduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intoduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analytical-Methods-and-Features-of-this-Report\" data-toc-modified-id=\"Analytical-Methods-and-Features-of-this-Report-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Analytical Methods and Features of this Report</a></span></li></ul></li><li><span><a href=\"#Modules-and--libraries--\" data-toc-modified-id=\"Modules-and--libraries---2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Modules and  libraries  <a id=\"libraries\" rel=\"nofollow\"></a></a></span></li><li><span><a href=\"#Custom-Functions\" data-toc-modified-id=\"Custom-Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Custom Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-'Transform'-function\" data-toc-modified-id=\"The-'Transform'-function-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>The 'Transform' function</a></span></li><li><span><a href=\"#The-'combine'--Function-Used-to-concateneate-country-by-country-climate-data\" data-toc-modified-id=\"The-'combine'--Function-Used-to-concateneate-country-by-country-climate-data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>The 'combine'  Function Used to concateneate country-by-country climate data</a></span></li><li><span><a href=\"#The--'snake'-Naming-Function-used-for-Filenames.\" data-toc-modified-id=\"The--'snake'-Naming-Function-used-for-Filenames.-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>The  'snake' Naming Function used for Filenames.</a></span></li><li><span><a href=\"#The-'pascal'-Naming-Function-Used-for-Folder-Names\" data-toc-modified-id=\"The-'pascal'-Naming-Function-Used-for-Folder-Names-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>The 'pascal' Naming Function Used for Folder Names</a></span></li><li><span><a href=\"#The-'camel'-Naming-Function-Used-For-Column-Labels\" data-toc-modified-id=\"The-'camel'-Naming-Function-Used-For-Column-Labels-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>The 'camel' Naming Function Used For Column Labels</a></span></li><li><span><a href=\"#The-'Title'-Naming-Function-converts-strings-to-Title-Case\" data-toc-modified-id=\"The-'Title'-Naming-Function-converts-strings-to-Title-Case-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>The 'Title' Naming Function converts strings to Title Case</a></span></li><li><span><a href=\"#The-'scan'-Function-Returns-a-List-of-Files-from-a-Directory\" data-toc-modified-id=\"The-'scan'-Function-Returns-a-List-of-Files-from-a-Directory-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>The 'scan' Function Returns a List of Files from a Directory</a></span></li><li><span><a href=\"#The-'read',-'readr'-and-'reade'-Functions\" data-toc-modified-id=\"The-'read',-'readr'-and-'reade'-Functions-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>The 'read', 'readr' and 'reade' Functions</a></span></li><li><span><a href=\"#The--groupcsv--function-loads-csv-data-and-groups-by-a-specified-column\" data-toc-modified-id=\"The--groupcsv--function-loads-csv-data-and-groups-by-a-specified-column-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>The  <em>groupcsv</em>  function loads csv data and groups by a specified column</a></span></li><li><span><a href=\"#The--groupdf-function\" data-toc-modified-id=\"The--groupdf-function-3.10\"><span class=\"toc-item-num\">3.10&nbsp;&nbsp;</span>The  <em>groupdf</em> function</a></span></li><li><span><a href=\"#The-delete-function\" data-toc-modified-id=\"The-delete-function-3.11\"><span class=\"toc-item-num\">3.11&nbsp;&nbsp;</span>The <em>delete</em> function</a></span></li><li><span><a href=\"#The-rename--function\" data-toc-modified-id=\"The-rename--function-3.12\"><span class=\"toc-item-num\">3.12&nbsp;&nbsp;</span>The <em>rename</em>  function</a></span></li><li><span><a href=\"#The-table2folder-function-code:\" data-toc-modified-id=\"The-table2folder-function-code:-3.13\"><span class=\"toc-item-num\">3.13&nbsp;&nbsp;</span>The table2folder function code:</a></span></li><li><span><a href=\"#The-Replace-Function\" data-toc-modified-id=\"The-Replace-Function-3.14\"><span class=\"toc-item-num\">3.14&nbsp;&nbsp;</span>The <em>Replace</em> Function</a></span></li><li><span><a href=\"#The-key-function\" data-toc-modified-id=\"The-key-function-3.15\"><span class=\"toc-item-num\">3.15&nbsp;&nbsp;</span>The <em>key</em> function</a></span></li><li><span><a href=\"#The-eu-Function\" data-toc-modified-id=\"The-eu-Function-3.16\"><span class=\"toc-item-num\">3.16&nbsp;&nbsp;</span>The <em>eu Function</em></a></span></li><li><span><a href=\"#The-Read-Rawdata-to-df-Function\" data-toc-modified-id=\"The-Read-Rawdata-to-df-Function-3.17\"><span class=\"toc-item-num\">3.17&nbsp;&nbsp;</span>The Read Rawdata to df Function</a></span></li><li><span><a href=\"#Write-df-to-csv-Function\" data-toc-modified-id=\"Write-df-to-csv-Function-3.18\"><span class=\"toc-item-num\">3.18&nbsp;&nbsp;</span>Write df to csv Function</a></span></li><li><span><a href=\"#Function-'recent'->1999\" data-toc-modified-id=\"Function-'recent'->1999-3.19\"><span class=\"toc-item-num\">3.19&nbsp;&nbsp;</span>Function 'recent' &gt;1999</a></span></li><li><span><a href=\"#The-Prepare-function-3-in-1\" data-toc-modified-id=\"The-Prepare-function-3-in-1-3.20\"><span class=\"toc-item-num\">3.20&nbsp;&nbsp;</span>The Prepare function 3 in 1</a></span></li><li><span><a href=\"#Functions-to-count---words---and--characters\" data-toc-modified-id=\"Functions-to-count---words---and--characters-3.21\"><span class=\"toc-item-num\">3.21&nbsp;&nbsp;</span>Functions to count   <em>words</em>   and  <em>characters</em></a></span></li></ul></li><li><span><a href=\"#Raw-Data-Sources-and-Licening\" data-toc-modified-id=\"Raw-Data-Sources-and-Licening-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Raw Data Sources and Licening</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Sources\" data-toc-modified-id=\"Data-Sources-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Data Sources</a></span></li><li><span><a href=\"#Data-Licencing\" data-toc-modified-id=\"Data-Licencing-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Data Licencing</a></span></li><li><span><a href=\"#FAOSTAT\" data-toc-modified-id=\"FAOSTAT-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>FAOSTAT</a></span></li><li><span><a href=\"#Table-2:-Raw-Data-files-from-FAOSTAT\" data-toc-modified-id=\"Table-2:-Raw-Data-files-from-FAOSTAT-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Table 2: Raw Data files from FAOSTAT</a></span><ul class=\"toc-item\"><li><span><a href=\"#About-FAOSTAT\" data-toc-modified-id=\"About-FAOSTAT-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>About FAOSTAT</a></span></li><li><span><a href=\"#FAOSTAT-Data-Domains\" data-toc-modified-id=\"FAOSTAT-Data-Domains-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>FAOSTAT Data Domains</a></span></li></ul></li><li><span><a href=\"#Climate-Change-Knowledge-Portal-(CCKP)\" data-toc-modified-id=\"Climate-Change-Knowledge-Portal-(CCKP)-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Climate Change Knowledge Portal (CCKP)</a></span><ul class=\"toc-item\"><li><span><a href=\"#About-the-CCKP\" data-toc-modified-id=\"About-the-CCKP-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>About the CCKP</a></span></li></ul></li><li><span><a href=\"#Predictor-variables--of-average-temperature-and-rainfall-from-the--the-CCKP-website\" data-toc-modified-id=\"Predictor-variables--of-average-temperature-and-rainfall-from-the--the-CCKP-website-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Predictor variables  of average temperature and rainfall from the  <a href=\"https://climateknowledgeportal.worldbank.org/\" rel=\"nofollow\" target=\"_blank\">the CCKP website</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#CCKP-Data\" data-toc-modified-id=\"CCKP-Data-4.6.1\"><span class=\"toc-item-num\">4.6.1&nbsp;&nbsp;</span>CCKP Data</a></span></li></ul></li><li><span><a href=\"#Keeping-Null-Values-at-Download-Stage\" data-toc-modified-id=\"Keeping-Null-Values-at-Download-Stage-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Keeping Null Values at Download Stage</a></span></li><li><span><a href=\"#Tailoring--Downloads-for-the-EU-27--Countries:-Possible-but-unwise.\" data-toc-modified-id=\"Tailoring--Downloads-for-the-EU-27--Countries:-Possible-but-unwise.-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Tailoring  Downloads for the EU 27  Countries: Possible but unwise.</a></span></li><li><span><a href=\"#Production-of-live-animals\" data-toc-modified-id=\"Production-of-live-animals-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;</span>Production of live animals</a></span></li><li><span><a href=\"#Production-Livestock-Primary-Meat-data\" data-toc-modified-id=\"Production-Livestock-Primary-Meat-data-4.10\"><span class=\"toc-item-num\">4.10&nbsp;&nbsp;</span>Production Livestock Primary Meat data</a></span></li><li><span><a href=\"#Production-Prices--Data\" data-toc-modified-id=\"Production-Prices--Data-4.11\"><span class=\"toc-item-num\">4.11&nbsp;&nbsp;</span>Production Prices  Data</a></span></li><li><span><a href=\"#Trade-Crops-and-livestock-products\" data-toc-modified-id=\"Trade-Crops-and-livestock-products-4.12\"><span class=\"toc-item-num\">4.12&nbsp;&nbsp;</span>Trade Crops and livestock products</a></span></li><li><span><a href=\"#Trade-Indices\" data-toc-modified-id=\"Trade-Indices-4.13\"><span class=\"toc-item-num\">4.13&nbsp;&nbsp;</span>Trade Indices</a></span></li><li><span><a href=\"#Producer-Prices-Annual\" data-toc-modified-id=\"Producer-Prices-Annual-4.14\"><span class=\"toc-item-num\">4.14&nbsp;&nbsp;</span>Producer Prices Annual</a></span></li><li><span><a href=\"#Consumer-Price-Indices\" data-toc-modified-id=\"Consumer-Price-Indices-4.15\"><span class=\"toc-item-num\">4.15&nbsp;&nbsp;</span>Consumer Price Indices</a></span></li><li><span><a href=\"#Price-Deflators---price_deflators.csv\" data-toc-modified-id=\"Price-Deflators---price_deflators.csv-4.16\"><span class=\"toc-item-num\">4.16&nbsp;&nbsp;</span>Price Deflators - price_deflators.csv</a></span></li><li><span><a href=\"#Price-Exchange-Rates----price_exchange_rates.csv\" data-toc-modified-id=\"Price-Exchange-Rates----price_exchange_rates.csv-4.17\"><span class=\"toc-item-num\">4.17&nbsp;&nbsp;</span>Price Exchange Rates  - price_exchange_rates.csv</a></span></li><li><span><a href=\"#Land-Use\" data-toc-modified-id=\"Land-Use-4.18\"><span class=\"toc-item-num\">4.18&nbsp;&nbsp;</span>Land Use</a></span></li><li><span><a href=\"#Land-Cover\" data-toc-modified-id=\"Land-Cover-4.19\"><span class=\"toc-item-num\">4.19&nbsp;&nbsp;</span>Land Cover</a></span></li><li><span><a href=\"#Input-Fertilizers-by-Nutrient\" data-toc-modified-id=\"Input-Fertilizers-by-Nutrient-4.20\"><span class=\"toc-item-num\">4.20&nbsp;&nbsp;</span>Input Fertilizers by Nutrient</a></span></li><li><span><a href=\"#Input-Livestock-Manure\" data-toc-modified-id=\"Input-Livestock-Manure-4.21\"><span class=\"toc-item-num\">4.21&nbsp;&nbsp;</span>Input Livestock Manure</a></span></li><li><span><a href=\"#Input-Pesticides-Use\" data-toc-modified-id=\"Input-Pesticides-Use-4.22\"><span class=\"toc-item-num\">4.22&nbsp;&nbsp;</span>Input Pesticides Use</a></span></li><li><span><a href=\"#Population-and-Employment\" data-toc-modified-id=\"Population-and-Employment-4.23\"><span class=\"toc-item-num\">4.23&nbsp;&nbsp;</span>Population and Employment</a></span></li><li><span><a href=\"#Investment-Credit-to-Agriculture\" data-toc-modified-id=\"Investment-Credit-to-Agriculture-4.24\"><span class=\"toc-item-num\">4.24&nbsp;&nbsp;</span>Investment Credit to Agriculture</a></span></li><li><span><a href=\"#Investment-Government-Expenditure\" data-toc-modified-id=\"Investment-Government-Expenditure-4.25\"><span class=\"toc-item-num\">4.25&nbsp;&nbsp;</span>Investment Government Expenditure</a></span></li></ul></li><li><span><a href=\"#Cleaning-and-Preparing\" data-toc-modified-id=\"Cleaning-and-Preparing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Cleaning and Preparing</a></span><ul class=\"toc-item\"><li><span><a href=\"#CCKP--Precipitation-and-Temperature-Data-Preparation\" data-toc-modified-id=\"CCKP--Precipitation-and-Temperature-Data-Preparation-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>CCKP  Precipitation and Temperature Data Preparation</a></span></li></ul></li><li><span><a href=\"#FAOSTAT-Response-Variable-Main-Data-Frame\" data-toc-modified-id=\"FAOSTAT-Response-Variable-Main-Data-Frame-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>FAOSTAT Response Variable Main Data Frame</a></span><ul class=\"toc-item\"><li><span><a href=\"#the-query()-method-EU-filter\" data-toc-modified-id=\"the-query()-method-EU-filter-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>the query() method EU filter</a></span></li><li><span><a href=\"#Mergable-Key-Using-Concatenate-and-Astype()-Method\" data-toc-modified-id=\"Mergable-Key-Using-Concatenate-and-Astype()-Method-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Mergable Key Using Concatenate and Astype() Method</a></span></li><li><span><a href=\"#Lineplots-of-the-top-5-producers\" data-toc-modified-id=\"Lineplots-of-the-top-5-producers-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Lineplots of the top 5 producers</a></span></li><li><span><a href=\"#EDA-code-lines-commented-out-below-and-the-distribution-of-yields--all-indicated-that-no-cleaning-or-imputation-was-required\" data-toc-modified-id=\"EDA-code-lines-commented-out-below-and-the-distribution-of-yields--all-indicated-that-no-cleaning-or-imputation-was-required-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>EDA code lines commented out below and the distribution of yields  all indicated that no cleaning or imputation was required</a></span></li></ul></li><li><span><a href=\"#The-price-a-farmer-gets-for-her-meat-ppi-should-be-an-influencial-predictor-variable\" data-toc-modified-id=\"The-price-a-farmer-gets-for-her-meat-ppi-should-be-an-influencial-predictor-variable-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>The price a farmer gets for her meat ppi should be an influencial predictor variable</a></span></li><li><span><a href=\"#Shit-happens!\" data-toc-modified-id=\"Shit-happens!-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Shit happens!</a></span></li><li><span><a href=\"#Processing-the--CCKP-weather\" data-toc-modified-id=\"Processing-the--CCKP-weather-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Processing the  CCKP weather</a></span><ul class=\"toc-item\"><li><span><a href=\"#a)-First-we-will-deal-with-the-missing-values-using-the-method-constructed-above\" data-toc-modified-id=\"a)-First-we-will-deal-with-the-missing-values-using-the-method-constructed-above-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>a) First we will deal with the missing values using the method constructed above</a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Modeling</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Results</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>References</a></span></li><li><span><a href=\"#Snippets\" data-toc-modified-id=\"Snippets-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Snippets</a></span></li><li><span><a href=\"#Appendices\" data-toc-modified-id=\"Appendices-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Appendices</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and  comparison of Ireland's beef sector with other  EU contries\n",
    "\n",
    "## Intoduction \n",
    "\n",
    "![image1](../images/tu04.png)\n",
    "\n",
    "<span style=\"font-size: 24px;\">                 </span>\n",
    "\n",
    "Between €28.5bn and €32.6bn of the European Union's subsidies for farmers supports the livestock sector, [Greenpeace said in a new report published Tuesday (12 February)]( https://storage.googleapis.com/planet4-eu-unit-stateless/2019/02/83254ee1-190212-feeding-the-problem-dangerous-intensification-of-animal-farming-in-europe.pdf).\n",
    "That estimate amounts to between 18 and 20 percent of the entire EU budget. \n",
    "\n",
    "Beef production analysis and comparison using machine learning (ML) techniques then provides valuable insights into the industry here in Ireland. Forecasting prices and fertilizer demand, as well as understanding the sentiment of key stakeholders are all provided  as well as an  exposition and justification for the methodologies used throughout.\n",
    "\n",
    "The Department of Agriculture, Food and the Marine is responsible for implementing The Common Agricultural Policy (CAP) and managing the various programs and initiatives available to Irish beef farmers. For this reason the departments [The CAP Strategic Plan 2023 -2027](https://www.gov.ie/en/publication/76026-common-agricultural-policy-cap-post-2020/#irelands-cap-strategic-plan-2023-2027) documents were used as motivation to discover data with the most significant statistical impact.\n",
    "\n",
    "###   Analytical Methods and Features of this Report \n",
    "- forecasting\n",
    "- sentiment analysis\n",
    "- Evidence based recommendations for farmers and policy makers are given.\n",
    "- Exposition of and justification for all methodologies.\n",
    "- Discussion  of choice of project management framework.\n",
    "- Regular commits of my report, code files and project repository directories were made to Github.\n",
    "- Ireland is the base line for this study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules and  libraries  <a id=\"libraries\"></a>\n",
    "\n",
    "This  code block imports various Python libraries  used for data analysis and visualization, including Pandas, Matplotlib, Seaborn, Scikit-Learn, and others. It also sets some options for data wrangling, visualization, and controlling warnings. Additionally, it imports a few specific functions and classes from some of these libraries, such as GridSearchCV from Scikit-Learn and ks_2samp from Scipy. Finally, it sets a color variable to be used in plotting. It's visibility can be toggled on and off woth the button underneath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import fancyimpute\n",
    "import html\n",
    "import matplotlib.axes\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import plotly.express as px\n",
    "import re\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import HTML, Image, display\n",
    "from countrygroups import EUROPEAN_UNION\n",
    "from countryinfo import CountryInfo\n",
    "from functools import partial, reduce\n",
    "from scipy.stats import ks_2samp, shapiro\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EU_comparative_analysis_of_irelands_beef_sector_with_ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'Transform' function \n",
    "\n",
    "Specifically, it first reads in the CSV file from the provided file path, skips any bad lines, and skips the first row. Then, it renames the column named 'Unnamed: 0' to 'Year' and creates a new column called 'key', which is a combination of the second column and the 'Year' column as strings. Next, it renames the second column to the given col_name argument. Finally, it filters the resulting dataframe to only include the 'key' and specified column, and returns the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(file_path: str, col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads in a CSV file from the given file path, renames a column to the given column name, and\n",
    "    filters the resulting dataframe to only include the 'key' and specified column.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file to read in.\n",
    "        col_name (str): The name to assign to the specified column in the resulting dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The resulting dataframe after renaming the specified column and filtering\n",
    "        to only include the 'key' and specified column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, on_bad_lines='skip', skiprows=1)\n",
    "        df.rename(columns={'Unnamed: 0': 'Year'}, inplace=True)\n",
    "        df['key'] = df.columns[1] + df['Year'].astype(str)\n",
    "        df.rename(columns={df.columns[1]: col_name}, inplace=True)\n",
    "        df = df.filter(['key', col_name])\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading in CSV file {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  The 'combine'  Function Used to concateneate country-by-country climate data\n",
    "\n",
    "The combine function is designed to read in all  the **precicipitation** or **mean-Temperature** CSV files  from either the 'pr' or 'ts' folders, apply the transform function to each file to rename a column with the country the data relates to and filter at the superregional level. It then combines    all the the resulting dataframes together into a single dataframe for all the 27 EU counties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(csv_folder: str, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads in all CSV files in the given folder, applies the 'transform' function to each file, and\n",
    "    combines the resulting dataframes together into a single dataframe.\n",
    "    \n",
    "    Args:\n",
    "        csv_folder (str): The path to the folder containing the CSV files to read in.\n",
    "        column_name (str): The name to assign to the specified column in the resulting dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The resulting dataframe after combining data from all CSV files in the\n",
    "        specified folder.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        csv_filenames = glob.glob(csv_folder + \"/*.csv\")\n",
    "        processed_dfs = (transform(FileName, column_name) for FileName in csv_filenames)\n",
    "        df = pd.concat(processed_dfs, ignore_index=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining CSV files in folder {csv_folder}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The  'snake' Naming Function used for Filenames.\n",
    "The **snake_case** naming   convention involves combining words with underscores, with all letters in lowercase, and no punctuation.\n",
    "\n",
    "explanations for your code\n",
    "Code explanation:\n",
    "\n",
    "-  Snake input is any string and output same in  snake case.\n",
    "  \n",
    "-  Converts input to string if required\n",
    "  \n",
    "-  Uses the re.sub method to replace  whitespace with underscores (_) in the text string and strips away leading or trailing whitespace. \n",
    "-  The function then converts the string to lowercase using the lower method and  checks if the string starts with an underscore (_) using the startswith method. If it does, the first character is removed from the string using slicing, so that the string is not preceded by an underscore in snake case.\n",
    "-  The function returns the resulting snake-cased string, surrounded by spaces for padding, using string concatenation and the return statement.\n",
    "-  The code then creates a variable text with the value 'text in snake case'.\n",
    "\n",
    "-  The snake function is called with text as input, and the result is stored in a variable result.\n",
    "\n",
    "-  The resulting snake-cased string is printed to the console using the print statement.\n",
    "\n",
    "Overall, the snake function takes a string and converts it to snake case, which is a naming convention for variables and functions where words are separated by underscores. The function achieves this by replacing whitespace with underscores, converting to lowercase, and removing any leading underscores. The resulting string is then padded with spaces and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snake(text):\n",
    "    # Convert to string and strip leading/trailing whitespace\n",
    "    text = str(text).strip()\n",
    "    # Replace any whitespace characters with underscores\n",
    "    text = re.sub(r'\\s+', '_', text)\n",
    "    # Convert to all lowercase\n",
    "    text = text.lower()\n",
    "    # Add an underscore to the beginning if it doesn't already start with one\n",
    "    if not text.startswith('_'):\n",
    "        text = '_' + text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The 'pascal' Naming Function Used for Folder Names\n",
    "A convention almost identical to CamelCase, but the first word is also capitalized as well. For example, \"RawData\" or \"EngineeredData\". In this project it is used predominanly for naming folders. It uses the replace() method and splits the string into words to be exploited with the capitalize method before the join method returns the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ThisTextHasBeenPascalled    \n"
     ]
    }
   ],
   "source": [
    "###  The 'pascal' Naming Function\n",
    "def pascal(string):\n",
    "    words = string.replace(\"_\", \" \").split()\n",
    "    pascal_cased = [word.capitalize() for word in words]\n",
    "    return \"    \" + ''.join(pascal_cased) + \"    \"\n",
    "\n",
    "text = 'This text has been pascalled'\n",
    "result = pascal(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'camel' Naming Function Used For Column Labels\n",
    "\n",
    "This code defines a function called **camel** that takes a string as input and returns a camel-cased version of the string. Camel case is a naming convention in which multiple words are combined into a single word, with each word capitalized except the first.\n",
    "\n",
    "Here's how the code in the function works:\n",
    "\n",
    "- The input string is first processed to replace any underscores with spaces. This allows the function to handle strings that are written in snake case (i.e., with underscores between words) as well as in regular text format.\n",
    "\n",
    "- The split() method is used to split the string into a list of words. This assumes that the words in the string are separated by spaces.\n",
    "\n",
    "- The first word in the list is converted to lowercase using the lower() method, since it is not capitalized in camel case.\n",
    "\n",
    "- The remaining words in the list are capitalized using the capitalize() method, which capitalizes the first letter of each word.\n",
    "\n",
    "- The lowercase first word and capitalized remaining words are then combined into a single string using the join() method, with no spaces between the words.\n",
    "\n",
    "- It returns the camel-cased string, with  spaces added to the beginning and end to make it easier to copy paste the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'camel' Naming Function \n",
    "def camel(string):\n",
    "    words = string.replace(\"_\", \" \").split()\n",
    "    camel_cased = [words[0].lower()] + [word.capitalize() for word in words[1:]]\n",
    "    return  \"     \" +''.join(camel_cased)+ \"     \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'Title' Naming Function converts strings to Title Case\n",
    "In traditional title case, the first word and all other words except for prepositions and conjunctions are capitalized.\n",
    "The **title** function takes a string sentence as input and returns a new string with every word capitalized. Here's a step-by-step explanation of the code:\n",
    "\n",
    "- The re module is imported to enable the use of regular expressions.\n",
    "- The function title() takes in a sentence and returns a string that represents the title-cased version of the input sentence.\n",
    "- The input sentence is split into words using the split() method, and the resulting list of words is stored in the words variable.\n",
    "- An initial list called prepositions_conjunctions is created to store the prepositions and conjunctions that should not be capitalized in the title and this should be added to as exceptions are discovered.\n",
    "- A list comprehension is used to create a new list called capitalized_words, which consists of the words from the words list, but with the first letter capitalized if the word is not in the prepositions_conjunctions list. If the word is in the prepositions_conjunctions list, it is added to the capitalized_words list without any changes.\n",
    "- The capitalized_words list is joined back into a string using the join() method, and the resulting string is stored in the output variable.\n",
    "- Regular expressions are used to remove any leading/trailing whitespace from the output string and to replace any extra whitespace within the string with a single space character.\n",
    "- The resulting string is returned, with some additional padding to make it stand out visually.\n",
    "\n",
    "\n",
    "Here's a breakdown of the return statement:\n",
    "\n",
    "- re.sub('\\s+', ' ', output.strip()) replaces any runs of whitespace characters (including spaces, tabs, and newlines) in the output string with a single space character.\n",
    "- output.strip() removes any leading/trailing whitespace from the output string.\n",
    "- \" \" + re.sub('\\s+', ' ', output.strip()) + \" \" concatenates the stripped, cleaned up output string with extra padding (five spaces on each side), and returns the resulting string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     this Text has Been Title_cased     '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def title(sentence):\n",
    "    words = sentence.split()\n",
    "    prepositions_conjunctions = ['a', 'this','an', 'the', 'and', 'but', 'or', 'for', 'has','nor', 'on', 'at', 'to', 'from', 'by', 'over', 'under', 'in', 'out', 'of']\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        if word.lower() not in prepositions_conjunctions:\n",
    "            processed_words.append(word.capitalize())\n",
    "        else:\n",
    "            processed_words.append(word.lower())\n",
    "    output = \" \".join(processed_words)\n",
    "    # remove any leading/trailing whitespace and add some padding\n",
    "    return \"     \" + re.sub('\\s+', ' ', output.strip()) + \"     \"\n",
    "title('this text Has Been Title_Cased')\n",
    "# title('       ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     Analytical Methods and Features of this Report     '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title(     '            Analytical Methods and Features of this  Report') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The 'scan' Function Returns a List of Files from a Directory\n",
    "\n",
    "\n",
    "In this administrative function we use a try-except block to catch two potential errors. The FileNotFoundError exception is raised if the specified directory does not exist, and the OSError exception is raised if the specified path is not valid. If either of these exceptions is raised, the function prints a corresponding error message and returns without attempting to list the directory contents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scan(folder_name):\n",
    "    current_dir = os.getcwd()\n",
    "    folder_path = os.path.join(current_dir, folder_name)\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found\")\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path\")\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_name))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            print(\"{:30} {:10} {}\".format(item, size, modified_time))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#just scan hardcoded to RawData folder\n",
    "def scanr():\n",
    "    folder_name = 'RawData'\n",
    "    current_dir = os.getcwd()\n",
    "    folder_path = os.path.join(current_dir, folder_name)\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found\")\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path\")\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_name))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            print(\"{:30} {:10} {}\".format(item, size, modified_time))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just scan hardcoded toengineered_data folder\n",
    "def scane():\n",
    "    folder_name = 'engineered_data'\n",
    "    current_dir = os.getcwd()\n",
    "    folder_path = os.path.join(current_dir, folder_name)\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found\")\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path\")\n",
    "        return\n",
    "    print(\"Contents of the folder '{}':\".format(folder_name))\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            print(\"{:30} {:10} {}\".format(item, size, modified_time))\n",
    "        else:\n",
    "            print(\"{} (directory)\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory not found\n"
     ]
    }
   ],
   "source": [
    "scanr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory not found\n"
     ]
    }
   ],
   "source": [
    "scane()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'read', 'readr' and 'reade' Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(filename, df_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the base directory and returns a pandas DataFrame\n",
    "    with the specified name.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The name of the CSV file to be read.\n",
    "        df_name (str): The name to be assigned to the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame created from the data in the CSV file with\n",
    "        the specified name.\n",
    "    \"\"\"\n",
    "    file_path = f'{filename}.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.name = df_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def readr(filename, df_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from  rawdata folder and returns a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        filename (str): base name of CSV file to be read.\n",
    "        df_name (str): The DataFrame name to be assigned \n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame created from the data in the CSV file with\n",
    "        the specified name.\n",
    "    \"\"\"\n",
    "    file_path = '../data/raw/' + filename + '.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.name = df_name\n",
    "    return df\n",
    "#lu = readr('land_use', 'lu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reade(filename, df_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the specified file path in the processed folder and returns a pandas DataFrame\n",
    "    with the specified name.\n",
    "    \n",
    "    Parameters:\n",
    "        filename (str): The name of the CSV file to be read.\n",
    "        df_name (str): The name to be assigned to the resulting DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame created from the data in the CSV file with\n",
    "        the specified name.\n",
    "    \"\"\"\n",
    "    file_path = f'../data/processed/{filename}.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.name = df_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rawdata/trade_indices.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6388\\2146889983.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Call the readr function to create a DataFrame called trade\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'trade_indices'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'trade'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# trade\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6388\\1219938548.py\u001b[0m in \u001b[0;36mreadr\u001b[1;34m(filename, df_name)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \"\"\"\n\u001b[0;32m     13\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'rawdata/{filename}.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rawdata/trade_indices.csv'"
     ]
    }
   ],
   "source": [
    "# Call the readr function to create a DataFrame called trade\n",
    "trade = readr('trade_indices','trade')\n",
    "# trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=reade('temp','temp')\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def readr(filename, df_name):\n",
    "#     \"\"\"\n",
    "#     Reads a CSV file from the specified file path in the rawdata folder and returns a pandas DataFrame\n",
    "#     with the specified name.\n",
    "    \n",
    "#     Parameters:\n",
    "#         filename (str): The name of the CSV file to be read.\n",
    "#         df_name (str): The name to be assigned to the resulting DataFrame.\n",
    "        \n",
    "#     Returns:\n",
    "#         pandas.DataFrame: A DataFrame created from the data in the CSV file with\n",
    "#         the specified name.\n",
    "#     \"\"\"\n",
    "#     file_path = f'rawdata/{filename}.csv'\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     df.name = df_name\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the folder 'engineered_data':\n",
      ".ipynb_checkpoints (directory)\n",
      "cover.csv                         2141160 2023-02-16 12:23\n",
      "main.csv                            56353 2023-02-16 03:53\n",
      "missing.csv                          3630 2023-02-16 04:55\n",
      "production.csv                    1173771 2023-02-16 05:55\n",
      "rain.csv                            65684 2023-02-16 05:55\n",
      "stocks.csv                        1438299 2023-02-16 05:55\n",
      "temp.csv                            59959 2023-02-16 05:55\n"
     ]
    }
   ],
   "source": [
    "scane()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the folder '.':\n",
      ".ipynb_checkpoints (directory)\n",
      "archive (directory)\n",
      "Beef-Thurs13.ipynb                 159160 2023-02-16 12:52\n",
      "checkpoints - Copy (directory)\n",
      "engineered_data (directory)\n",
      "images (directory)\n",
      "pr (directory)\n",
      "python-notebooks-data-wrangling-master (directory)\n",
      "RawData (directory)\n",
      "tas (directory)\n"
     ]
    }
   ],
   "source": [
    "scan('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The  _groupcsv_  function loads csv data and groups by a specified column \n",
    "\n",
    "In this function, df_name is a string that specifies the name or path of the input dataframe file, and column_name is a string that specifies the name of the column to group by. The function uses the pandas library to load the dataframe from the specified file, and then groups the dataframe by the specified column using the groupby method. The resulting dictionary of dataframes is then returned as output.\n",
    "\n",
    "You can call this function by passing in the name of your dataframe and the name of the column you want to group by, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupcsv(df_name, column_name):\n",
    "    # Load the specified dataframe\n",
    "    df = pd.read_csv(df_name)\n",
    "    # Group the dataframe by the specified column\n",
    "    dfs = dict(tuple(df.groupby(column_name)))\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The  _groupdf_ function \n",
    "\n",
    "The groupdf function takes in a pandas dataframe 'df' and a column name 'column_name' as input, and then groups the dataframe by the specified column using the groupby method. The resulting dictionary of dataframes is then returned as output.\n",
    "\n",
    "A  detailed breakdown of the function reads as:\n",
    "\n",
    "- The function takes in a pandas dataframe df and a column name column_name.\n",
    "- The groupby method is called on the input dataframe df using the specified column name column_name. This groups the dataframe by the values in the specified column, creating a new dataframe for each unique value in that column.\n",
    "- The dict and tuple functions are used to convert the resulting pandas GroupBy object into a dictionary of dataframes. Each key in the dictionary corresponds to a unique value in the specified column, and the corresponding value is a dataframe containing only the rows with that value in the specified column.\n",
    "- The resulting dictionary of dataframes is returned as output.\n",
    "\n",
    "Overall, this function allows us to do the  analysis and visualization for the project in python and not spend too long at[FAOSTAT](https://www.fao.org/faostat/en/#home) worrying about what filters to set ubder items and elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupdf(df, column_name):\n",
    "    # Group the dataframe by the specified column\n",
    "    dfs = dict(tuple(df.groupby(column_name)))\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The _delete_ function\n",
    "\n",
    "In this code, we use the os module to delete a file at the specified file path. We first check if the file exists using the os.path.exists() function. If the file exists, we delete it using the os.remove() function. If the file does not exist, we print a message indicating that the file was not found.\n",
    "\n",
    "Note that this code permanently deletes the file, so you should use it with caution. Once a file is deleted, it cannot be easily recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(\"File deleted successfully\")\n",
    "    else:\n",
    "        print(\"File not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The _rename_  function\n",
    "\n",
    "This function takes two arguments: old_filename specifies the current name or path of the file to rename, and new_filename specifies the new name or path to give the file.\n",
    "\n",
    "The os.rename() function is used to rename the file. If the file is successfully renamed, the function prints a success message. If an error occurs, such as the file not being found, a file with the new name already existing, or an invalid file path or name, the function prints an appropriate error message.\n",
    "\n",
    "To use this function, simply call it with the old and new filenames as arguments, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(old_filename, new_filename):\n",
    "    try:\n",
    "        os.rename(old_filename, new_filename)\n",
    "        print(\"File renamed successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found\")\n",
    "    except FileExistsError:\n",
    "        print(\"A file with the new name already exists\")\n",
    "    except OSError:\n",
    "        print(\"Invalid file path or name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The table2folder function code:\n",
    "\n",
    "Explanation of the tableefolder function code:\n",
    "\n",
    "The print_folder_contents function takes a folder_name argument, which is the name of the folder to print the contents of, and an optional scale_factor argument, which is a scaling factor for the font size of the table (default is 1.0, no scaling).\n",
    "\n",
    "The function first gets the current working directory and appends the folder_name to it to get the path to the folder. It then uses os.listdir to get a list of the files and directories in the folder.\n",
    "\n",
    "The function then prints an HTML table using a series of print statements. The first print statement prints the opening HTML table tag with a style attribute that sets the font size to the specified scale_factor.\n",
    "\n",
    "The next few print statements print the table header with column labels.\n",
    "\n",
    "The function then iterates over the contents of the folder and uses os.path.isfile to check if an item is a file. If it is a file, the function gets the file size using os.path.getsize, converts it to kilobytes, and scales it according to the scale_factor. It then gets the modified time of the file using os.path.getmtime, formats it using datetime.datetime.fromtimestamp and prints the filename, size, and modified time in an HTML table row.\n",
    "\n",
    "Finally, the function prints the closing tags for the HTML table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tableafolder(folder_name, scale_factor=1.0):\n",
    "    current_dir = os.getcwd()\n",
    "    folder_path = os.path.join(current_dir, folder_name)\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Directory not found\")\n",
    "        return\n",
    "    except OSError:\n",
    "        print(\"Invalid folder path\")\n",
    "        return\n",
    "\n",
    "    print('<table style=\"font-size:{}%\">'.format(scale_factor))\n",
    "    print('<thead>')\n",
    "    print('<tr><th>File Name</th><th>Size</th><th>Modified Time</th></tr>')\n",
    "    print('</thead>')\n",
    "    print('<tbody>')\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size = os.path.getsize(item_path) // 1000 // scale_factor\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            print(\"<tr><td>{}</td><td>{:,} KB</td><td>{}</td></tr>\".format(item, size, modified_time))\n",
    "    print('</tbody>')\n",
    "    print('</table>') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The _Replace_ Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(input_str, find_str, replace_str):\n",
    "    output_str = re.sub(find_str, replace_str, input_str)\n",
    "    \n",
    "    if output_str == input_str:\n",
    "        warnings.warn(\"Replacement unsuccessful: '{}' not found in input string.\".format(find_str))\n",
    "    \n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The _key_ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key(df, area_col='Area', year_col='Year', key_col='key'):\n",
    "    \"\"\"\n",
    "    Add a new column to a pandas DataFrame that concatenates the values in the 'area_col' and 'year_col' columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to add the new column to.\n",
    "    - area_col (str): The name of the column that contains the area values.\n",
    "    - year_col (str): The name of the column that contains the year values.\n",
    "    - key_col (str): The name of the new column to create.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    df[key_col] = df[area_col] + df[year_col].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The _eu Function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eu(df):\n",
    "    \"\"\"\n",
    "    Filter a pandas DataFrame to include only the rows where the 'Area' column contains values that match the countries in the European Union.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to filter.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    country_list = ['Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czechia', 'Denmark', \n",
    "                    'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Ireland', 'Italy', \n",
    "                    'Latvia', 'Lithuania', 'Luxembourg', 'Malta', 'Netherlands', 'Poland', 'Portugal', 'Romania', \n",
    "                    'Slovakia', 'Slovenia', 'Spain', 'Sweden']\n",
    "    df = df[df['Area'].isin(country_list)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Read Rawdata to df Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def raw(base_file_name):\n",
    "    \"\"\"\n",
    "    Read a CSV file from the 'rawdata' subfolder with the given base file name, skipping any badly formatted lines.\n",
    "\n",
    "    Parameters:\n",
    "    - base_file_name (str): The base name of the CSV file to read (without the file extension).\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The DataFrame containing the data from the CSV file.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join('rawdata', base_file_name + '.csv')\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"File {file_path} not found\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "        return df\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error parsing CSV file {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write df to csv Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def writee(df, filename):\n",
    "    \"\"\"\n",
    "    Write a pandas DataFrame to a CSV file in the 'engineered_data' folder.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to write to a CSV file.\n",
    "    - filename (str): The filename for the output CSV file (without extension).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Check if the 'engineered_data' folder exists, and create it if it doesn't\n",
    "    if not os.path.exists('engineered_data'):\n",
    "        os.makedirs('engineered_data')\n",
    "\n",
    "    # Save the DataFrame to a CSV file in the 'engineered_data' folder\n",
    "    file_path = os.path.join('engineered_data', f'{filename}.csv')\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Function 'recent' >1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recent(df):\n",
    "#     df_filtered = df[df['Year'] >= 2000]\n",
    "#     return df\n",
    "\n",
    "def year(df):\n",
    "    return df[df['Year'] >= 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Prepare function 3 in 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(df):\n",
    "    \"\"\"\n",
    "    Prepare a pandas DataFrame for analysis by adding a key column, filtering to European Union countries, and selecting only recent data.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to prepare.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The prepared DataFrame.\n",
    "    \"\"\"\n",
    "    key(df)\n",
    "    eu(df)\n",
    "    year(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Functions to count   *words*   and  *characters*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparative analysis of Irelands beef sector includeing forecasting prices, as well as sentiment analysis. Evidence-based recommendations are made and methodologies  and  main decisions are  explained and justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_count('A comparative analysis of Irelands beef sector includeing forecasting prices, as well as sentiment analysis. Evidence-based recommendations are made and methodologies  and  main decisions are  explained and justified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    \"\"\"\n",
    "    Counts the number of words in a given text.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The text to be counted.\n",
    "        \n",
    "    Returns:\n",
    "        int: The number of words in the text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_count(string):\n",
    "    return len(string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Raw Data Sources and Licening\n",
    "\n",
    "\n",
    "\n",
    "### Data Sources\n",
    "[FAOSTAT](https://www.fao.org/faostat/en/#home) and [the CCKP website](https://climateknowledgeportal.worldbank.org/) were the only two sources of secondary data but with a broader research question that encompassed nutrition and health interventions  other sources such as [Eurostat](https://ec.europa.eu/eurostat) could also have been brought into play.\n",
    "\n",
    "\n",
    "\n",
    "Data from FAOSTAT  is used to support informed decision-making and policy formulation, as well as monoitor beef production trends. The different domains provide a comprehensive overview of the food and agriculture sector and of course the beef sector.\n",
    "\n",
    "Only two  attributes are taken from the  CCKP database: **Mean_Temperature** and  **Precipitation**.\n",
    "\n",
    "\n",
    "\n",
    "### Data Licencing\n",
    "\n",
    "<img src=\"images/tu06.png\" alt=\"image5\" width=\"30%\">\n",
    "\n",
    "<span style=\"font-size: 24px;\">                  </span>\n",
    "All three data sources make data freely available under the CC BY-NC-SA 3.0 IGO license. This is the  Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International Governmental Organizations license. It allows others to remix, tweak, and build upon content in a  non-commercial manner, as long as they credit the owner and license their new creations under  identical terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAOSTAT\n",
    "<img src=\"images/tu02.png\" alt=\"image5\" width=\"30%\">\n",
    "\n",
    "<span style=\"font-size: 24px;\">                  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2: Raw Data files from FAOSTAT\n",
    "\n",
    "<table style=\"font-size:100%\">\n",
    "<thead>\n",
    "<tr><th>File Name</th><th>Size</th><th>Modified Time</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>consumer_price_indices.csv</td><td>279 KB</td><td>2023-02-15 19:20</td></tr>\n",
    "<tr><td>country_area.csv</td><td>16 KB</td><td>2023-02-15 19:40</td></tr>\n",
    "<tr><td>input_fertilizers_by_nutrient.csv</td><td>70 KB</td><td>2023-02-15 20:21</td></tr>\n",
    "<tr><td>input_livestock_manure.csv</td><td>206 KB</td><td>2023-02-15 20:28</td></tr>\n",
    "<tr><td>investment_credit_to_agriculture.csv</td><td>29 KB</td><td>2023-02-15 20:40</td></tr>\n",
    "<tr><td>investment_government_expenditure.csv</td><td>8 KB</td><td>2023-02-15 20:49</td></tr>\n",
    "<tr><td>land_cover.csv</td><td>27 KB</td><td>2023-02-15 20:18</td></tr>\n",
    "<tr><td>land_use.csv</td><td>76 KB</td><td>2023-02-15 19:45</td></tr>\n",
    "<tr><td>population.csv</td><td>170 KB</td><td>2023-02-15 20:37</td></tr>\n",
    "<tr><td>prices_for_producers.csv</td><td>19 KB</td><td>2023-02-15 18:29</td></tr>\n",
    "<tr><td>price_deflators.csv</td><td>138 KB</td><td>2023-02-15 19:29</td></tr>\n",
    "<tr><td>price_exchange_rates.csv</td><td>45 KB</td><td>2023-02-15 19:37</td></tr>\n",
    "<tr><td>produce_beef_value.csv</td><td>134 KB</td><td>2023-02-15 15:59</td></tr>\n",
    "<tr><td>produce_live_animal.csv</td><td>33 KB</td><td>2023-02-15 08:40</td></tr>\n",
    "<tr><td>produce_primary_meat.csv</td><td>75 KB</td><td>2023-02-15 13:35</td></tr>\n",
    "<tr><td>trade_indices.csv</td><td>230 KB</td><td>2023-02-15 18:25</td></tr>\n",
    "<tr><td>trade_produce.csv</td><td>26 KB</td><td>2023-02-15 17:12</td></tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About FAOSTAT\n",
    "FAOSTAT is a comprehensive database maintained by the Food and Agriculture Organization of the United Nations (FAO) and contains statistical information related to agriculture, food, and nutrition, covering over 200 countries and territories worldwide.\n",
    "\n",
    "One of the primary goals of FAOSTAT is to provide access to timely, accurate, and reliable data on food and agriculture to support decision-making, policy formulation, and research in this field. The database is updated regularly and provides data on a range of topics, including production, trade, food balance sheets, fertilizer use, and more.\n",
    "\n",
    "FAOSTAT is a valuable resource for a wide range of stakeholders, including governments, international organizations, academic researchers, and the general public. Its data is widely used to inform policy, monitor progress, and measure the impact of interventions aimed at improving food security and reducing poverty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "####  FAOSTAT Data Domains \n",
    "\n",
    "\n",
    "\n",
    "The [FAOSTAT Data Domains](https://www.fao.org/faostat/en/#data/domains_table)  are organised as follows:\n",
    "\n",
    "  -  Production: Production of crops and livestock products, including production indices and the value of agricultural production.\n",
    "\n",
    "  - Food Security and Nutrition: Information on SDG indicators related to food security and nutrition and food balances.\n",
    "\n",
    "  - Trade: Including detailed trade matrices, trade indices, and updates on related data.\n",
    "\n",
    "  - Prices: Producer and   consumer price indices, deflators, and exchange rates.\n",
    "\n",
    "  - Land, Inputs and Sustainability:Land use, land cover, inputs, including fertilizers and manure and pesticides.\n",
    "  \n",
    "  - Population and Employment: Annual population including those specific to agriculture and rural areas.\n",
    "\n",
    "  - Investment: Government expenditure, credit to agriculture, foreign direct investment, and country investment statistics.\n",
    "\n",
    "  - Macro-Economic Indicators: Such as capital stock.\n",
    "\n",
    "  - Food Value Chain: This domain provides information on the value shares of the food industry and primary factors.\n",
    "\n",
    "  - Climate Change: Emissions, crop residues, forests, and other indicators related to climate change.\n",
    "\n",
    "  - Forestry: Forestry production and trade, as well as forestry trade flows.\n",
    "\n",
    "  - SDG Indicators: The Sustainable Development Goals (SDGs) are a set of 17 goals established by the United Nations in 2015.\n",
    "\n",
    "  - World Census of Agriculture: This domain provides structural data from agricultural censuses taken around the world\n",
    "\n",
    "  - Discontinued archives and data series: This includes data on indicators from surveys and research.\n",
    "\n",
    "<img src=\"images/tu07.png\" alt=\"image7\" width=\"80%\">\n",
    "\n",
    "\n",
    "[<span style=\"font-size: 18px;\">Figure 1:Data Domain view of FAOSTAT </span>](https://www.fao.org/faostat/en/#data)\n",
    "\n",
    "\n",
    "<img src=\"images/we16.png\" alt=\"image7\" width=\"80%\">\n",
    "\n",
    "[<span style=\"font-size: 18px;\">Figure 2:Data Domain Table view of FAOSTAT which basically opens out all the drop down options </span>\n",
    "](https://www.fao.org/faostat/en/#data/domains_table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "All datasets from the FAOSTAT are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO [(CC BY-NC-SA 3.0 IGO)](https://creativecommons.org/licenses/by-nc-sa/3.0/igo/). Source: FAOSTAT (2023). Time Series datasets. Retrieved from [https://www.fao.org/faostat/en/#data.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate Change Knowledge Portal (CCKP)\n",
    "\n",
    "![image3](images/cckp.png)\n",
    "\n",
    "<span style=\"font-size: 24px;\">                  </span>\n",
    "\n",
    "\n",
    "\n",
    "#### About the CCKP\n",
    "\n",
    "\n",
    "\n",
    "The Climate Change Knowledge Portal (CCKP) is a  resource for information on the impacts of climate change and the actions taken to address these impacts. While this is outside the remit of this project the CCKP also provides access to global data on a historical  basis for the **Mean_Temperature** and **Precipitation** at the  country-by-country level  on both monthly and yearly aggregates. While **humidity** is also a known influential predictor variable these two should have a statistically significant imapct on our predictive modelling. Spatial data is provided as a global NetCDF file, with Climatology, Timeseries and Heatplot data is provided as a CSV file.\n",
    "\n",
    "Climate conditions, such as average temperature and rainfall (precipitation ) can greatly affect the growth and health of cattle. Precipitation and temperature predictor variables were  retrieved  but they were   aggragated by country and this necessitated  the **cckp** and  **combine**  functions for data wrangling.\n",
    "\n",
    "\n",
    "### Predictor variables  of average temperature and rainfall from the  [the CCKP website](https://climateknowledgeportal.worldbank.org/)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### CCKP Data \n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <img src=\"images/we28.png\" alt=\"image19\" style=\"width: 30%;\">\n",
    "  <img src=\"images/we29.png\" alt=\"image20\" style=\"width: 30%;\">\n",
    "    <img src=\"images/we30.png\" alt=\"image20\" style=\"width: 30%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size: 18px;\">Figure 3:Time Series Data downloaded for 27 EU countries from  CCKP site </span>\n",
    "\n",
    "\n",
    "To maintain consistency with the FAO data, annual and not monthly time series  aggregates were taken from the Climatic Research Unit (CRU) dataset for **precipitation** and   **mean-temperature**. These datasets are provided by the CRU TS 4.04 dataset, a gridded climate dataset produced by the Climatic Research Unit (CRU) at the University of East Anglia in the United Kingdom. In the statistics section, range, variance, and standard deviation of monthly data may be revisited for insights.\n",
    "\n",
    "The file names and folder names of the CCKP data used in this project are tabulated below.\n",
    "\n",
    "<span style=\"font-size: 24px;\">Table : Table shows Time Series data filenames and folders  for 27 EU countries from  CCKP site </span>\n",
    "\n",
    "\n",
    "\n",
    "| Country        | Code | TasAnnual (Folder with Tempature data)                              | PrAnnual   (Folder with Precipitation Data)                        |\n",
    "|:--------------:|:----:|:--------------------------------------:|:---------------------------------:|\n",
    "| Albania        | ALB  | tas_timeseries_annual_cru_1901-2021_ALB.csv | pr_timeseries_annual_cru_1901-2021_ALB.csv |\n",
    "| Andorra        | AND  | tas_timeseries_annual_cru_1901-2021_AND.csv | pr_timeseries_annual_cru_1901-2021_AND.csv |\n",
    "| Austria        | AUT  | tas_timeseries_annual_cru_1901-2021_AUT.csv | pr_timeseries_annual_cru_1901-2021_AUT.csv |\n",
    "| Belarus        | BLR  | tas_timeseries_annual_cru_1901-2021_BLR.csv | pr_timeseries_annual_cru_1901-2021_BLR.csv |\n",
    "| Belgium        | BEL  | tas_timeseries_annual_cru_1901-2021_BEL.csv | pr_timeseries_annual_cru_1901-2021_BEL.csv |\n",
    "| Bosnia and Herzegovina | BIH  | tas_timeseries_annual_cru_1901-2021_BIH.csv | pr_timeseries_annual_cru_1901-2021_BIH.csv |\n",
    "| Bulgaria       | BGR  | tas_timeseries_annual_cru_1901-2021_BGR.csv | pr_timeseries_annual_cru_1901-2021_BGR.csv |\n",
    "| Croatia        | HRV  | tas_timeseries_annual_cru_1901-2021_HRV.csv | pr_timeseries_annual_cru_1901-2021_HRV.csv |\n",
    "| Cyprus         | CYP  | tas_timeseries_annual_cru_1901-2021_CYP.csv | pr_timeseries_annual_cru_1901-2021_CYP.csv |\n",
    "| Czech Republic | CZE  | tas_timeseries_annual_cru_1901-2021_CZE.csv | pr_timeseries_annual_cru_1901-2021_CZE.csv |\n",
    "| Denmark        | DNK  | tas_timeseries_annual_cru_1901-2021_DNK.csv | pr_timeseries_annual_cru_1901-2021_DNK.csv |\n",
    "| Estonia        | EST  | tas_timeseries_annual_cru_1901-2021_EST.csv | pr_timeseries_annual_cru_1901-2021_EST.csv |\n",
    "| Finland        | FIN  | tas_timeseries_annual_cru_1901-2021_FIN.csv | pr_timeseries_annual_cru_1901-2021_FIN.csv |\n",
    "| France         | FRA  | tas_timeseries_annual_cru_1901-2021_FRA.csv | pr_timeseries_annual_cru_1901-2021_FRA.csv |\n",
    "| Germany        | DEU  | tas_timeseries_annual_cru_1901-2021_DEU.csv | pr_timeseries_annual_cru_1901-2021_DEU.csv |\n",
    "| Gibraltar      | GIB  | tas_timeseries_annual_cru_1901-2021_GIB.csv | pr_timeseries_annual_cru_1901-2021_GIB.csv |\n",
    "| Greece | GRC | tas_timeseries_annual_cru_1901-2021_GRC.csv | pr_timeseries_annual_cru_1901-2021_GRC.csv |\n",
    "| Croatia | HRV | tas_timeseries_annual_cru_1901-2021_HRV.csv | pr_timeseries_annual_cru_1901-2021_HRV.csv |\n",
    "| Hungary | HUN | tas_timeseries_annual_cru_1901-2021_HUN.csv | pr_timeseries_annual_cru_1901-2021_HUN.csv |\n",
    "| Ireland | IRL | tas_timeseries_annual_cru_1901-2021_IRL.csv | pr_timeseries_annual_cru_1901-2021_IRL.csv |\n",
    "| Italy | ITA | tas_timeseries_annual_cru_1901-2021_ITA.csv | pr_timeseries_annual_cru_1901-2021_ITA.csv |\n",
    "| Lithuania | LTU | tas_timeseries_annual_cru_1901-2021_LTU.csv | pr_timeseries_annual_cru_1901-2021_LTU.csv |\n",
    "| Luxembourg | LUX | tas_timeseries_annual_cru_1901-2021_LUX.csv | pr_timeseries_annual_cru_1901-2021_LUX.csv |\n",
    "| Latvia | LVA | tas_timeseries_annual_cru_1901-2021_LVA.csv | pr_timeseries_annual_cru_1901-2021_LVA.csv |\n",
    "| Malta | MLT | tas_timeseries_annual_cru_1901-2021_MLT.csv | pr_timeseries_annual_cru_1901-2021_MLT.csv |\n",
    "| Netherlands | NLD | tas_timeseries_annual_cru_1901-2021_NLD.csv | pr_timeseries_annual_cru_1901-2021_NLD.csv |\n",
    "| Poland | POL | tas_timeseries_annual_cru_1901-2021_POL.csv | pr_timeseries_annual_cru_1901-2021_POL.csv |\n",
    "| Portugal | PRT | tas_timeseries_annual_cru_1901-2021_PRT.csv | pr_timeseries_annual_cru_1901-2021_PRT.csv |\n",
    "| Romania | ROU | tas_timeseries_annual_cru_1901-2021_ROU.csv | pr_timeseries_annual_cru_1901-2021_ROU.csv |\n",
    "| Slovakia | SVK | tas_timeseries_annual_cru_1901-2021_SVK.csv | pr_timeseries_annual_cru_1901-2021_SVK.csv |\n",
    "| Slovenia | SVN | tas_timeseries_annual_cru_1901-2021_SVN.csv | pr_timeseries_annual_cru_1901-2021_SVN.csv |\n",
    "| Sweden | SWE | tas_timeseries_annual_cru_1901-2021_SWE.csv | pr_timeseries_annual_cru_1901-2021_SWE.csv |\n",
    "\n",
    "\n",
    "All datasets from the CCKP are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO (CC BY-NC-SA 3.0 IGO). \n",
    "Source: CCKP (2023). Time Series datasets. Retrieved from [https://climateknowledgeportal.worldbank.org/download-data].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping Null Values at Download Stage\n",
    "All data is downloaded as CSV Tables  with no thousands separator and and all output formating options turned on. These are all defaul settings except the Null Values button which is toggled 'off' by default!\n",
    "\n",
    "\n",
    "\n",
    "![Example Image](images/we03.png)\n",
    "\n",
    "<span style=\"font-size: 18px;\"> Figure :Null values are omitted by default but we always include them                 </span>\n",
    "\n",
    "By keeping null values we get a complete picture and allow for the option to remove them later, impute missing values with estimated values. This can be done by filling missing values with a constant value, the mean or median of the remaining values in the column, or using more advanced techniques such as Forward fill,  backward fill or even reformating the research question based on quality of the data.\n",
    "\n",
    "\n",
    "###  Tailoring  Downloads for the EU 27  Countries: Possible but unwise.\n",
    "\n",
    "Taoiloring FAOSTAT downloads for the EU 27  coutries was possible as shown but leaving this to the pythonic EDA stage allows for the option to broaden the research during or after the project is submitted.\n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we14.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we15.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size: 18px;\">Figure 5: Filtering to the EU 27 at retrieval  was possible but not choosen.</span>\n",
    "\n",
    "\n",
    "Dealing with null (missing) values is a strategic step in data analysis and not to choose this option would be effectively taking the **Drop missing values  approach** and the dropna() method in pandas will have the same outcome  if deemed appropriate to the emerging goals of the analysis.\n",
    "\n",
    "\n",
    "\n",
    "We can aggregate dataframes latter using our  **groupdf** function so in principle downloading super sets of data is preffered over pre-emptive filtration of data. Having said that, that is **super** with a small 's' as  common sense segregation now introduces an element of organisation and and brings valuable insights.\n",
    "\n",
    "Fist we download a csv file that is likely to give rise to a **response variable**  that is indicative of production in the beef sector and can be normalised for population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production of live animals\n",
    "\n",
    "Crop and livestock statistics  for cattle and meat\n",
    "\n",
    "\n",
    "<br>\n",
    "This file will most likely lead to a predictor variable for the study. This might be in terms of head of cattle or yield.\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we11.png\" alt=\"Image 1\" style=\"width:33.33%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we12.png\" alt=\"Image 2\" style=\"width:33.33%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we13.png\" alt=\"Image 3\" style=\"width:33.33%; height:auto;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"font-size: 18px;\">Figure 4:Filters and Controls for produce_live_animal.csv           3312619 2023-02-15 08:40:53.</span>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Cattle was choosen from the live animals list in the crops and livestock section of the production domain of FAOSTAT and all elements were selected. The file was renamed and downloades as **production_and_slaughter.csv.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Production Livestock Primary Meat data\n",
    "Crop and livestock statistics  for cattle and meat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we83.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we84.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:18px;\">     </span>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"font-size: 18px;\"> Figure 6: Shows that only 'meat of cattle with the bone, fresh or chilled' was the only selection from livestock primary. All elements were selected. The csv was downloaded as  LivestockPrimaryMeat.csv which may provide response and/or predictor   variables  depending on EDA.          </span>\n",
    "\n",
    "\n",
    "produce_primary_meat.csv          7500469   2023-02-15 13:35:40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Production Prices  Data\n",
    "Crop and livestock statistics  for cattle and meat\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we40.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we41.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size: 18px;\">Figure 7:Production value of meat of cattle across 5 units of measurement called elements. ProductionValueMeatofCattle.csv</span>\n",
    "\n",
    "producer_prices.csv               1945297 2023-02-15 18:29:49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Trade Crops and livestock products\n",
    "\n",
    "Trade data for the food and agricultural trade dataset is collected, processed and disseminated by FAO according to the standard International Merchandise Trade Statistics (IMTS) Methodology. The data is mainly provided by UNSD, Eurostat, and other national authorities as needed. This source data is checked for outliers. The trade database includes the following variables: export quantity, export value, import quantity, and import value. The trade database includes all food and agricultural products imported/exported annually by all the countries in the world. \n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we45.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we46.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size: 16px;\">Figure 8 :The trade database includes all food and agricultural products imported/exported annually by all the countries in the world according to the standard International Merchandise Trade Statistics (IMTS) Methodology. Trade.csv</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Trade Indices\n",
    "\n",
    "Trade data for the food and agricultural trade dataset is collected, processed and disseminated by FAO according to the standard International Merchandise Trade Statistics (IMTS) Methodology. The data is mainly provided by UNSD, Eurostat, and other national authorities as needed. This source data is checked for outliers. The trade database includes the following variables: export quantity, export value, import quantity, and import value. The trade database includes all food and agricultural products imported/exported annually by all the countries in the world. \n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we47.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we48.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size: 16px;\">Figure 9 :</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Producer Prices Annual\n",
    "\n",
    "prices_for_producers.csv          1945297 2023-02-15 18:29:49\n",
    "\n",
    "This domain contains data on Agriculture Producer Prices and Producer Price Index. Agriculture Producer Prices are prices received by farmers for primary crops, live animals and livestock primary products as collected at the point of initial sale (prices paid at the farm-gate). Annual data are provided from 1991, while mothly data from January 2010 for 180 country and 212 product. Producer Price Index is the index of agricultural producer prices that measures the average annual change over time in the selling prices received by farmers (prices at the farm-gate or at the first point of sale). The three categories of producer price index available in FAOSTAT comprise: single-item price index, commodity group index and the agriculture producer price index. \n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we49.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we50.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size: 16px;\">Figure11 : producer_prices.csv         </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Consumer Price Indices \n",
    "\n",
    "consumer_price_indices.csv       27918339 2023-02-15 19:20:16\n",
    "\n",
    "\n",
    "A complete and consistent set of time series from **January 2000** onwards. Data gaps on monthly Food CPI and General CPI are filled using statistical estimation procedures to have full data coverage for all countries for Food CPI and for General CPI. These indices measure the price change between the current and reference periods of the average basket of goods and services purchased by households. The CPI,all items is typically **used to measure and monitor inflation**, set monetary policy targets, index social benefits such as pensions and unemployment benefits, and to escalate thresholds and credits in the income tax systems and wages in public and private wage contracts. \n",
    "\n",
    "\n",
    "\n",
    "The FAOSTAT monthly Food CPI inflation rates are annual year-over-year inflation or percentage change over corresponding month of the previous year. The FAOSTAT regional Food CPI and CPI of General are weighted average of Food CPI and General CPI of countries in each region using the weights of HouseHold Final Consumption Expenditure in USD in 2015 from UNSD.\n",
    "\n",
    "Note: This data will need to be reaggregated to annual data.\n",
    "\n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we53.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we54.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size: 16px;\">Figure12 :consumer_price_indices.csv        </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "###   Price Deflators - price_deflators.csv  \n",
    "\n",
    "price_deflators.csv              13862875 2023-02-15 19:29:01\n",
    "\n",
    "\n",
    "The FAOSTAT Deflators database provides the following selection of implicit price deflator series by country and at regional level: Gross Domestic Product (GDP) deflator, Gross Fixed Capital Formation (GFCF) deflator, Agriculture, Forestry, Fishery Value-Added (VA_AFF) deflator, and Manufacturing Valued-Added (VA_MAN) deflator.\n",
    "\n",
    "\n",
    "\n",
    "A deflator is a figure expressing the change in prices over a period of time for a product or a basket of products by comparing a reference period to a base period. It is obtained by dividing a current price value of a given aggregate by its real counterpart. When calculated from the major national accounting aggregates such as GDP or agriculture value added, implicit price deflators pertains to wider ranges of goods and services in the economy than that represented by any of the individual price indexes (such as CPIs, PPIs). Movements in an implicit price deflator reflect both changes in price and changes in the composition of the aggregate for which the deflator is calculated.\n",
    "\n",
    "\n",
    "\n",
    "In the FAOSTAT Deflators database, all series are derived from the United Nations Statistics Division (UNSD) National Accounts Analysis of Main Aggregates database (UNSD AMA). In particular, the implicit GDP deflator, the implicit GFCF deflator, the implicit value added deflator of Agriculture, Forestry, Fishery and the implicit value added deflator of Manufacturing are obtained by dividing the series in current prices by those in constant 2015 prices (base year)\n",
    "\n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we55.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we56.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:18px;\">Figure 15 : Price Deflators.csv        </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Price Exchange Rates  - price_exchange_rates.csv\n",
    "\n",
    "price_exchange_rates.csv          4531028 2023-02-15 19:37:52\n",
    "\n",
    "\n",
    "Exchange rates\n",
    "- Annual exchange rates, Standard Local Currency Units (SLC) per US dollar.\n",
    "\n",
    "- Annual and Monthly exchange rates, Local currency Units (LCU) per US dollar.\n",
    "\n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we57.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we58.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:18px;\">Figure 16 : price_exchange_rates.csv       </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Land Use\n",
    "\n",
    "|CA2-Beef\\rawdata\\land_use.csv  |        7647950 |2023-02-15| 19:45:10|\n",
    "\n",
    "\n",
    "The FAOSTAT Land Use domain contains data on forty-four categories of land use, irrigation and agricultural practices, relevant to monitor agriculture, forestry and fisheries activities at national, regional and global level.\n",
    "\n",
    "Data are available by country and year, with global coverage and annual updates.\n",
    "\n",
    "\n",
    "Food and Agricult\n",
    "\n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we60.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we61.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:18px;\">Figure 16 : price_exchange_rates.csv       </span>\n",
    "\n",
    "Scanned folder: C:\\Users\\ronan\\CA2-Beef\\rawdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Land Cover\n",
    "\n",
    "\n",
    "\n",
    "The FAOSTAT domain Land Cover under the Agri-Environmental Indicators section contains land cover information organized by the land cover classes of the international standard system for Environmental and Economic Accounting Central Framework (SEEA CF). The land cover information is compiled from publicly available Global Land Cover (GLC) maps: a) MODIS land cover types based on the Land Cover Classification System, LCCS (2001–2018) and b) the European Spatial Agency (ESA) Climate Change Initiative (CCI) annual land cover maps (1992–2018) produced by the Université catholique de Louvain (UCL)-Geomatics and now under the European Copernicus Program.\n",
    "\n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we62.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we63.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:18px;\">Figure 16 : price_exchange_rates.csv       </span>\n",
    "\n",
    "Scanned folder: C:\\Users\\ronan\\CA2-Beef\\rawdatam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Input Fertilizers by Nutrient\n",
    "\n",
    "\n",
    "The Fertilizers by Nutrient dataset contains information on the totals in nutrients for Production, Trade and Agriculture Use of inorganic (chemical or mineral) fertilizers, over the time series 1961-present. The data are provided for the three primary plant nutrients: nitrogen (N), phosphorus (expressed as P2O5) and potassium (expressed as K2O). Both straight and compound fertilizers are included.\n",
    "\n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we64.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we65.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:18px;\">Figure: input_fertilizers_by_nutrient      </span>\n",
    "\n",
    "input_fertilizers_by_nutrient.csv    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Input Livestock Manure \n",
    "\n",
    "\n",
    "The Livestock Manure domain of FAOSTAT contains estimates of nitrogen (N) inputs to agricultural soils from livestock manure. Data on the N losses to air and water are also disseminated. These estimates are compiled using official FAOSTAT statistics of animal stocks and by applying the internationally approved Guidelines of the Intergovernmental Panel on Climate Change (IPCC). Data are available by country, with global coverage and relative to the period 1961–2020, with annual updates. The following elements are disseminated: 1) Stocks; 2) Amount excreted in manure (N content); 3) Manure left on pasture (N content); 4) Manure left on pasture that volatilises (N content); 5) Manure left on pasture that leaches (N content); 6) Manure treated (N content); 7) Losses from manure treated (N content); 8) Manure applied to soils (N content); 9) Manure applied to soils that volatilises (N content); 10) Manure applied to soils that leaches (N content)\n",
    "\n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we69.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we70.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:18px;\">Figure: Input Livestock Manure       </span>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Input Pesticides Use \n",
    "\n",
    "\n",
    "The Pesticides Use database includes data on the use of major pesticide groups (Insecticides, Herbicides, Fungicides, Plant growth regulators and Rodenticides) and of relevant chemical families. Data report the quantities (in tonnes of active ingredients) of pesticides used in or sold to the agricultural sector for crops and seeds. Information on quantities applied to single crops is not available. \n",
    "\n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we71.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we72.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:18px;\">Figure: Input Pesticides Use        </span>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population and Employment\n",
    "\n",
    "\n",
    "The FAOSTAT Population module contains time series data on population, by sex and urban/rural. The series consist of both estimates and projections for different periods as available from the original sources, namely:\n",
    "1. Population data refers to the World Population Prospects: The 2019 Revision from the UN Population Division.\n",
    "2. Urban/rural population data refers to the World Urbanization Prospects: The 2018 Revision from the UN Population Division.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we77.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we78.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:18px;\">Figure: Population       </span>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investment Credit to Agriculture\n",
    "\n",
    "The Credit to Agriculture dataset provides national data for over 130 countries on the amount of loans provided by the private/commercial banking sector to producers in agriculture, forestry and fishing, including household producers, cooperatives, and agro-businesses. For some countries, the three subsectors of agriculture, forestry, and fishing are completely specified. In other cases, complete disaggregations are not available. The dataset also provides statistics on the total credit to all industries, indicators on the share of credit to agricultural producers, and an agriculture orientation index (AOI) for credit that normalizes the share of credit to agriculture over total credit by dividing it by the share of agriculture in gross domestic product (GDP). As such, it can provide a more accurate indication of the relative importance that banking sectors place on financing the sector. An AOI lower than 1 indicates that the agriculture sector receives a credit share lower than its contribution to the economy, while an AOI greater than 1 indicates a credit share to the agriculture sector greater than its economic contribution.\n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we79.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we80.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:18px;\">Figure: Investment Credit to Agriculture       </span>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investment Government Expenditure\n",
    "\n",
    "The Statistics Division of FAO collects annually data on Government Expenditure on Agriculture through a questionnaire, which was developed in partnership with the International Monetary Fund (IMF). The IMF is the responsible institution for the Government Finance Statistics (GFS) methodology and annually collects GFS data, including Expenditure by Functions of Government (COFOG). The Classification of the Functions of Government (COFOG) is an international classification developed by Organisation for Economic Co-operation and Development (OECD) and published by the United Nations Statistical Division (UNSD), with the aim of categorise governments' functions according to their purposes. The FAO questionnaire aligns with Table 7 of the IMF GFS questionnaire, replicates the relevant aggregates and drills down to request additional detail related to Agriculture. The FAO dataset consists of a time series, from 2001 onwards, of Total Government Expenditure and expenditure in: Economic affairs; Agriculture, Forestry, Fishing and Hunting, along with its three disaggregated subsectors of Agriculture, Forestry and Fishing; and Environmental Protection. In addition, expenditure in each detailed function are further disaggregated into Recurrent and Capital expenditure. Additional indicators include the Agriculture Share of Government Expenditure, and the Agriculture Orientation Index (ratio between the Agriculture Share of Government Expenditure and the Agriculture Value Added as Share of GDP). Imputation methods were used to provide a complete and consistent global dataset as some challenges arise with different stages of implementation of the GFS methodology and COFOG classification, and differences in the data collection and reporting at country level. Data are reported for the highest level of government available (Consolidated general government, consolidated central government or budgetary central government) and are available for about 100 countries on a regular basis. In some cases (for example, India and Pakistan), data may reflect the general government sector as per national norm. That is, budgetary central government combined with state government. \n",
    "\n",
    "<div style=\"display:flex; flex-direction: row;\">\n",
    "  <img src=\"images/we81.png\" alt=\"Image 1\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "  <img src=\"images/we82.png\" alt=\"Image 2\" style=\"width:45%; height:auto; margin-right:1%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:18px;\">Figure:nvestment Government Expenditure      </span>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trade dataset is collected, processed and disseminated by FAO . The data is mainly provided by UNSD, Eurostat, and other national authorities as needed. This source data is checked for outliers, trade partner data is used for non-reporting countries or missing cells, and data on food aid is added to take into account total cross-border trade flows. The trade database includes the following variables: export quantity, export value, import quantity, and import value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cleaning and Preparing \n",
    "\n",
    "We need to clean and preprocess the data, identify and process missing values, and deal with outliers and inconsistencies before using our raw  data for exploratory data analysis and machine learning. This includes:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Identify and handle missing values: Use pandas functions such as isna(), fillna(), dropna() to identify and handle missing values in the DataFrame.\n",
    "\n",
    "- Drop unnecessary columns: If the DataFrame contains columns that are not needed for the analysis, use the drop() method to drop those columns.\n",
    "\n",
    "- Rename columns: Use the rename() method to rename columns with descriptive labels that are easy to understand.\n",
    "\n",
    "- Convert data types: Use the astype() method to convert columns to the appropriate data type, such as converting numerical columns to int or float.\n",
    "\n",
    "- Remove duplicates: Use the drop_duplicates() method to remove duplicate rows from the DataFrame.\n",
    "\n",
    "- Filter rows: Use pandas functions such as query(), loc[], and iloc[] to filter rows that meet certain criteria.\n",
    "\n",
    "- Group and aggregate data: Use the groupby() method to group the data by one or more columns, and then use aggregation functions such as sum(), mean(), and count() to calculate summary statistics for each group.\n",
    "\n",
    "-  Visualize the data: Use matplotlib, seaborn, or other visualization libraries to create graphs and plots that help you understand the data and communicate your findings.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCKP  Precipitation and Temperature Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain = combine('pr', 'pr_mm/yr')\n",
    "\"\"\"\n",
    "Docstring\n",
    "Args:\n",
    "    csv_folder (str): 'pr' is the  path to the folder pr\n",
    "    that  contains 27 pr_timeseries_annual_cru_1901-2021  CSV files, one per EU country.\n",
    "    column_name (str): The column label must describe that it is rain and the units.\n",
    "        \n",
    "Returns:\n",
    "    pandas.DataFrame: The resulting dataframe after combining data from all CSV files in pr folder, \n",
    "    with the 'key' column indicating the unique combination of file and year for merging to FAOSTAT data and \n",
    " annual reading  column indicating the values from the original CSV file.\n",
    "\n",
    "\"\"\"\n",
    "rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain.to_csv('engineered_data/rain.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Docstring\n",
    "Args:\n",
    "    csv_folder (str): 'tas' is the  path to the folder tas\n",
    "    that  contains 27 tas_timeseries_annual_cru_1901-2021  CSV files, one per EU country.\n",
    "    column_name (str): The column label must describe that it is temperature  in units of °C.\n",
    "        \n",
    "Returns:\n",
    "    pandas.DataFrame: The resulting dataframe after combining data from all CSV files in tas folder, \n",
    "    with the 'key' column indicating the unique combination of file and year for merging to FAOSTAT data and \n",
    " annual reading  column indicating the values from the original CSV file.\n",
    "\n",
    "\"\"\"\n",
    "temp = combine('tas', 'temp_°C')\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv('engineered_data/temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan('rawdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  FAOSTAT Response Variable Main Data Frame \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the raw live animal produce data data file\n",
    "produce = pd.read_csv('rawdata/produce_live_animal.csv', on_bad_lines='skip')\n",
    "df=produce\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_df = df['Element'].unique()\n",
    "element_df                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# element_df = df['Element'].unique()\n",
    "# element_df  \n",
    "\n",
    "grouped_df = df.groupby('Element')\n",
    "production = grouped_df.get_group('Production')\n",
    "stocks = grouped_df.get_group('Stocks')\n",
    "production.to_csv('engineered_data/production.csv', index=False)\n",
    "stocks.to_csv('engineered_data/stocks.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan('engineered_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the query() method EU filter\n",
    "The stocks df is filterd to include only the rows where the 'Area' column  is in \n",
    "the EUROPEAN_UNION.names list of 27 countries.\n",
    "\n",
    "The query string \"Area in @EUROPEAN_UNION.names\" is passed to the query() method.\n",
    "If the 'Area' value is in EUROPEAN_UNION.names list it is kep and otherwise it is dropped.\n",
    "\n",
    "The result of the filtering is stored as a new df called main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stocks.query(\"Area in @EUROPEAN_UNION.names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate a list of European Union countries for the eu function\n",
    "country_list = df['Area'].unique().tolist()\n",
    "country_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Mergable Key Using Concatenate and Astype() Method \n",
    "This code adds a new column called 'key' to the pandas DataFrame main. The values in the new column are calculated by concatenating the values in the 'Area' column and the 'Year' column, after converting the 'Year' column to a string using the astype() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df['key'] = df['Area'] + df['Year'].astype(str) \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Select only the 'Area', 'Year', 'Value', and 'key' columns\n",
    "df = df.loc[:, ['Area', 'Year', 'Value', 'key']]\n",
    "df\n",
    "main=df\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=stocks\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Filter the DataFrame to include only European Union countries\n",
    "df = eu(df)\n",
    "\n",
    "# Add a new 'key' column to the DataFrame\n",
    "key(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract the value from the 'Element' column\n",
    "element_value = df['Element'].iloc[0]\n",
    "\n",
    "# Rename the 'Value' column to the value in the 'Element' column\n",
    "df = df.set_axis([col if col != 'Value' else element_value for col in df.columns], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all open variables\n",
    "print(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = main.rename(columns={'Value': 'cattle_head_count'})\n",
    "main.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the 'eu' function to filter the DataFrame to include only the European Union countries\n",
    "stocks = eu(df[df['Area'].isin(country_list)])\n",
    "stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod=eu(production)\n",
    "prod.sample(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key(production)\n",
    "production.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test if the 'Value' column is empty\n",
    "is_value_empty = production['Value'].isna().any()\n",
    "\n",
    "if is_value_empty:\n",
    "    print(\"The 'Value' column contains empty values.\")\n",
    "else:\n",
    "    print(\"The 'Value' column does not contain empty values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production.info()\n",
    "\n",
    "# No usable data and we move on to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan('rawdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw('price_deflators')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=eu(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=key(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = main.isnull().sum()\n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "msno.matrix(df)\n",
    "\n",
    "\n",
    "\"\"\" \n",
    ". This will display the distribution of missing values across all columns in the DataFrame, \n",
    "with each row representing a different observation or row in the DataFrame, and each column representing \n",
    "a different variable or column in the DataFrame. The matrix will display colored blocks where data is present and white blocks where data\n",
    "is missing. You can use this plot to identify which countries and years have the most missing values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = main.isnull()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows = main.loc[main['cattle_head_count'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(missing_rows.Year, bins=30)\n",
    "plt.xlim(right=2023)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of missing rows')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scan('engineered_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pd.isna(main.loc[main.index[-1], 'cattle_head_count']):\n",
    "    last_missing_index = main.index[-1]\n",
    "else:\n",
    "    last_missing_index = main['cattle_head_count'].last_valid_index() + 1\n",
    "\n",
    "last_missing_year = main.loc[last_missing_index - 1, 'Year']\n",
    "print(\"The last missing value arose in the year:\", last_missing_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count missing values per year\n",
    "missing_values_per_year = main['cattle_head_count'].isnull().groupby(main['Year']).sum()\n",
    "\n",
    "# Create bar plot\n",
    "plt.bar(missing_values_per_year.index, missing_values_per_year.values)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of missing values')\n",
    "plt.title('Missing values in cattle head count per year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = main[main['Year'] >= 1970]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing = master.loc[master['cattle_head_count'].isnull()]\n",
    "missing.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing(main, start_year, end_year):\n",
    "    # Filter for years within the specified range\n",
    "    main = main[(main['Year'] >= start_year) & (main['Year'] <= end_year)]\n",
    "    \n",
    "    # Find missing values\n",
    "    missing = main[main['cattle_head_count'].isnull()]\n",
    "\n",
    "    # Group by year and count missing values\n",
    "    missing_by_year = missing.groupby('Year').size()\n",
    "\n",
    "    # Create bar plot\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(missing_by_year.index, missing_by_year.values)\n",
    "    plt.title(f\"Missing Values in Cattle Head Count from {start_year} to {end_year}\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Missing Values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_missing(main,1961,2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_missing(main,1990,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing = master[master['cattle_head_count'].isnull()]\n",
    "print(missing['Area'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to missing values in Area column\n",
    "missing = master.loc[master['cattle_head_count'].isnull()]\n",
    "\n",
    "# group missing values by year and Area and count them\n",
    "missing_by_year_area = missing.groupby(['Year', 'Area']).size().reset_index(name='count')\n",
    "\n",
    "# create a bar plot of missing values by year and Area\n",
    "sns.barplot(x='Year', y='count', hue='Area', data=missing_by_year_area)\n",
    "\n",
    "# set plot title and labels\n",
    "\n",
    "ax.set_xlim(1993, 1994)\n",
    "ax.set_ylim(0, 30)\n",
    "\n",
    "\n",
    "plt.title('Missing Cattle Head Count by Year and Area')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Missing Count')\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter to missing values in Area column\n",
    "missing = master.loc[master['cattle_head_count'].isnull()]\n",
    "\n",
    "# group missing values by year and Area and count them\n",
    "missing_by_year_area = missing.groupby(['Year', 'Area']).size().reset_index(name='count')\n",
    "\n",
    "# create a bar plot of missing values by year and Area\n",
    "ax = sns.barplot(x='Year', y='count', hue='Area', data=missing_by_year_area)\n",
    "\n",
    "# set axis limits\n",
    "ax.set_xlim(1988, 1993)\n",
    "ax.set_ylim(0, 5)\n",
    "\n",
    "# set plot title and labels\n",
    "plt.title('Missing Cattle Head Count by Year and Area')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Missing Count')\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_by_year_area.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = main[main['Year'] >= 2000]\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write(missing_by_year_area,'missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scan('engineered_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineplots of the top 5 producers\n",
    "\n",
    "Ireland has moved up the rankings  from 5th to 3rd in terms of cattle head count overtakig Poland and Italy at about the tun of the centuary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data by key and calculate the mean cattle head count for each group\n",
    "grouped_data = main.groupby('key')['cattle_head_count'].mean().reset_index()\n",
    "\n",
    "# sort groups by average cattle head count in descending order\n",
    "sorted_groups = grouped_data.sort_values(by='cattle_head_count', ascending=False)\n",
    "\n",
    "# get the sorted key column\n",
    "sorted_key = sorted_groups['key']\n",
    "# group data by Area and Year, and calculate the sum of cattle_head_count for each group\n",
    "grouped_data = main.groupby(['Area', 'Year'])['cattle_head_count'].sum().reset_index()\n",
    "\n",
    "# get the total cattle head count for each Area\n",
    "area_totals = grouped_data.groupby('Area')['cattle_head_count'].sum()\n",
    "\n",
    "# sort areas by total cattle head count and get the top 5 areas\n",
    "top_areas = area_totals.sort_values(ascending=False)[:5].index.tolist()\n",
    "\n",
    "# filter data to include only the top 10 areas\n",
    "filtered_data = grouped_data[grouped_data['Area'].isin(top_areas)]\n",
    "\n",
    "# create a line plot with a different color for each Area\n",
    "sns.lineplot(data=filtered_data, x='Year', y='cattle_head_count', hue='Area')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df1.shape\n",
    "#  df1.isna().sum()\n",
    "#  yield_df.describe(include=object)\n",
    "#  duplicate_df = df1[df1.duplicated()]\n",
    "#  df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=year(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanraw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('rawdata/produce_primary_meat.csv')\n",
    "df=key(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = df['Element'].unique()\n",
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=eu(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=year(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scanraw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element_df = df['Element'].unique()\n",
    "# element_df  \n",
    "\n",
    "grouped_df = df.groupby('Element')\n",
    "production = grouped_df.get_group('Production')\n",
    "stocks = grouped_df.get_group('Stocks')\n",
    "production.to_csv('engineered_data/production.csv', index=False)\n",
    "stocks.to_csv('engineered_data/stocks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1.rename(columns = {'Value':'yield_hg_An'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the columns to keep\n",
    "columns_to_keep = [\"Key\",\"Year\",\"Area\",\"yield_hg_An\"] #faster than drop function here\n",
    "# Create a new dataframe with only the specified columns\n",
    "df1 = df1[columns_to_keep]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"df1.csv\", index=False)\n",
    "merged_df=df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### EDA code lines commented out below and the distribution of yields  all indicated that no cleaning or imputation was required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "   \n",
    "    plt.figure(figsize=(10, 20))\n",
    "    sns.displot(df1, x=\"yield_hg_An\")\n",
    "    plt.savefig(\"Yield_distribution.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract values of a specific column\n",
    "column = df1['yield_hg_An']\n",
    "\n",
    "# Perform the Shapiro-Wilk test\n",
    "stat, p = shapiro(column)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p < alpha:\n",
    "    print(\"Sample looks Gaussian (fail to reject H0)\")\n",
    "else:\n",
    "    print(\"Sample does not look Gaussian (reject H0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point I considered writing  a function for merging further\n",
    "# csv data but the human intervention for renaming value fields mitigated against it.\n",
    "\n",
    "df2 = pd.read_csv('FAOSTAT\\produce_primary_slaughtered.csv', on_bad_lines='skip')\n",
    "df2= df2[(df2['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df2['Key'] = df2['Area'] + df2['Year'].astype(str) # add a merging key\n",
    "df2.rename(columns = {'Value':'Slaughter'}, inplace = True)\n",
    "\n",
    "df0 = df1.merge(df2[['Key','Year','Slaughter']], on='Key')\n",
    "df0.head()\n",
    "\n",
    "# Apologies for ordinal indescrepancy on namin dataframes   \n",
    "# I used df0 to fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(data=df0, x=\"Slaughter\",  kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# At this point I considered writing  a function for merging further\n",
    "# csv data but the human intervention for renaming value fields mitigated against it.\n",
    "\n",
    "df2 = pd.read_csv('FAOSTAT\\produce_primary_slaughtered.csv', on_bad_lines='skip')\n",
    "df2= df2[(df2['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df2['Key'] = df2['Area'] + df2['Year'].astype(str) # add a merging key\n",
    "df2.rename(columns = {'Value':'Slaughter'}, inplace = True)\n",
    "\n",
    "merged_df = df1.merge(df2[['Key', 'Slaughter']], on='Key')\n",
    "merged_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 1990\n",
    "\n",
    "# # Create the boolean mask\n",
    "# mask = df0[\"Year\"] > threshold\n",
    "\n",
    "# # Filter the DataFrame using the mask\n",
    "# filtered_df = df0[mask]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(df0, x=\"Slaughter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('FAOSTAT\\produce_primary_quantity.csv', on_bad_lines='skip')\n",
    "df3= df3[(df3['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df3['Key'] = df3['Area'] + df3['Year'].astype(str) # add a merging key\n",
    "\n",
    "# df3['Value'] = df['Value'].astype(int)\n",
    "\n",
    "df3['Beef (T)'] = (df3['Value'] ).astype(int)\n",
    "df3 # take a look\n",
    "\n",
    "df4 = merged_df.merge(df3[['Key', 'Beef (T)']], on='Key')\n",
    "df4.head()\n",
    "# msno.matrix(df4)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('FAOSTAT\\produce_live_animals.csv', on_bad_lines='skip')\n",
    "df5= df5[(df5['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df5['Key'] = df5['Area'] + df5['Year'].astype(str) # add a merging key\n",
    "df5.rename(columns = {'Value':'Live Animals'}, inplace = True)\n",
    "df5 # take a look\n",
    "df6 = df4.merge(df5[['Key', 'Live Animals']], on='Key')\n",
    "df6.head() \n",
    "# msno.matrix(df6)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The price a farmer gets for her meat ppi should be an influencial predictor variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.read_csv('FAOSTAT\\price_meat_ppi.csv', on_bad_lines='skip')\n",
    "df7\n",
    "# msno.matrix(df7)  # ppi is dimensionless so NaN for units is not an issue\n",
    "df7= df7[(df7['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df7['Key'] = df7['Area'] + df7['Year'].astype(str) # add a merging key\n",
    "df7.rename(columns = {'Value':'PPI'}, inplace = True)\n",
    "df7 # take a look\n",
    "df8 = df6.merge(df7[['Key', 'PPI']], on='Key')\n",
    "df8.head()\n",
    "# msno.matrix(df8)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shit happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = pd.read_csv('FAOSTAT\\manure_nondairy_lefton_withNulls.csv', on_bad_lines='skip')\n",
    "df9= df9[(df9['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df9['Key'] = df9['Area'] + df9['Year'].astype(str) # add a merging key\n",
    "# convert from kg to tonnes and round for readibility\n",
    "df9['MOn(T)'] = (df9['Value'] / 1000).round(0).astype(int)\n",
    "df9.sample(3)\n",
    "df10 = df8.merge(df9[['Key', 'MOn(T)']], on='Key') # Man_on is not from sport but means manure left on pasture\n",
    "df10.head(4)\n",
    "# msno.matrix(df10)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df9 = pd.read_csv('FAOSTAT\\manure_nondairy_lefton.csv', on_bad_lines='skip')\n",
    "# df9= df9[(df9['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "# df9['Key'] = df9['Area'] + df9['Year'].astype(str) # add a merging key\n",
    "# # convert from kg to tonnes and round for readibility\n",
    "# df9['MOn(T)'] = (df9['Value'] / 1000).round(0).astype(int)\n",
    "# df9.sample(3)\n",
    "# df10 = df8.merge(df9[['Key', 'MOn(T)']], on='Key') # Man_on is not from sport but means manure left on pasture\n",
    "# df10.head(4)\n",
    "# # msno.matrix(df10)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = pd.read_csv('FAOSTAT\\manure_nondairy_applied.csv', on_bad_lines='skip')\n",
    "df11= df11[(df11['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df11['Key'] = df11['Area'] + df11['Year'].astype(str) # add a merging key\n",
    "# convert from kg to tonnes and round for readibility\n",
    "df11['MAp(T)'] = (df11['Value'] / 1000).round(0).astype(int)\n",
    "# df11.sample(3) # Look at a sample\n",
    "df12 = df10.merge(df11[['Key', 'MAp(T)']], on='Key') # Manure applied in tonnes is appended\n",
    "df12.sample(4)\n",
    "# msno.matrix(df12)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df13 = pd.read_csv('FAOSTAT\\livestock_density.csv', on_bad_lines='skip')\n",
    "# # msno.matrix(df13)  # ppi is dimensionless so NaN for units is not an issue\n",
    "df13= df13[(df13['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df13['Key'] = df13['Area'] + df13['Year'].astype(str) # add a merging key\n",
    "df13.rename(columns = {'Value':'LSU/ha'}, inplace = True)\n",
    "# df13 # take a look\n",
    "df14 = df12.merge(df13[['Key', 'LSU/ha']], on='Key')\n",
    "df14.head()\n",
    "# msno.matrix(df14)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df15 = pd.read_csv('FAOSTAT\\land-temp.csv', on_bad_lines='skip')\n",
    "# # msno.matrix(df15)  # ppi is dimensionless so NaN for units is not an issue\n",
    "df15= df15[(df15['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df15['Key'] = df15['Area'] + df15['Year'].astype(str) # add a merging key\n",
    "# df15 # take a quick look\n",
    "df15['Tem kha'] = (df15['Value'] ).astype(int)\n",
    "# \n",
    "\n",
    "# print(df15['Tem kha'].unique())  # No NaNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16 = df14.merge(df15[['Key', 'Tem kha']], on='Key')\n",
    "df16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(df16, x=\"Tem kha\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df17 = pd.read_csv('FAOSTAT\\land-perm.csv', on_bad_lines='skip')\n",
    "# # msno.matrix(df17)  # ppi is dimensionless so NaN for units is not an issue\n",
    "df17= df17[(df17['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df17['Key'] = df17['Area'] + df17['Year'].astype(str) # add a merging key\n",
    "df17['Per kha'] = (df17['Value'] ).astype(int)\n",
    "# df17 # take a look\n",
    "df18 = df16.merge(df17[['Key', 'Per kha']], on='Key')\n",
    "df18.head()\n",
    "msno.matrix(df18)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df19 = pd.read_csv('FAOSTAT\\land-agri.csv', on_bad_lines='skip')\n",
    "# # msno.matrix(df19)  # ppi is dimensionless so NaN for units is not an issue\n",
    "df19= df19[(df19['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df19['Key'] = df19['Area'] + df19['Year'].astype(str) # add a merging key\n",
    "df19['Ag_kha'] = (df19['Value']).astype(int)\n",
    "df20 = df18.merge(df19[['Key','Ag_kha']], on='Key')\n",
    "df20.head()\n",
    "# # msno.matrix(df20)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(df20, x=\"LSU/ha\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df21 = pd.read_csv('FAOSTAT\\import_value.csv', on_bad_lines='skip')\n",
    "# # msno.matrix(df21)  # ppi is dimensionless so NaN for units is not an issue\n",
    "df21= df21[(df21['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df21['Key'] = df21['Area'] + df21['Year'].astype(str) # add a merging key\n",
    "df21.rename(columns = {'Value':'Imp$1000'}, inplace = True)\n",
    "df22 = df20.merge(df21[['Key', 'Imp$1000']], on='Key')\n",
    "df22.head()\n",
    "# # msno.matrix(df22)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df23 = pd.read_csv('FAOSTAT\\import_quantity.csv', on_bad_lines='skip')\n",
    "# # msno.matrix(df23)  # ppi is dimensionless so NaN for units is not an issue\n",
    "df23= df23[(df23['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df23['Key'] = df23['Area'] + df23['Year'].astype(str) # add a merging key\n",
    "df23\n",
    "df23.rename(columns = {'Value':'Imp#'}, inplace = True)\n",
    "df24 = df22.merge(df23[['Key', 'Imp#']], on='Key')\n",
    "df24.head()\n",
    "# msno.matrix(df24)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df25 = pd.read_csv('FAOSTAT\\gross_meat_production.csv', on_bad_lines='skip')\n",
    "# # msno.matrix(df25)  # ppi is dimensionless so NaN for units is not an issue\n",
    "df25= df25[(df25['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df25['Key'] = df25['Area'] + df25['Year'].astype(str) # add a merging key\n",
    "df25\n",
    "# print(df25['Value'].unique())\n",
    "df25.rename(columns = {'Value':'1000SLC'}, inplace = True)\n",
    "df26 = df24.merge(df25[['Key', '1000SLC']], on='Key')\n",
    "df26.head()\n",
    "\n",
    "# # msno.matrix(df26)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df26.to_csv('data\\data26.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rawdata=os.listdir('FAOSTAT')\n",
    "rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df27 = pd.read_csv('FAOSTAT\\p2o5.csv', on_bad_lines='skip') # Fertiliser tonnes of p2o5 (phosphate)\n",
    "# # msno.matrix(df27)  # ppi is dimensionless so NaN for units is not an issue\n",
    "df27= df27[(df27['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df27['Key'] = df27['Area'] + df27['Year'].astype(str) # add a merging key\n",
    "df27.rename(columns = {'Value':'p2o5-T'}, inplace = True)\n",
    "df27['p2o5-T'] = df27['p2o5-T'].astype(int)  # Converts phosphate float column to an integer number of tonnes\n",
    "df28 = df26.merge(df27[['Key', 'p2o5-T']], on='Key')\n",
    "df28.head()\n",
    "# msno.matrix(df28)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(df27)\n",
    "# # # msno.matrix(df16)  # When uncommented indicates no missing data \n",
    "df27.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df29 = pd.read_csv('FAOSTAT\\k2o.csv', on_bad_lines='skip') # Fertiliser tonnes of k20 (phosphate)\n",
    "df29= df29[(df29['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df29['Key'] = df29['Area'] + df29['Year'].astype(str) # add a merging key\n",
    "df29['k2o-T'] = (df29['Value'] ).astype(int)\n",
    "df29.fillna(0, inplace=True)\n",
    "df30 = df28.merge(df29[['Key', 'k2o-T']], on='Key')\n",
    "df30.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata=os.listdir('FAOSTAT')\n",
    "rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df40= pd.read_csv('FAOSTAT\\FAOSTAT_data_en_2-6-2023 (4).csv', on_bad_lines='skip') \n",
    "\n",
    "\n",
    "# dfA = pd.read_csv('FAOSTAT\\FAOSTAT_data_en_2-6-2023 (4).csv', on_bad_lines='skip') \n",
    "# dfA\n",
    "\n",
    "\n",
    "df40= df40[(df40['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "\n",
    "df40['Key'] = df40['Area'] + df40['Year'].astype(str) # add a merging key\n",
    "df40\n",
    "df40['N2_T'] = (df40['Value'] ).astype(int)\n",
    "# df40.rename(columns = {'Value':'n-T'}, inplace = True)\n",
    "df41 = df30.merge(df40[['Key', 'N2_T']], on='Key')\n",
    "df41.head()\n",
    "# # msno.matrix(df28)  # When uncommented indicates no missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df31 = pd.read_csv('FAOSTAT\\export-value.csv', on_bad_lines='skip') # Fertiliser tonnes of k20 (phosphate)\n",
    "df31= df31[(df31['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df31['Key'] = df31['Area'] + df31['Year'].astype(str) # add a merging key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df31\n",
    "df31['ex$1000'] = (df31['Value'] ).astype(int)\n",
    "df31.head()\n",
    "# msno.bar(df)\n",
    "#msno.matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df31.rename(columns = {'Value':'export-value-T'}, inplace = True)\n",
    "df32 = df41.merge(df31[['Key', 'ex$1000']], on='Key')\n",
    "df32.head()\n",
    "# print(df32['Ex$1000'].unique())\n",
    "\n",
    "# msno.matrix(df32)  # When uncommented indicates no missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df33 = pd.read_csv('FAOSTAT\\export-quantity.csv', on_bad_lines='skip') # Fertiliser tonnes of k20 (phosphate)\n",
    "df33= df33[(df33['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df33['Key'] = df33['Area'] + df33['Year'].astype(str) # add a merging key\n",
    "\n",
    "# print(df33['Value'].unique())\n",
    "df33.rename(columns = {'Value':'ex#'}, inplace = True)\n",
    "df34 = df32.merge(df33[['Key', 'ex#']], on='Key')\n",
    "df34.head()\n",
    "fao_df=df34\n",
    "fao_df # easier to remember later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df53 = pd.read_csv('FAOSTAT\\population.csv', on_bad_lines='skip') # Fertiliser tonnes of k20 (phosphate)\n",
    "df53= df53[(df53['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "df53['Key'] = df53['Area'] + df53['Year'].astype(str) # add a merging key\n",
    "df53\n",
    "\n",
    "\n",
    "\n",
    "df53.rename(columns = {'Value':'Pop'}, inplace = True)\n",
    "df54 = df32.merge(df53[['Key', 'Pop']], on='Key')\n",
    "df54.head()\n",
    "fao_df=df54\n",
    "fao_df # easier to remember later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df53 = pd.read_csv('FAOSTAT\\population.csv', on_bad_lines='skip') # Fertiliser tonnes of k20 (phosphate)\n",
    "# df53= df53[(df53['Area'].isin(EUROPEAN_UNION.names))] #EU only\n",
    "# df53['Key'] = df53['Area'] + df53['Year'].astype(str) # add a merging key\n",
    "# df53.rename(columns = {'Value':'Pop'}, inplace = True)\n",
    "# df54 = df32.merge(df53[['Key', 'Pop']], on='Key')\n",
    "# df54['Pop'] = df54['pop'].astype(int)  # Converts phosphate float column to an integer number of tonnes\n",
    "# df54.head()\n",
    "# fao_df=df54\n",
    "# fao_df.head() # easier to remember later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fao_df['SL/kpop'] = fao_df['Slaughter'] / fao_df['Pop']\n",
    "fao_df['SL/kpop'] = fao_df['SL/kpop'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fao_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(fao_df)  # This matrix demonstrates the quality of the FAO data \n",
    "# and only a few NaN needed to be replaced by zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.displot(fao_df, x=\"SL/kpop\")\n",
    "plt.show()\n",
    "# For beef production at its most intense there is one carcuss produced per two people per year!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the  CCKP weather \n",
    "\n",
    "data was a little trickier because they are aggregated on a per country basis annually and so as well as downlaoding them individually we need to concatenate their values to single files\n",
    "\n",
    "A number of custom functions  and list creation will help!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rainfiles=os.listdir('cckp-rain')\n",
    "rainfiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan('cckp-rain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempfiles=os.listdir('cckp-temp')\n",
    "tempfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We explore a rain file to understand the structure by copy pasting its long name from above list\n",
    "\n",
    "df35 = pd.read_csv('cckp-rain\\pr_timeseries_annual_cru_1901-2021_AUT.csv', on_bad_lines='skip') \n",
    "# sample precipitation rain (pr)file\n",
    "df35.head() \n",
    "# We can see that rainfall is given for major cities but this is not of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to process the CCKP csv files to suitable format\n",
    "def process(file, col_name):\n",
    "    df = pd.read_csv(file, on_bad_lines='skip', skiprows=1)\n",
    "    df.rename(columns={'Unnamed: 0': 'Year'}, inplace=True)\n",
    "    #Columate the country code from its single entry\n",
    "    df['Key'] = df.columns[1] + df['Year'].astype(str)  # pluck the country with absolute referencing\n",
    "    df.rename(columns={df.columns[1]: col_name}, inplace=True)\n",
    "    df = df.filter(['Key', col_name] )\n",
    "    return df\n",
    "\n",
    "#function to create a dataframe from all of the csv files in a folder, and apply the pre processing steps \n",
    "#detailed in the process function\n",
    "\n",
    "def files_to_df(path, col_name):\n",
    "    csv_files = glob.glob(path + \"/*.csv\")\n",
    "    df_list = (process(filename, col_name) for filename in csv_files)\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "#creating a dataframe for the rain data\n",
    "rain_df = files_to_df('cckp-rain', 'Rainfall_mm/yr')\n",
    " \n",
    "#creating a dataframe for the temp data\n",
    "temp_df = files_to_df('cckp-temp', 'Temp_C')\n",
    "\n",
    "#We can form a list of EU countries from any dataframe from the FAOSTAT data\n",
    "\n",
    "df3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = df3['Area'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now we define our column value splitting function\n",
    "def reformat(dataframe, column):\n",
    "    container = {} #create an empty dictionary to store the dataframes\n",
    "    for i in dataframe[column].unique(): #loop through the unique values in the column of interest\n",
    "        container[f'{i}'] = dataframe[(dataframe[column] == i) & (dataframe['Area'].isin(countries))] #create a dataframe for each unique value in the column of interest\n",
    "        container[f'{i}']['Key'] = container[f'{i}']['Area'] + container[f'{i}']['Year'].astype(str)   #create a key column to merge the dataframes later\n",
    "\n",
    "    for i in container: #loop through dataframes in the dict and apply some conditions\n",
    "        container[i] = container[i][['Key', 'Area', 'Value']]#filter the dataframes to only include the key, area and value columns\n",
    "        container[i].rename(columns={'Value': f'{i}'}, inplace=True)#rename the value column to the name of the item\n",
    "\n",
    "\n",
    "    my_reduce = partial(pd.merge, on=['Key', 'Area'], how='right')  #create a function to merge the dataframes in the dict                                                           \n",
    "    df =  reduce(my_reduce, container.values()) #calling that function on the values in the dict\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create a dataframe from all of the csv files in a folder, and apply the pre processing steps \n",
    "#detailed in the process function\n",
    "def files_to_df(path, col_name):\n",
    "    csv_files = glob.glob(path + \"/*.csv\")\n",
    "    df_list = (process(filename, col_name) for filename in csv_files)\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe for the rain\n",
    "rain_df = files_to_df('cckp-rain', 'Rain_mm/yr')\n",
    "rain_df.head()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe for the temperature\n",
    "temp_df = files_to_df('cckp-temp', 'Temp_C')\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Penultimate data wrangling step merges  rain data to faostat data\n",
    "\n",
    "fao_with_rain_df = fao_df.merge(rain_df[['Key', 'Rain_mm/yr']], on='Key')\n",
    "fao_with_rain_df.head() # one predictor to go now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally\n",
    "# Drum Role PLease!!!\n",
    "# fao_with_rain merges with temp data into master_df\n",
    "\n",
    "\n",
    "master_df = fao_with_rain_df.merge(temp_df[['Key', 'Temp_C']], on='Key')\n",
    "master_df.head() # one predictor to go now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_df.head(11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "master_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data\\master.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_df.isna().sum() # Confirms we do not have any missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.sample(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Data cleaning & feature engineering stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets kick things off by building a function that will loop over many imputation techniques and will compare their results, spitting out the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dropping redundant columns\n",
    "# milk_eu.drop(['Domain Code', 'Domain', 'Area Code (M49)',\n",
    "#  'Element Code', 'Element', 'Item Code (CPC)', 'Item', 'Year Code', 'Flag', 'key', 'Flag Description'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wilcoxon test\n",
    "from scipy.stats import wilcoxon\n",
    "def impute_data(milk_eu, columns, k, iter, target):\n",
    "    #defining the imputation methods\n",
    "    imputation_type = ['.mean()', '.mode()', '.median()','bfill', 'ffill', 'knn', 'mice']\n",
    "    #dropping null columns and making nan values are np.nan format\n",
    "    master_df = master_df.fillna(np.nan).drop(columns, axis=1).select_dtypes(exclude=['object'])\n",
    "    results = {}#creating a dictionary to store the results\n",
    "    best = {}#creating a dictionary to store the best results\n",
    "\n",
    "    \n",
    "    \n",
    "    #defining the mannwhitneyu test\n",
    "    def score_dataset(milk_eu, impute):\n",
    "        #take a random sample of n = 500 from the dataset\n",
    "        sample_milk = milk_eu.dropna().sample(n=700, random_state=0)\n",
    "        sample_imp = impute.dropna().sample(n=700, random_state=0)ss\n",
    "        u, p = wilcoxon(sample_milk, sample_imp)\n",
    "\n",
    "        return p #returning the p value\n",
    "\n",
    "    #looping through the columns to be imputed\n",
    "    for col in columns:\n",
    "\n",
    "        #defining the imputation methods in a dictionary to allow iteration\n",
    "        #this is done to avoid having to write out each method individually\n",
    "        functions = {\n",
    "        'mean': milk_eu[col].mean(),\n",
    "        'mode': milk_eu[col].mode(),\n",
    "        'median': milk_eu[col].median(),\n",
    "        'bfill': milk_eu[col].fillna(method='bfill'),\n",
    "        'ffill': milk_eu[col].fillna(method='ffill')\n",
    "    }\n",
    "\n",
    "        #looping through the imputation methods\n",
    "        for type in imputation_type:\n",
    "\n",
    "            if type in functions:#if the imputation method is in the dictionary\n",
    "                impute = functions[type]\n",
    "                results[f'{type}_{col}'] = score_dataset(milk_eu[col], impute)#adding the results to the results dictionary\n",
    "\n",
    "            elif type == 'knn':#if the imputation method is knn\n",
    "                for i in k:#looping through the k values\n",
    "                    milk_eu_imp[col] = milk_eu[col].fillna(np.nan)\n",
    "                    impute = pd.DataFrame(fancyimpute.KNN(k=i, verbose=False).fit_transform(milk_eu_imp))\n",
    "                    impute.columns = milk_eu_imp.columns #setting the column names to the original column names\n",
    "                    results[f'{type}_{i}_{col}'] = score_dataset(milk_eu[col], impute[col])#adding the results to the results dictionary\n",
    "\n",
    "            elif type == 'mice':#if the imputation method is mice\n",
    "                for iterations in iter:#looping through the iterations\n",
    "                    milk_eu_imp[col] = milk_eu[col].fillna(np.nan)\n",
    "                    imputed = pd.DataFrame(fancyimpute.IterativeImputer(max_iter=iterations).fit_transform(milk_eu_imp))\n",
    "                    impute.columns = milk_eu_imp.columns #setting the column names to the original column names\n",
    "                    results[f'{type}_{iterations}_{col}'] = score_dataset(milk_eu[col], impute[col]) #adding the results to the results dictionary \n",
    "\n",
    "        #put max value in 'best' dict, this returrns the best imputation method\n",
    "        best[col] = max(results, key=results.get)\n",
    "        \n",
    "    return results, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) First we will deal with the missing values using the method constructed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover=readr('land_cover','cover')\n",
    "cover.head(2\n",
    "writee(cover,'cover')\n",
    "scane()\n",
    "\n",
    "missing=reade('missing','missing')\n",
    "missing\n",
    "\n",
    "lu = readr('land_use', 'lu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents (Clickable in sidebar)",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "2237.31px",
    "left": "1138px",
    "top": "201px",
    "width": "254.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "890563eb1401dd7c5eac482b2070a231034cb0eabe59bf1a3eb86f9e36919f52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
