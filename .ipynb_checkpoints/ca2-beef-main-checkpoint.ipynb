{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "## Analysis and comparison of the Irish Beef Sector with leading EU countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and setting warnings and display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import warnings\n",
    "import glob\n",
    "import requests\n",
    "from countrygroups import EUROPEAN_UNION\n",
    "from countryinfo import CountryInfo\n",
    "from functools import partial, reduce \n",
    "import missingno as msno\n",
    "import os\n",
    "import pycountry     #297\n",
    "\n",
    "############ break\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import fancyimpute\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "\n",
    "#managing warnings(ignoring them mostly)\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)     \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#setting display options\n",
    "pd.set_option('display.max_columns', 200)\n",
    "plt.rcParams['figure.figsize'] = (25, 14)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "from matplotlib.pyplot import cm\n",
    "color = 'tab20c'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions files_to_df, process and reformat  defined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function with argument of folder followed by units.\n",
    "def files_to_df(path, col_name):\n",
    "    csv_files = glob.glob(path + \"/*.csv\")\n",
    "    df_list = (process(filename, col_name) for filename in csv_files)\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "#Formatting function for shaping data\n",
    "def process(file, col_name):\n",
    "    df = pd.read_csv(file,skiprows=1)      \n",
    "    df.rename(columns={'Unnamed: 0': 'Year'}, inplace=True)\n",
    "    #add acolumn that contains country code in each cell\n",
    "    df['key'] = df.columns[1] + df['Year'].astype(str)\n",
    "    df.rename(columns={df.columns[1]: col_name}, inplace=True)\n",
    "    df = df.filter(['key', col_name] )\n",
    "    return df\n",
    "\n",
    "# Column splitting function ( more transparent than pivot!)\n",
    "def reformat(dataframe, column):\n",
    "    container = {} #create an empty dictionary to store the dataframes\n",
    "    for i in dataframe[column].unique(): #loop through the unique values in the column of interest\n",
    "        container[f'{i}'] = dataframe[(dataframe[column] == i) & (dataframe['Area'].isin(countries))] #create a dataframe for each unique value in the column of interest\n",
    "        container[f'{i}']['key'] = container[f'{i}']['Area'] + container[f'{i}']['Year'].astype(str)   #create a key column to merge the dataframes later\n",
    "\n",
    "    for i in container: #loop through dataframes in the dict and apply some conditions\n",
    "        container[i] = container[i][['key', 'Area', 'Value']]#filter the dataframes to only include the key, area and value columns\n",
    "        container[i].rename(columns={'Value': f'{i}'}, inplace=True)#rename the value column to the name of the item\n",
    "\n",
    "\n",
    "    my_reduce = partial(pd.merge, on=['key', 'Area'], how='right')  #create a function to merge the dataframes in the dict                                                           \n",
    "    df =  reduce(my_reduce, container.values()) #calling that function on the values in the dict\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of cattle, land, price, export, manure, fertiliser, rain and temperature data\n",
    "\n",
    "csv files in the rawdata folder were downloaded from FAOSTAT.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=os.listdir(rawdata)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# importing and preparing cattle FAOSTAT data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cattle_global_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrawdata/cattle_global.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#read\u001b[39;00m\n\u001b[0;32m      3\u001b[0m cattle_df\u001b[38;5;241m=\u001b[39m cattle_global_df[(cattle_global_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArea\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(EUROPEAN_UNION\u001b[38;5;241m.\u001b[39mnames))] \u001b[38;5;66;03m#European contries only\u001b[39;00m\n\u001b[0;32m      4\u001b[0m cattle_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cattle_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArea\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m cattle_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;66;03m# key for merging later\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# importing and preparing cattle FAOSTAT data\n",
    "cattle_global_df = pd.read_csv('rawdata/cattle_global.csv') #read\n",
    "cattle_df= cattle_global_df[(cattle_global_df['Area'].isin(EUROPEAN_UNION.names))] #European contries only\n",
    "cattle_df['key'] = cattle_df['Area'].astype(str) + cattle_df['Year'].astype(str) # key for merging later\n",
    "cattle_df.to_csv('data/cattle.csv', index=False) # write csv to data folder\n",
    "cattle_df = cattle_df[['key', 'Value','Flag','Flag Description']]\n",
    "cattle_df  # Take a look or comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>Temporary</th>\n",
       "      <th>Permanent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1801.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austria1962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1785.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria1963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1768.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austria1964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1752.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austria1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1736.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           key  Temporary  Permanent\n",
       "0  Austria1961        0.0     1801.4\n",
       "1  Austria1962        0.0     1785.0\n",
       "2  Austria1963        0.0     1768.8\n",
       "3  Austria1964        0.0     1752.5\n",
       "4  Austria1965        0.0     1736.2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lands_df = pd.read_csv('rawdata/lands.csv')     #read raw land data\n",
    "#calling the function on the lands_df data and renamimng land_df\n",
    "land_df = reformat(lands_df, 'Item')\n",
    "#replace NaN with 0 in the temporary pasture column,and interpret as a zero value\n",
    "land_df['Land under temp. meadows and pastures'].fillna(0, inplace=True) \n",
    "#rename specific long column names\n",
    "land_df.rename(columns = {'Land under temp. meadows and pastures':'Temporary', 'Land under perm. meadows and pastures':'Permanent'}, inplace = True)\n",
    "land_df= land_df[(land_df['Area'].isin(EUROPEAN_UNION.names))] # Filter to Europe\n",
    "land_df.to_csv('data/land.csv', index=False)\n",
    "land_df.sample(11) \n",
    "#filter to key and temporary and permanant land usage\n",
    "land_df = land_df[['key', 'Temporary','Permanent']]\n",
    "#rename value to land use Kha\n",
    "# land = land.rename(columns={'Value': 'Land Use Kha'})\n",
    "land_df.head()\n",
    "#now we can merge the two dataframes\n",
    "# cattle_merged_df = pd.merge(cattle_df, land_df, on='key')\n",
    "# cattle_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>Value</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Flag Description</th>\n",
       "      <th>Temporary</th>\n",
       "      <th>Permanent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria1961</td>\n",
       "      <td>2386761</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1801.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austria1962</td>\n",
       "      <td>2456557</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1785.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria1963</td>\n",
       "      <td>2437123</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1768.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austria1964</td>\n",
       "      <td>2310667</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1752.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austria1965</td>\n",
       "      <td>2350269</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1736.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>Sweden2016</td>\n",
       "      <td>1436050</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "      <td>1052.56</td>\n",
       "      <td>451.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>Sweden2017</td>\n",
       "      <td>1448590</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "      <td>1035.11</td>\n",
       "      <td>452.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>Sweden2018</td>\n",
       "      <td>1435450</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "      <td>1048.39</td>\n",
       "      <td>455.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>Sweden2019</td>\n",
       "      <td>1404670</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "      <td>1084.50</td>\n",
       "      <td>461.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>Sweden2020</td>\n",
       "      <td>1390960</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "      <td>1064.81</td>\n",
       "      <td>463.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1263 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              key    Value Flag Flag Description  Temporary  Permanent\n",
       "0     Austria1961  2386761    A  Official figure       0.00    1801.40\n",
       "1     Austria1962  2456557    A  Official figure       0.00    1785.00\n",
       "2     Austria1963  2437123    A  Official figure       0.00    1768.80\n",
       "3     Austria1964  2310667    A  Official figure       0.00    1752.50\n",
       "4     Austria1965  2350269    A  Official figure       0.00    1736.20\n",
       "...           ...      ...  ...              ...        ...        ...\n",
       "1258   Sweden2016  1436050    A  Official figure    1052.56     451.94\n",
       "1259   Sweden2017  1448590    A  Official figure    1035.11     452.94\n",
       "1260   Sweden2018  1435450    A  Official figure    1048.39     455.14\n",
       "1261   Sweden2019  1404670    A  Official figure    1084.50     461.28\n",
       "1262   Sweden2020  1390960    A  Official figure    1064.81     463.52\n",
       "\n",
       "[1263 rows x 6 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we can merge the two dataframes\n",
    "merged_df = pd.merge(cattle_df, land_df, on='key')\n",
    "merged_df.to_csv('data/merged.csv', index=False) # write csv to data folder\n",
    "merged_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain Code</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Area Code (M49)</th>\n",
       "      <th>Area</th>\n",
       "      <th>Element Code</th>\n",
       "      <th>Element</th>\n",
       "      <th>Item Code (CPC)</th>\n",
       "      <th>Item</th>\n",
       "      <th>Year Code</th>\n",
       "      <th>Year</th>\n",
       "      <th>Months Code</th>\n",
       "      <th>Months</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Value</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Flag Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5530</td>\n",
       "      <td>Producer Price (LCU/tonne)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1991</td>\n",
       "      <td>1991</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>LCU</td>\n",
       "      <td>49338.00</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5530</td>\n",
       "      <td>Producer Price (LCU/tonne)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>LCU</td>\n",
       "      <td>48602.00</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5530</td>\n",
       "      <td>Producer Price (LCU/tonne)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>LCU</td>\n",
       "      <td>46536.00</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5530</td>\n",
       "      <td>Producer Price (LCU/tonne)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1994</td>\n",
       "      <td>1994</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>LCU</td>\n",
       "      <td>46804.00</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>40</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5530</td>\n",
       "      <td>Producer Price (LCU/tonne)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1995</td>\n",
       "      <td>1995</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>LCU</td>\n",
       "      <td>38625.00</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8117</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>752</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>5539</td>\n",
       "      <td>Producer Price Index (2014-2016 = 100)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.11</td>\n",
       "      <td>I</td>\n",
       "      <td>Imputed value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8118</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>752</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>5539</td>\n",
       "      <td>Producer Price Index (2014-2016 = 100)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103.68</td>\n",
       "      <td>I</td>\n",
       "      <td>Imputed value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>752</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>5539</td>\n",
       "      <td>Producer Price Index (2014-2016 = 100)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.33</td>\n",
       "      <td>I</td>\n",
       "      <td>Imputed value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>752</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>5539</td>\n",
       "      <td>Producer Price Index (2014-2016 = 100)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.97</td>\n",
       "      <td>I</td>\n",
       "      <td>Imputed value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>752</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>5539</td>\n",
       "      <td>Producer Price Index (2014-2016 = 100)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.62</td>\n",
       "      <td>I</td>\n",
       "      <td>Imputed value</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2132 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Domain Code           Domain  Area Code (M49)     Area  Element Code  \\\n",
       "519           PP  Producer Prices               40  Austria          5530   \n",
       "520           PP  Producer Prices               40  Austria          5530   \n",
       "521           PP  Producer Prices               40  Austria          5530   \n",
       "522           PP  Producer Prices               40  Austria          5530   \n",
       "523           PP  Producer Prices               40  Austria          5530   \n",
       "...          ...              ...              ...      ...           ...   \n",
       "8117          PP  Producer Prices              752   Sweden          5539   \n",
       "8118          PP  Producer Prices              752   Sweden          5539   \n",
       "8119          PP  Producer Prices              752   Sweden          5539   \n",
       "8120          PP  Producer Prices              752   Sweden          5539   \n",
       "8121          PP  Producer Prices              752   Sweden          5539   \n",
       "\n",
       "                                     Element  Item Code (CPC)  \\\n",
       "519               Producer Price (LCU/tonne)         21111.01   \n",
       "520               Producer Price (LCU/tonne)         21111.01   \n",
       "521               Producer Price (LCU/tonne)         21111.01   \n",
       "522               Producer Price (LCU/tonne)         21111.01   \n",
       "523               Producer Price (LCU/tonne)         21111.01   \n",
       "...                                      ...              ...   \n",
       "8117  Producer Price Index (2014-2016 = 100)         21111.01   \n",
       "8118  Producer Price Index (2014-2016 = 100)         21111.01   \n",
       "8119  Producer Price Index (2014-2016 = 100)         21111.01   \n",
       "8120  Producer Price Index (2014-2016 = 100)         21111.01   \n",
       "8121  Producer Price Index (2014-2016 = 100)         21111.01   \n",
       "\n",
       "                                                Item  Year Code  Year  \\\n",
       "519   Meat of cattle with the bone, fresh or chilled       1991  1991   \n",
       "520   Meat of cattle with the bone, fresh or chilled       1992  1992   \n",
       "521   Meat of cattle with the bone, fresh or chilled       1993  1993   \n",
       "522   Meat of cattle with the bone, fresh or chilled       1994  1994   \n",
       "523   Meat of cattle with the bone, fresh or chilled       1995  1995   \n",
       "...                                              ...        ...   ...   \n",
       "8117  Meat of cattle with the bone, fresh or chilled       2017  2017   \n",
       "8118  Meat of cattle with the bone, fresh or chilled       2018  2018   \n",
       "8119  Meat of cattle with the bone, fresh or chilled       2019  2019   \n",
       "8120  Meat of cattle with the bone, fresh or chilled       2020  2020   \n",
       "8121  Meat of cattle with the bone, fresh or chilled       2021  2021   \n",
       "\n",
       "      Months Code        Months Unit     Value Flag Flag Description  \n",
       "519          7021  Annual value  LCU  49338.00    A  Official figure  \n",
       "520          7021  Annual value  LCU  48602.00    A  Official figure  \n",
       "521          7021  Annual value  LCU  46536.00    A  Official figure  \n",
       "522          7021  Annual value  LCU  46804.00    A  Official figure  \n",
       "523          7021  Annual value  LCU  38625.00    A  Official figure  \n",
       "...           ...           ...  ...       ...  ...              ...  \n",
       "8117         7021  Annual value  NaN    111.11    I    Imputed value  \n",
       "8118         7021  Annual value  NaN    103.68    I    Imputed value  \n",
       "8119         7021  Annual value  NaN    102.33    I    Imputed value  \n",
       "8120         7021  Annual value  NaN    100.97    I    Imputed value  \n",
       "8121         7021  Annual value  NaN     99.62    I    Imputed value  \n",
       "\n",
       "[2132 rows x 16 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_df = pd.read_csv('rawdata/price.csv')\n",
    "price_df= price_df[(price_df['Area'].isin(EUROPEAN_UNION.names))] #Reduce to European countries\n",
    "price_df\n",
    "\n",
    "# price_df['key'] = price_df['Area'].astype(str) + price_df['Year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Area'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Area'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m rain_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/rain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#Testing # temp_df.head(6) # rain_df.head(6)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# EU countries\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m countries \u001b[38;5;241m=\u001b[39m \u001b[43mcattle_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mArea\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     19\u001b[0m price_df\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Area'"
     ]
    }
   ],
   "source": [
    "export_df = pd.read_csv('rawdata/export.csv')\n",
    "manure_df = pd.read_csv('rawdata/manure.csv')\n",
    "fertiliser_df = pd.read_csv('rawdata/fertilizersnutrient.csv')\n",
    "# sample testing # fertiliser_df.sample(4)# lands_df.sample(4)# price_df.sample(4)# export_df.sample(4)# manure_df.sample(4)\n",
    "#Concatenate Rain Climate Change Knowledge Portal data\n",
    "rain_df = files_to_df('rawdata/Eu_rain_data', 'Rainfall_mm/yr')\n",
    "#Concatenate Temperature data\n",
    "temp_df = files_to_df('rawdata/Eu_temp_data', 'Temperature_C')\n",
    "#Write both to csv files\n",
    "temp_df.to_csv('data/temp.csv', index=False)\n",
    "rain_df.to_csv('data/rain.csv', index=False)\n",
    "#Testing # temp_df.head(6) # rain_df.head(6)\n",
    "# EU countries\n",
    "countries = cattle_df['Area'].unique().tolist()\n",
    "price_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An initial look and preparation of the dataframes and use of the pd.merge function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)  Land under temporary and permanent meadows and pastures separated as independant predictor variables and reduntant fields are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain Code</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Area Code (M49)</th>\n",
       "      <th>Area</th>\n",
       "      <th>Element Code</th>\n",
       "      <th>Element</th>\n",
       "      <th>Item Code (CPC)</th>\n",
       "      <th>Item</th>\n",
       "      <th>Year Code</th>\n",
       "      <th>Year</th>\n",
       "      <th>Months Code</th>\n",
       "      <th>Months</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Value</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Flag Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>4</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>5539</td>\n",
       "      <td>Producer Price Index (2014-2016 = 100)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1991</td>\n",
       "      <td>1991</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.34</td>\n",
       "      <td>I</td>\n",
       "      <td>Imputed value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>4</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>5539</td>\n",
       "      <td>Producer Price Index (2014-2016 = 100)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.34</td>\n",
       "      <td>I</td>\n",
       "      <td>Imputed value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>4</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>5539</td>\n",
       "      <td>Producer Price Index (2014-2016 = 100)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.34</td>\n",
       "      <td>I</td>\n",
       "      <td>Imputed value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>4</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>5539</td>\n",
       "      <td>Producer Price Index (2014-2016 = 100)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1994</td>\n",
       "      <td>1994</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.46</td>\n",
       "      <td>I</td>\n",
       "      <td>Imputed value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>4</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>5539</td>\n",
       "      <td>Producer Price Index (2014-2016 = 100)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1995</td>\n",
       "      <td>1995</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.23</td>\n",
       "      <td>I</td>\n",
       "      <td>Imputed value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9142</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>716</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>5530</td>\n",
       "      <td>Producer Price (LCU/tonne)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>LCU</td>\n",
       "      <td>3690.00</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9143</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>716</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>5530</td>\n",
       "      <td>Producer Price (LCU/tonne)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>LCU</td>\n",
       "      <td>4610.00</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9144</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>716</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>5531</td>\n",
       "      <td>Producer Price (SLC/tonne)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1991</td>\n",
       "      <td>1991</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>SLC</td>\n",
       "      <td>10.30</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9145</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>716</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>5531</td>\n",
       "      <td>Producer Price (SLC/tonne)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>SLC</td>\n",
       "      <td>10.20</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9146</th>\n",
       "      <td>PP</td>\n",
       "      <td>Producer Prices</td>\n",
       "      <td>716</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>5531</td>\n",
       "      <td>Producer Price (SLC/tonne)</td>\n",
       "      <td>21111.01</td>\n",
       "      <td>Meat of cattle with the bone, fresh or chilled</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993</td>\n",
       "      <td>7021</td>\n",
       "      <td>Annual value</td>\n",
       "      <td>SLC</td>\n",
       "      <td>12.70</td>\n",
       "      <td>A</td>\n",
       "      <td>Official figure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9147 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Domain Code           Domain  Area Code (M49)         Area  Element Code  \\\n",
       "0             PP  Producer Prices                4  Afghanistan          5539   \n",
       "1             PP  Producer Prices                4  Afghanistan          5539   \n",
       "2             PP  Producer Prices                4  Afghanistan          5539   \n",
       "3             PP  Producer Prices                4  Afghanistan          5539   \n",
       "4             PP  Producer Prices                4  Afghanistan          5539   \n",
       "...          ...              ...              ...          ...           ...   \n",
       "9142          PP  Producer Prices              716     Zimbabwe          5530   \n",
       "9143          PP  Producer Prices              716     Zimbabwe          5530   \n",
       "9144          PP  Producer Prices              716     Zimbabwe          5531   \n",
       "9145          PP  Producer Prices              716     Zimbabwe          5531   \n",
       "9146          PP  Producer Prices              716     Zimbabwe          5531   \n",
       "\n",
       "                                     Element  Item Code (CPC)  \\\n",
       "0     Producer Price Index (2014-2016 = 100)         21111.01   \n",
       "1     Producer Price Index (2014-2016 = 100)         21111.01   \n",
       "2     Producer Price Index (2014-2016 = 100)         21111.01   \n",
       "3     Producer Price Index (2014-2016 = 100)         21111.01   \n",
       "4     Producer Price Index (2014-2016 = 100)         21111.01   \n",
       "...                                      ...              ...   \n",
       "9142              Producer Price (LCU/tonne)         21111.01   \n",
       "9143              Producer Price (LCU/tonne)         21111.01   \n",
       "9144              Producer Price (SLC/tonne)         21111.01   \n",
       "9145              Producer Price (SLC/tonne)         21111.01   \n",
       "9146              Producer Price (SLC/tonne)         21111.01   \n",
       "\n",
       "                                                Item  Year Code  Year  \\\n",
       "0     Meat of cattle with the bone, fresh or chilled       1991  1991   \n",
       "1     Meat of cattle with the bone, fresh or chilled       1992  1992   \n",
       "2     Meat of cattle with the bone, fresh or chilled       1993  1993   \n",
       "3     Meat of cattle with the bone, fresh or chilled       1994  1994   \n",
       "4     Meat of cattle with the bone, fresh or chilled       1995  1995   \n",
       "...                                              ...        ...   ...   \n",
       "9142  Meat of cattle with the bone, fresh or chilled       1992  1992   \n",
       "9143  Meat of cattle with the bone, fresh or chilled       1993  1993   \n",
       "9144  Meat of cattle with the bone, fresh or chilled       1991  1991   \n",
       "9145  Meat of cattle with the bone, fresh or chilled       1992  1992   \n",
       "9146  Meat of cattle with the bone, fresh or chilled       1993  1993   \n",
       "\n",
       "      Months Code        Months Unit    Value Flag Flag Description  \n",
       "0            7021  Annual value  NaN    17.34    I    Imputed value  \n",
       "1            7021  Annual value  NaN    17.34    I    Imputed value  \n",
       "2            7021  Annual value  NaN    17.34    I    Imputed value  \n",
       "3            7021  Annual value  NaN    18.46    I    Imputed value  \n",
       "4            7021  Annual value  NaN    18.23    I    Imputed value  \n",
       "...           ...           ...  ...      ...  ...              ...  \n",
       "9142         7021  Annual value  LCU  3690.00    A  Official figure  \n",
       "9143         7021  Annual value  LCU  4610.00    A  Official figure  \n",
       "9144         7021  Annual value  SLC    10.30    A  Official figure  \n",
       "9145         7021  Annual value  SLC    10.20    A  Official figure  \n",
       "9146         7021  Annual value  SLC    12.70    A  Official figure  \n",
       "\n",
       "[9147 rows x 16 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to filter the dataframe by countries, making sure that we only have eu member states\n",
    "price_df = price_df[price_df['Area'].isin(countries)]\n",
    "#filter the Months column to show only annual value\n",
    "price_df = price_df[price_df['Months'] == 'Annual value']\n",
    "#make a key column to merge the data\n",
    "price_df['key'] = price_df['Area'] + price_df['Year'].astype(str)\n",
    "#merge the data onto the milk_euy dataframe\n",
    "milk_eu = pd.merge(milk_eu, price_df[['key', 'Value']], on='key', how='left')\n",
    "#since prices are only recorded after 1990 we will filter the years\n",
    "milk_eu = milk_eu[milk_eu['Year'] > 1990]\n",
    "milk_eu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) almost there for the dataset construction. We will now look at the quantity of milk exported from the countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(milk_eu.shape)\n",
    "#reformat the dataframe using our lovely function\n",
    "export_info_df = reformat(export_df, 'Element')\n",
    "#and simply merge this onto ourt milk_eu dataframe\n",
    "milk_eu = pd.merge(milk_eu, export_info_df[['key', 'Export Value', 'Export Quantity']], on='key', how = 'left')\n",
    "milk_eu[(milk_eu['Area'] == 'Spain') & (milk_eu['Year'] == 2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Append the temperature and rain data to the milk_eu dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "milk_eu = pd.merge(milk_eu, temp_df, on='key', how='left')\n",
    "milk_eu = pd.merge(milk_eu, rain_df, on='key', how='left')\n",
    "milk_eu[(milk_eu['Area'] == 'Italy') & (milk_eu['Year'] == 2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Last part of the dataset construction, lets get this shit sorted... were looking at manure and fertiliser used for pasture land."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manure_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the dataframe for only eu memberstate countries\n",
    "manure_df = manure_df[manure_df['Area'].isin(countries)]\n",
    "#create a key column to merge the data\n",
    "manure_df['key'] = manure_df['Area'] + manure_df['Year'].astype(str)\n",
    "#rename the value column to manure_kg\n",
    "manure_df.rename(columns={'Value': 'Manure_kg'}, inplace=True)\n",
    "#merge the data onto the milk_eu dataframe\n",
    "milk_eu = pd.merge(milk_eu, manure_df[['key', 'Manure_kg']], on='key', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fert_df = reformat(fertilizer_df, 'Item')\n",
    "milk_eu = pd.merge(milk_eu, fert_df, on=['key', 'Area'], how = 'left')\n",
    "milk_eu[(milk_eu['Area'] == 'Italy') & (milk_eu['Year'] == 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu[milk_eu['Area'] == 'Spain'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(milk_eu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theres a few things we will need to look at before moving forward here\n",
    "* Missing values in the meadows and pastures data\n",
    "* Misssing values in the Exports Data\n",
    "* Missing values in the fertilizer data\n",
    "\n",
    "I will address these in the following section & determine the best approach on a case by case basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Data cleaning & feature engineering stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets kick things off by building a function that will loop over many imputation techniques and will compare their results, spitting out the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping redundant columns\n",
    "milk_eu.drop(['Domain Code', 'Domain', 'Area Code (M49)',\n",
    " 'Element Code', 'Element', 'Item Code (CPC)', 'Item', 'Year Code', 'Flag', 'key', 'Flag Description'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wilcoxon test\n",
    "from scipy.stats import wilcoxon\n",
    "def impute_data(milk_eu, columns, k, iter, target):\n",
    "    #defining the imputation methods\n",
    "    imputation_type = ['.mean()', '.mode()', '.median()','bfill', 'ffill', 'knn', 'mice']\n",
    "    #dropping null columns and making nan values are np.nan format\n",
    "    milk_eu_imp = milk_eu.fillna(np.nan).drop(columns, axis=1).select_dtypes(exclude=['object'])\n",
    "\n",
    "    results = {}#creating a dictionary to store the results\n",
    "    best = {}#creating a dictionary to store the best results\n",
    "\n",
    "    #defining the mannwhitneyu test\n",
    "    def score_dataset(milk_eu, impute):\n",
    "        #take a random sample of n = 500 from the dataset\n",
    "        sample_milk = milk_eu.dropna().sample(n=700, random_state=0)\n",
    "        sample_imp = impute.dropna().sample(n=700, random_state=0)\n",
    "        u, p = wilcoxon(sample_milk, sample_imp)\n",
    "\n",
    "        return p #returning the p value\n",
    "\n",
    "    #looping through the columns to be imputed\n",
    "    for col in columns:\n",
    "\n",
    "        #defining the imputation methods in a dictionary to allow iteration\n",
    "        #this is done to avoid having to write out each method individually\n",
    "        functions = {\n",
    "        'mean': milk_eu[col].mean(),\n",
    "        'mode': milk_eu[col].mode(),\n",
    "        'median': milk_eu[col].median(),\n",
    "        'bfill': milk_eu[col].fillna(method='bfill'),\n",
    "        'ffill': milk_eu[col].fillna(method='ffill')\n",
    "    }\n",
    "\n",
    "        #looping through the imputation methods\n",
    "        for type in imputation_type:\n",
    "\n",
    "            if type in functions:#if the imputation method is in the dictionary\n",
    "                impute = functions[type]\n",
    "                results[f'{type}_{col}'] = score_dataset(milk_eu[col], impute)#adding the results to the results dictionary\n",
    "\n",
    "            elif type == 'knn':#if the imputation method is knn\n",
    "                for i in k:#looping through the k values\n",
    "                    milk_eu_imp[col] = milk_eu[col].fillna(np.nan)\n",
    "                    impute = pd.DataFrame(fancyimpute.KNN(k=i, verbose=False).fit_transform(milk_eu_imp))\n",
    "                    impute.columns = milk_eu_imp.columns #setting the column names to the original column names\n",
    "                    results[f'{type}_{i}_{col}'] = score_dataset(milk_eu[col], impute[col])#adding the results to the results dictionary\n",
    "\n",
    "            elif type == 'mice':#if the imputation method is mice\n",
    "                for iterations in iter:#looping through the iterations\n",
    "                    milk_eu_imp[col] = milk_eu[col].fillna(np.nan)\n",
    "                    imputed = pd.DataFrame(fancyimpute.IterativeImputer(max_iter=iterations).fit_transform(milk_eu_imp))\n",
    "                    impute.columns = milk_eu_imp.columns #setting the column names to the original column names\n",
    "                    results[f'{type}_{iterations}_{col}'] = score_dataset(milk_eu[col], impute[col]) #adding the results to the results dictionary \n",
    "\n",
    "        #put max value in 'best' dict, this returrns the best imputation method\n",
    "        best[col] = max(results, key=results.get)\n",
    "        \n",
    "    return results, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) First we will deal with the missing values using the method constructed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the function inputs\n",
    "\n",
    "columns = [i for i in milk_eu.columns if milk_eu[i].isnull().any()] #columns containing null values\n",
    "k = [1, 3, 5, 7, 9, 11] #number of neighbours for knn imputation\n",
    "iter = [20, 40, 60, 80, 100, 120] #number of iterations for mice imputation\n",
    "\n",
    "#calling the function\n",
    "results_imp, best_imp = impute_data(milk_eu, columns, k, iter, 'Tonne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though the best imputation method for most of our columns is the forward filling method. This is great for us as it really speeeds up imputing the data, we will do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can see from the results above that knn imputation will work very well for all of the columns so we will use it now\n",
    "columns = [i for i in milk_eu.columns if milk_eu[i].isnull().any()] #columns containing null values\n",
    "#imputing the data using ffll\n",
    "milk_eu[columns] = milk_eu[columns].fillna(method='ffill')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot all the numerical columns \n",
    "milk_eu.hist(figsize=(20,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that this is done we can move along to some more exciting tasks... Feature engineering & EDA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Feature Engineering - Calculated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu_eda = milk_eu.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first port of call will be to prepare the dataset for plotting a choropleth map. This means that we need the 3 letter code that corresponds to each country in the dataset. Conveniently the pycountry library makes this very easy for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting country name to country code, this will be needed for the choropleth map\n",
    "import pycountry\n",
    "def country_name_to_country_code(country_name):\n",
    "    country_code = pycountry.countries.search_fuzzy(country_name)[0].alpha_3\n",
    "    return country_code\n",
    "\n",
    "#now applying this to the dataframe\n",
    "milk_eu_eda['Country Code'] = milk_eu_eda['Area'].apply(country_name_to_country_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our investigations it may be beneficial to know the amount of milk exported in terms of volume, specifically litres. To convert tonnes to litres, we simply look back to our beginner physics  class and remember the following formula,\n",
    "\n",
    "$$ V = \\frac{m}{\\rho{}} $$\n",
    " \n",
    "where V is volume, m is mass. and  $\\rho{}$  is the density. The density for raw milk will be 1.027kg/m3 on average at a temperature of 20c.\n",
    "\n",
    "nice to finally be making good use of that physics degree..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert tonnes to litres\n",
    "def volume(x):\n",
    "    return x*1000/1.027\n",
    "\n",
    "milk_eu_eda['volume(litre)'] = milk_eu_eda['Tonne'].apply(volume)#Icould probably have used a lambda function here, but i might use the volume function later so no harm having close to hand\n",
    "milk_eu_eda['volume/cow(litre)'] = milk_eu_eda['volume(litre)']/milk_eu['Head']\n",
    "milk_eu_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We eould now like to know more about the total pasture land taken up by farming in each EU country. We will simply find the sum of perminant and temporary pasture land to find the total pasture land area in each country. Following this we will find the dutface area of each country, I could just google all the surface area values, but honestly who has the time. Thankfully some group of beautiful nerds made a python library that just so happens to contain this data for our convenience. Using all of this we will finally calculate the percentage of land area in each country that is used for pasture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the total pasture by adding temporary and permanent pasture\n",
    "milk_eu_eda['Total Pasture(ha)'] = milk_eu_eda['Land under perm. meadows and pastures'] + milk_eu['Land under temp. meadows and pastures']\n",
    "#converting the data to hectares instead of 1000s of hectares\n",
    "milk_eu_eda['Total Pasture(ha)'] = milk_eu_eda['Total Pasture(ha)'].apply(lambda x: x*1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding surface area of each country(thanks wikipedia) in the dataset in hectares, and make a dataframe\n",
    "Surface_area_countries = [] #creating an empty list to store the surface area\n",
    "for i in countries:#looping through the countries in the list\n",
    "   if i == 'Czechia': #changing the name of the country to match the name in the countryinfo library\n",
    "         i = 'Czech Republic'\n",
    "   country = CountryInfo(i)#getting the country info\n",
    "   area = country.area()#getting the area\n",
    "   Surface_area_countries.append(area*100)#converting to hectares from km^2 and appending to the list\n",
    "\n",
    "#making a dataframe of the surface area\n",
    "area_df = pd.DataFrame({'Area': countries, 'Surface Area': Surface_area_countries})\n",
    "#we are going to merge the area dataframe on countries\n",
    "milk_eu_eda = pd.merge(milk_eu_eda, area_df, on='Area', how='left')\n",
    "\n",
    "#calculating the percentage of pasture to total area\n",
    "milk_eu_eda['% Pasture'] = milk_eu_eda['Total Pasture(ha)']/milk_eu_eda['Surface Area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the milk_eu dataframe we have the amount of money, value, paid to producers per tonne in usd. We will use this to calculate the total revenue earned by producers in each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the revenue from milk\n",
    "milk_eu_eda['Revenue(usd)'] = milk_eu_eda['Tonne']*milk_eu_eda['Value']\n",
    "\n",
    "#calculating the revenue per cow\n",
    "milk_eu_eda['Revenue per cow(usd)'] = milk_eu_eda['Revenue(usd)']/milk_eu_eda['Head']\n",
    "\n",
    "#calculating the revenue per hectare\n",
    "milk_eu_eda['Revenue per hectare(usd)'] = milk_eu_eda['Revenue(usd)']/milk_eu_eda['Total Pasture(ha)']\n",
    "\n",
    "#calculating the price per litre\n",
    "milk_eu_eda['Price per litre(usd)'] = milk_eu_eda['Value']/(1000/1.027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu_eda.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we would like to have columns in the dataset which show the export volume & export value per litre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the volume of milk exported per country\n",
    "milk_eu_eda['Exported volume'] = milk_eu_eda['Export Quantity']*1000/1.027\n",
    "#we want to add an export usd per litre column\n",
    "milk_eu_eda['Export Value'] = milk_eu_eda['Export Value']*1000\n",
    "milk_eu_eda['Export Value per litre(USD)'] = milk_eu_eda['Export Value']/milk_eu_eda['Exported volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu_eda[milk_eu_eda['Exported volume'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these 0 values in the exportted volume/quantity are causing nan values in the export value per litre column. These nan values will be replaced with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu_eda['Export Value per litre(USD)'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop malta from the dataset as it has no land use data\n",
    "milk_eu_eda = milk_eu_eda[milk_eu_eda['Area'] != 'Malta']\n",
    "#same with milk_eu\n",
    "milk_eu = milk_eu[milk_eu['Area'] != 'Malta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sound, thats the majority of the calculated columns that we want. Lets plot some of this data & see what obvious relationships are present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to look at is the geographical distribution of herd size and milk production. This will be done by creating a function that plots a choropleth map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to plot the choropleth map which takes the measure and title as inputs\n",
    "def plot_map(measure, title):\n",
    "    import plotly.express as px\n",
    "    fig = px.choropleth(milk_eu_eda, locations=\"Country Code\", color=measure,\n",
    "                    hover_name=\"Area\", animation_frame=\"Year\",\n",
    "                    color_continuous_scale=px.colors.sequential.YlGnBu)\n",
    "\n",
    "    fig.update_layout(title_text= title, geo_scope='europe', width = 1000, height = 600, )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the choropleth map for milk production distribution in litres\n",
    "plot_map('volume(litre)', 'Milk Production Distribution in Litres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the choropleth map for herd size distribution\n",
    "plot_map('Head', 'Herd Size Distribution')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same thing, but plotting the herd size and milk production for 2020 on a barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this barplot is produced to show the prefoprmanxce of the top 8 milk producing countries in the EU in 2020.\n",
    "#Ireland is there wohoo!\n",
    "def plot_barplot(measure, title):\n",
    "    sns.barplot(x='Area', y=measure, data=milk_eu_eda[(milk_eu_eda['Year'] == 2020 )],\n",
    "    palette = color, order=milk_eu_eda[(milk_eu_eda['Year'] == 2020 )].sort_values(measure, ascending = False).Area\n",
    "    ) \n",
    "    plt.title(title, fontsize=25)\n",
    "    plt.xlabel('Country', fontsize=20)\n",
    "    plt.ylabel(measure, fontsize=20)\n",
    "    plt.xticks(rotation = 45, fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the milk production per country in 2020\n",
    "plot_barplot('volume(litre)', 'Milk Production per Country in 2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#germany milk production as a proportion of france in 2020\n",
    "milk_eu_eda[(milk_eu_eda['Year'] == 2020) & (milk_eu_eda['Area'] == 'Germany')]['volume(litre)'].values[0]/milk_eu_eda[(milk_eu_eda['Year'] == 2020) & (milk_eu_eda['Area'] == 'France')]['volume(litre)'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and plotting the head size per country in 2020\n",
    "plot_barplot('Head', 'Herd Size per Country in 2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can see that the top 6 milk producing countries are the same as the top 6 herd size countries, \n",
    "#lets plot these against eachother to see if there is a correlation\n",
    "sns.regplot(x='Head', y='volume(litre)', data=milk_eu_eda[(milk_eu_eda['Year'] == 2020 )], ci = None)\n",
    "plt.title('Herd Size vs Milk Production in 2020', fontsize=25)\n",
    "plt.xlabel('Herd Size', fontsize=20)\n",
    "plt.ylabel('Total milk production', fontsize=20)\n",
    "#correlation between volume and head\n",
    "milk_eu_eda[(milk_eu_eda['Year'] == 2020 )]['volume(litre)'].corr(milk_eu_eda[(milk_eu_eda['Year'] == 2020 )]['Head'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like these values are pretty closely correlated. We will see the correlation coefficient soon. First im interested in seeing how the milk production and herd size changed over time. To prevent extremely messy & hard to read plots we will only consider the countries that the top 6 herd sizes and milk production in 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = milk_eu_eda[(milk_eu_eda['Year'] == 2020)].nlargest(6, 'Head')['Area']\n",
    "years = milk_eu_eda['Year'].unique().tolist()\n",
    "def trend_plot(df, measure, title, y_label):\n",
    "    #defining a dictionary to store the data\n",
    "    milk = {}\n",
    "    #looping through the countries and years to get the data\n",
    "    cmap = cm.tab20c(np.linspace(0, 1, len(countries)))\n",
    "    for i, h in zip(countries, cmap):\n",
    "        milk_volume = [] #creating a list to store the data\n",
    "        year = []\n",
    "        for j in years: \n",
    "            value = df[(df['Area'] == i) & (df['Year'] == j)][measure]  #getting the value\n",
    "            if len(value) ==1:#checking if the value is present and length is 1\n",
    "                milk_volume.append(value)\n",
    "                year.append(j)\n",
    "\n",
    "            else:\n",
    "                pass #if not present, print error\n",
    "        milk[i] = milk_volume #adding the data to the dictionary\n",
    "        plt.plot(year, milk_volume, '--o', label=i, color = h)#plotting the data\n",
    "\n",
    "    #styling the plot\n",
    "    plt.legend(fontsize = 15)\n",
    "    plt.title(title, fontsize=25)\n",
    "    plt.xlabel('Year', fontsize=20)\n",
    "    plt.ylabel(y_label, fontsize=20)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the change in head size over the years for each country\n",
    "trend_plot(milk_eu_eda, 'Head', 'Change in Head Size over the Years', 'Head Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and do the same for milk production\n",
    "trend_plot(milk_eu_eda, 'volume(litre)', 'Change in Milk Production over the Years', 'Milk Production')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats interesting... while the milk production has remained constant over the years, or even increased... the herd size has decreased for many of the countries... Ireland is an exception though. Lets plot the change in milk per cow over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the volume per cow over the years for each country\n",
    "trend_plot(milk_eu_eda, 'volume/cow(litre)', 'Change in Milk Production per Cow over the Years', 'Milk Production per Cow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats a bit mad, it appears that cows are producing more milk on average in 2020 than they were back in 1991... This really goes to show the efficacy of genetic breeding in agriculture. Out of interest lets find on average the multiple of milk production these days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = countries.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu_eda[(milk_eu_eda['Year'] == 1991)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this loop will calculate the ratio of milk production in 1991 to 2020 for each country\n",
    "#and store it in a dictionary\n",
    "milk_ratio = {}\n",
    "for i in countries:\n",
    "    ratio = milk_eu_eda[(milk_eu_eda['Area'] == i) & (milk_eu_eda['Year'] == 2020)]['volume/cow(litre)'].values[0]/milk_eu_eda[(milk_eu_eda['Area'] == i) & (milk_eu_eda['Year'] == 1991)]['volume/cow(litre)'].values[0]\n",
    "    milk_ratio[i] = ratio\n",
    "#plotting the ratio of milk production in 1991 to 2020 for each country in seaborn barplot\n",
    "sns.barplot(x=list(milk_ratio.keys()), y=list(milk_ratio.values()), palette = color)\n",
    "plt.title('Increase in cow productivity for each of the top 6 milk producing countries', fontsize=25)\n",
    "#print the average increase in milk production ,ultiple\n",
    "print('The average increase in milk production per cow is', round(np.mean(list(milk_ratio.values())), 2))\n",
    "#print the max increase in milk production and country\n",
    "print('The max increase in milk production per cow is', round(max(milk_ratio.values()), 2), 'in', max(milk_ratio, key=milk_ratio.get))\n",
    "#print the min increase in milk production and country\n",
    "print('The min increase in milk production per cow is', round(min(milk_ratio.values()), 2), 'in', min(milk_ratio, key=milk_ratio.get))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the exports from each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the export quantity over the years for each country\n",
    "trend_plot(milk_eu_eda, 'Exported volume', 'Change in Export volume over the Years', 'Export Quantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the export volume for each of these countries over the years\n",
    "for i in countries:\n",
    "    print(i, milk_eu_eda[(milk_eu_eda['Area'] == i) & (milk_eu_eda['Year'] == 2020)]['Exported volume'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu_eda.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the amounts of fertilizer used on each country per yar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amount of fertilizer used per country in 2020\n",
    "nutrients = ['Nutrient nitrogen N (total)', 'Nutrient phosphate P2O5 (total)', 'Nutrient potash K2O (total)']\n",
    "#create correlation heatmap between these and milk production\n",
    "corr = milk_eu_eda[(milk_eu_eda['Year'] == 2020)][nutrients + ['volume(litre)']].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', annot_kws={\"size\":25})\n",
    "#fontsize of the labels\n",
    "plt.title('Correlation between Nutrients and Milk Production', fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=25, rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trendplot for nitrogen fertilizer\n",
    "trend_plot(milk_eu_eda, 'Nutrient nitrogen N (total)', 'Change in Nitrogen Fertilizer over the Years', 'Nitrogen Fertilizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trendplot for phosphate fertilizer\n",
    "trend_plot(milk_eu_eda, 'Nutrient phosphate P2O5 (total)', 'Change in Phosphate Fertilizer over the Years', 'Phosphate Fertilizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trendplot for potash fertilizer\n",
    "trend_plot(milk_eu_eda, 'Nutrient potash K2O (total)', 'Change in Potash Fertilizer over the Years', 'Potash Fertilizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trendplot for kg of manure per country per year\n",
    "trend_plot(milk_eu_eda, 'Manure_kg', 'Change in Manure over the Years', 'Manure_kg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot % pasture per country per year\n",
    "trend_plot(milk_eu_eda, '% Pasture', 'Change in % pasture over the Years', '% Pasture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot value of milk per country per year\n",
    "trend_plot(milk_eu_eda, 'Value', 'Change in Value of Milk over the Years', 'Value of Milk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the last 30 years a cows milk production has increased 1.67x on average, with a peak of 2.2x in poland & a minimum of 1.47x in ireland... It seems as though we need to rsmp up the genetic breeding, but for now lets just pretend out milk is better quality ha ha ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets lok at the correlation between features using a seaborn heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between columns and plotting as heatmap\n",
    "sns.heatmap(milk_eu.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation between attributes in our dataframe', fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#to allow for analysis we need to encode the Area column we will one hot encode it so the model doesnt mistake it for some sort of order and derive some correlation\n",
    "milk_eu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export this dataset for use in our dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu_eda.to_csv('milk_eu_eda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Machine learning applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Before we can go ahead with the machine learning we need to make a decision regarding our target variable. This will dictate the attributes that we keep for the features dataframe and the attribute that we set as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the Area column\n",
    "milk_eu.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target will be the price paid to the producers. This means that we will need to get rid of any attributes calculated from this column,\n",
    "we will be keeping,\n",
    "\n",
    "* year\n",
    "* Head\n",
    "* volume(litre)\n",
    "* Land under temp pasture\n",
    "* Land under perm pasture\n",
    "* Temp_c\n",
    "* Rain_mm/yr\n",
    "* manure\n",
    "* nitrogen\n",
    "* potash\n",
    "* phosphate\n",
    "* % pasture\n",
    "* price per litre ---> target\n",
    "* exported volume\n",
    "* exported value per litre usd\n",
    "* keep all areas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection\n",
    "Y = milk_eu['Tonne']\n",
    "\n",
    "#drop the columns we dont need\n",
    "X = milk_eu.drop(['Tonne'], axis=1)\n",
    "#Removing malta due to lack of export data\n",
    "X = X[X['Area'] != 'Malta']\n",
    "#correlation between columns and plotting as heatmap\n",
    "sns.heatmap(X.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation between attributes in our dataframe', fontsize=25)\n",
    "\n",
    "#one hot encode the Area column\n",
    "X = pd.get_dummies(X, columns=['Area'])\n",
    "\n",
    "X.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) There is quite a bit of correlation among our attributes in our feature array. This could lead to issues when we train our models, so to get around this we will use principal component analys to reduce the dimension of our data & remove any highly correlated columns. To do this we will first scale our data & then determine the optimal number of components using a 95% explained variance threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.35, random_state=42)\n",
    "\n",
    "\n",
    "#scaling the data#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#pca\n",
    "from sklearn.decomposition import PCA\n",
    "explained_variance_ratios = []\n",
    "# Loop through a range of number of components\n",
    "for i in range(1, X.shape[1]+1):\n",
    "    # Initialize PCA\n",
    "    pca = PCA(n_components=i)\n",
    "\n",
    "    # Fit the model to the training data and transform the data\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "    # Transform the test data\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # Calculate the explained variance ratio and sum\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_.sum()\n",
    "\n",
    "    # Append the explained variance ratio to the list\n",
    "    explained_variance_ratios.append(explained_variance_ratio)\n",
    "\n",
    "#plot this\n",
    "plt.plot(range(1, X.shape[1]+1), explained_variance_ratios)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio vs Number of Components')\n",
    "#set x ticks to be the number of components\n",
    "plt.xticks(range(1, X.shape[1]+1))\n",
    "#hline at 0.95\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.show()\n",
    "\n",
    "#find the coordinates where these lines intersect\n",
    "x = np.arange(1, X.shape[1]+1)\n",
    "y = np.array(explained_variance_ratios)\n",
    "#find the index of the first value that is greater than 0.95\n",
    "index = np.argmax(y >= 0.95)\n",
    "#find the x value at that index\n",
    "print('The number of components needed to explain 95% of the variance is', x[index])\n",
    "\n",
    "pca = PCA(n_components=x[index])\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Now that we have our train and test data sorted we will move onto fitting models. We will fit 8 linear models and rank them based off the median absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a quick note about the following code. I will not be making this loop into a function, even though similar methods will be used later on in the notebook. This is because i would rather have the parameter dictionaries clearly visible in my code so that i can see the parameter distributions. The alternative to this would be looping over a dictionary of parameter dictionaries, but i feel this would have a negative impact on the code from a redability pov. And when we consider the fact that we would need to define these parameters for each problem anyway, i think the tradeoff of some slight code repetition is a fair tradeoff. Also the next time similar code is used we will be fitting classification models, so the evaluation metrics will need to be changed completly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_params = {}\n",
    "results = {}\n",
    "mape_score = {}\n",
    "#dictionary of models\n",
    "models = { 'Linear Regression': LinearRegression, 'Ridge Regression': Ridge, 'Lasso Regression': Lasso,\n",
    "            'KNN Regression': KNeighborsRegressor, 'Random Forest Regression': RandomForestRegressor, \n",
    "            'Decision Tree Regression': DecisionTreeRegressor, 'Gradient Boosting Regression': GradientBoostingRegressor,\n",
    "            'Linear SVR': LinearSVR}\n",
    "np.random.seed(42)\n",
    "for model in models.values():\n",
    "    if model == LinearRegression:\n",
    "        param = {}\n",
    "    elif model == Ridge:\n",
    "        param = {'alpha': np.linspace(0.001, 1, 100), 'random_state': [42], \"solver\": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'], 'max_iter': [10000] }\n",
    "    elif model == RandomForestRegressor:\n",
    "        param = {'n_estimators': np.arange(100, 800, 10), 'random_state': [42] , 'criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'], 'max_features':['sqrt', 'log2', None], 'max_depth': np.arange(0, 25, 1) }\n",
    "    elif model == DecisionTreeRegressor:\n",
    "        param = {'max_depth': np.arange(0, 50, 1),'ccp_alpha': np.linspace(0, 2, 100), 'criterion': ['squared_error', 'friedman_mse' 'absolute_error', 'poisson'], 'random_state': [42]}\n",
    "    elif model == LinearSVR:\n",
    "        param = {'C': np.linspace(0, 5, 100), 'random_state': [42], 'epsilon': np.linspace(0, 5, 1000), 'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], 'max_iter': [10000]}\n",
    "    elif model == Lasso:\n",
    "        param = {'alpha': np.linspace(400, 800, 100), 'random_state': [42], 'max_iter': [10000]}\n",
    "    elif model == GradientBoostingRegressor:\n",
    "        param = {'n_estimators': np.arange(0, 100, 10) , 'max_depth': np.arange(5, 10, 1), 'random_state': [42], 'criterion': ['friedman_mse', 'squared_error'], 'max_features': ['sqrt', 'log2', 'auto']}\n",
    "    elif model == KNeighborsRegressor:\n",
    "        param = {'n_neighbors': np.arange(1, 15, 1), 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "    \n",
    "    \n",
    "    grid = RandomizedSearchCV(estimator = model(), param_distributions = param, cv = 10, n_jobs = -1, n_iter = 500, random_state = 42)\n",
    "    grid.fit(X_train, y_train)\n",
    "    #predict\n",
    "    grid_predictions = grid.predict(X_test)\n",
    "\n",
    "    #best parameters\n",
    "    best_params[model] = grid.best_estimator_\n",
    "    print(grid.best_estimator_)\n",
    "\n",
    "    #median absolute percentage error\n",
    "    ape_list = np.abs((grid_predictions-y_test)/y_test)*100\n",
    "   \n",
    "\n",
    "    mape_score[model] = np.median(ape_list)\n",
    "\n",
    "    #r2 score\n",
    "    r2 = r2_score(y_test, grid_predictions)\n",
    "    results[model] = r2\n",
    "\n",
    "    #plotting the results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(y_train, grid.predict(X_train), color = 'green', label = 'Training predictions')\n",
    "    plt.scatter(y_test, grid_predictions, label = 'Test predictions')\n",
    "    plt.plot(Y, Y, color = 'red', label = 'Perfect predictions')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title('True Values vs Predictions for {}'.format(model))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the model scores\n",
    "model_compare = pd.DataFrame.from_dict(results, orient = 'index', columns=['R2 Score'])\n",
    "#add new index column\n",
    "model_compare['model'] = model_compare.index\n",
    "model_compare.reset_index(drop=True, inplace=True)\n",
    "#add best params\n",
    "model_compare['best params'] = best_params.values()\n",
    "#add mape\n",
    "model_compare['MAPE'] = mape_score.values()\n",
    "#median average percentage accuracy\n",
    "model_compare['Accuracy'] = 100 - model_compare['MAPE']\n",
    "model_compare.sort_values(by='MAPE', ascending=True, inplace=True)\n",
    "model_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regressor = MLPRegressor(max_iter=1000)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "              'activation': ['tanh', 'relu'],\n",
    "              'solver': ['sgd', 'adam'],\n",
    "              'alpha': [0.0001, 0.05],\n",
    "              'learning_rate': ['constant','adaptive']}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=5, return_train_score=True)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Use the best parameters to create a new neural network regressor\n",
    "best_regressor = MLPRegressor(max_iter=1000, **best_params)\n",
    "\n",
    "# Fit the best regressor to the training data\n",
    "best_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = best_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "ape_list = np.abs((grid_predictions-y_test)/y_test)*100\n",
    "   \n",
    "\n",
    "mse = np.median(ape_list)\n",
    "print(f'Median absolute percentage error: {mse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Now that we have carried out predictive analysis regarding the milk production, we will move on to some sentiment analysis. For this we will focus on sentiment regarding consumer prices of dairy products, as the prices have seen a big spike recently.\n",
    "\n",
    "The pre-processing steps used here are those which were covered during the data preparation and visualisation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to our .env file\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "#assingning the .env file to a variable\n",
    "config = dotenv_values(\".env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "tweet_list = []\n",
    "#Put your Bearer Token in the parenthesis below\n",
    "client = tweepy.Client(bearer_token=config['BEARER_TOKEN'])\n",
    "\n",
    "\n",
    "# -is:retweet removes retweets & lang:en limits to English tweets this is applied after every keyword due to issues otherwise\n",
    "query = 'price of milk -is:retweet lang:en OR cost of milk -is:retweet lang:en OR cost of dairy -is:retweet lang:en OR price of dairy -is:retweet lang:en'\n",
    "tweets = tweepy.Paginator(client.search_recent_tweets, query=query,\n",
    "                              tweet_fields=['context_annotations', 'created_at'], max_results=100).flatten(limit=1500)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet_list.append(tweet.text)\n",
    "\n",
    "\n",
    "# Create a dataframe from the tweets\n",
    "tweets = pd.DataFrame(tweet_list, columns=['full_text'])\n",
    "tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#importing the dataset\n",
    "tweets = pd.read_csv('/home/faelan/Downloads/dataset_twitter-scraper_2022-12-30_18-59-56-893.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tweets.filter(['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to remove the stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "#remove usernames\n",
    "text['full_text'] = text['full_text'].str.replace('@[^\\s]+',' ')#remove usernames\n",
    "text['full_text'] = text['full_text'].str.replace('[^\\w\\s]',' ')#remove punctuation\n",
    "text['full_text'] = text['full_text'].str.replace(\"[^a-zA-Z#]\", ' ')#remove special characters\n",
    "text['full_text'] = text['full_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))#stopwords\n",
    "text['full_text'] = text['full_text'].apply(lambda x: \" \".join(x.lower() for x in x.split())) #lowercase\n",
    "text['full_text'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "text['full_text'] = text['full_text'].apply(lambda x: str(TextBlob(x).correct())) #spell check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leminiyezation\n",
    "from textblob import Word\n",
    "text['full_text'] = text['full_text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))#removes the suffixes\n",
    "text['full_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzing the sentiment of the tweets\n",
    "text['sentiment'] = text['full_text'].apply(lambda x: TextBlob(x).sentiment[0])#polarity AKA assigning sentiment\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning the sentiment to the tweets\n",
    "text['sentiment'] = text['sentiment'].apply(lambda x: 'positive' if x > 0.05 else ('negative' if x < -0.05 else 'neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the sentiment\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='sentiment', data=text)\n",
    "plt.title('Sentiment Analysis')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is unbalanced. We will employ smote for class balancing before we proceed onto applying nlp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a word cloud\n",
    "from wordcloud import WordCloud\n",
    "def word_cloud(input):\n",
    "    all_words = ' '.join([text for text in text[text['sentiment'] == input]['full_text']])\n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.title(f'Word cloud for tweets with {input} sentiment', fontsize=25)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "text['tokenized'] = text['full_text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pichart for sentiment\n",
    "labels = text['sentiment'].value_counts().index\n",
    "sizes = text['sentiment'].value_counts().values\n",
    "colors = ['yellowgreen', 'gold', 'lightskyblue']\n",
    "explode = (0.1, 0.1, 0.1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.title('Sentiment Analysis', fontsize=20)\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Producing word cloud plots for tweets belonging to each sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud('positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud('neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = text['sentiment'].tolist()\n",
    "for i in range(len(Y)):\n",
    "    if Y[i] == 'positive':\n",
    "        Y[i] = 0\n",
    "    elif Y[i] == 'negative':\n",
    "        Y[i] = 1\n",
    "    elif Y[i] == 'neutral':\n",
    "        Y[i] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bag_words = CountVectorizer(max_df=0.2, min_df=10, max_features=1000, stop_words='english')\n",
    "bag = bag_words.fit_transform(text['full_text'])\n",
    "\n",
    "X = bag\n",
    "\n",
    "\n",
    "#splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#balance the target using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "#plot the count of the target\n",
    "sns.countplot(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification for this code repitition is found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a model we want to classify the tweets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "#create a dictionary of models\n",
    "models = {'SVC': SVC, 'LogisticRegression': LogisticRegression, 'RandomForestClassifier': RandomForestClassifier, 'MultinomialNB': MultinomialNB, 'GradientBoostingClassifier': GradientBoostingClassifier}\n",
    "\n",
    "#loop over the models\n",
    "for model in models.values():\n",
    "    if model == SVC:\n",
    "        #defining gridsearch parameters SVC\n",
    "        param_grid = {'C': np.linspace(0.01, 25, 1000) , 'gamma': np.linspace(0.001, 0.2, 1000), 'kernel': ['rbf'], 'random_state': [42], 'max_iter': [10000]}\n",
    "\n",
    "    elif model == LogisticRegression:\n",
    "        #defining gridsearch parameters LogisticRegression\n",
    "        param_grid = {'C': np.linspace(0.01,200,100 ), 'penalty': ['l1', 'l2'], 'max_iter': [10000], 'random_state': [42]}\n",
    "    elif model == RandomForestClassifier:\n",
    "        #defining gridsearch parameters RandomForestClassifier\n",
    "        param_grid = {'n_estimators': np.arange(450, 610, 5), 'max_depth': np.arange(30, 110, 5), 'random_state': [42]}\n",
    "\n",
    "    elif model == MultinomialNB:\n",
    "        #defining gridsearch parameters MultinomialNB\n",
    "        param_grid = {'alpha': np.linspace(0.01, 0.2, 1000), 'fit_prior': [True, False]}\n",
    "\n",
    "    elif model == GradientBoostingClassifier:\n",
    "        #defining gridsearch parameters GradientBoostingClassifier\n",
    "        param_grid = {'n_estimators': np.arange(200, 400, 5), 'max_depth': np.arange(0, 100, 10), 'random_state': [42]}\n",
    "    grid = RandomizedSearchCV(estimator = model(), param_distributions = param_grid, cv = 10, n_jobs = -1, n_iter=300, random_state=42)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(grid.best_estimator_)\n",
    "\n",
    "    #predicting the values\n",
    "    y_pred = grid.predict(X_test)\n",
    "\n",
    "    #printing the results\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    #plotting the confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')\n",
    "    plt.title(f'Confusion Matrix for {model}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Truth')\n",
    "    names = ['positive', 'negative', 'neutral']\n",
    "    plt.xticks(ticks=[0.5, 1.5, 2.5], labels=names)\n",
    "    plt.yticks(ticks=[0.5, 1.5, 2.5], labels=names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# Define Search Param\n",
    "params = {'n_components': [1, 2,3,5,7,9], 'learning_decay': [.9, 1, 1.2, 1.4]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "grid = GridSearchCV(lda, param_grid=params)\n",
    "\n",
    "# Do the Grid Search\n",
    "grid.fit(X)\n",
    "#best params\n",
    "print(grid.best_params_)\n",
    "#log likelihood\n",
    "print(grid.best_score_)\n",
    "#model perplexity\n",
    "print(grid.best_estimator_.perplexity(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the keywords for each topic, based off of ml tutorial 7 part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topic-word matrix\n",
    "lda = LatentDirichletAllocation(learning_decay=0.9, n_components=1, random_state=42)\n",
    "topic = lda.fit_transform(X)\n",
    "topic_word = lda.components_\n",
    "vocab = bag_words.get_feature_names()\n",
    "\n",
    "# Get the top 10 key words for each topic\n",
    "n_top_words = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our EDA did feature only the top 10, we can not continue with this in our statistics section as it will heavily skew our figures. When it comes to the inferential statistics section we may choose a sub set of countries at random and treat this as our sample population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#descriptive statistics for each numeric column in milk_eu_eda dataset\n",
    "milk_eu_eda.describe()\n",
    "\n",
    "#check if distributio each column is normal\n",
    "#choose the columns that are not object\n",
    "columns = milk_eu_eda.drop(['Year'], axis = 1).select_dtypes(exclude=['object'])\n",
    "#create a dictionary to store the results\n",
    "is_normal = {}\n",
    "for area in milk_eu_eda['Area'].unique():\n",
    "    for col in columns:\n",
    "        test = stats.shapiro(milk_eu_eda[milk_eu_eda['Area'] == area][col])\n",
    "        if test[1] > 0.05:\n",
    "            is_normal[f'{area} {col}'] = 'normal'\n",
    "        else:\n",
    "            is_normal[f'{area} {col}'] = 'not normal'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that none of these columns have normally distributed dataWhen separated by country we can see that some of the columns are normal, but not all. We have made a dictionary of normal and not normal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will generage descriptive stats for the values in each column per country\n",
    "def descriptive_stats(column, country):\n",
    "    #check if the column is numeric\n",
    "    if milk_eu_eda[column].dtype != 'object':\n",
    "       #check if normal\n",
    "        if is_normal[f'{country} {column}'] == 'normal':\n",
    "            #get mean and standard deviation for the column\n",
    "            mean = milk_eu_eda[milk_eu_eda['Area'] == country][column].mean()\n",
    "            std = milk_eu_eda[milk_eu_eda['Area'] == country][column].std()\n",
    "\n",
    "            return mean, std\n",
    "        else:\n",
    "            #get median, 25th percentile and 75th percentile for the column, max and min if not normal\n",
    "            median = milk_eu_eda[milk_eu_eda['Area'] == country][column].median()\n",
    "            q1 = milk_eu_eda[milk_eu_eda['Area'] == country][column].quantile(0.25)\n",
    "            q3 = milk_eu_eda[milk_eu_eda['Area'] == country][column].quantile(0.75)\n",
    "            max = milk_eu_eda[milk_eu_eda['Area'] == country][column].max()\n",
    "            min = milk_eu_eda[milk_eu_eda['Area'] == country][column].min()\n",
    "\n",
    "            return median, q1, q3, max, min\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#loop over each country and their columns using this function and store the results in a dictionary\n",
    "descriptive_stats_dict = {}\n",
    "for country in milk_eu_eda['Area'].unique():\n",
    "    for col in columns:\n",
    "        descriptive_stats_dict[f'{country} {col}'] = descriptive_stats(col, country)\n",
    "        \n",
    "\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats for head dataframes\n",
    "head = pd.DataFrame()\n",
    "head['Country'] = milk_eu_eda['Area'].unique()\n",
    "#append head descriptive stats to the head dataframe\n",
    "def stats_list(measure):\n",
    "    stats = []\n",
    "    for country in milk_eu_eda['Area'].unique():\n",
    "        for col in columns:\n",
    "            if col == measure:\n",
    "                stats.append(descriptive_stats_dict[f'{country} {col}'])\n",
    "            else:\n",
    "                pass\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_eu_eda.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = stats_list('Head')\n",
    "Volume = stats_list('volume(litre)')\n",
    "pasture = stats_list('% Pasture')\n",
    "Revenue = stats_list('Revenue(usd)')\n",
    "Export = stats_list('Export Value per litre(USD)')\n",
    "Tonne = stats_list('Tonne')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add the descriptive stats to a dataframe\n",
    "def add_stats(stats):\n",
    "    df = pd.DataFrame(columns = ['Country', 'Mean', 'Std', 'Median', 'Q1', 'Q3', 'Max', 'Min'])\n",
    "    for i,j in zip(milk_eu_eda.Area.unique(), stats):\n",
    "        if len(j) == 2:\n",
    "            row = [i, j[0], j[1], np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "        else:\n",
    "            row = [i, np.nan, np.nan, j[0], j[1], j[2], j[3], j[4]]\n",
    "\n",
    "        #append the row to the dataframe\n",
    "        df.loc[len(df)] = row\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Head = add_stats(head)\n",
    "Volume = add_stats(Volume)\n",
    "Pasture = add_stats(pasture)\n",
    "Revenue = add_stats(Revenue)\n",
    "Export = add_stats(Export)\n",
    "Tonne = add_stats(Tonne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country to index dictionary\n",
    "country_to_index = {}\n",
    "for i, country in enumerate(milk_eu_eda['Area'].unique()):\n",
    "    country_to_index[country] = i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#produce boxplot or normal distribution plot for each column per country\n",
    "import ast\n",
    "def plot_dist(country, df):\n",
    "    #check if normal\n",
    "    if is_normal[f'{country} {df}'] == 'normal':\n",
    "        data = milk_eu_eda[milk_eu_eda['Area'] == country][str(df)]\n",
    "        mean = descriptive_stats_dict[f'{country} {df}'][0]\n",
    "        std = descriptive_stats_dict[f'{country} {df}'][1]\n",
    "        x = np.linspace(mean - 5*std, mean + 5*std, 100)\n",
    "        plt.plot(x, stats.norm.pdf(x, mean, std))\n",
    "        plt.hist(data, bins = 10, alpha = 0.5, density = True, color = 'gray')\n",
    "        plt.title(f'{country} {df} Distribution', fontsize = 25)\n",
    "        plt.xlabel(f'{df}', fontsize = 20)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        data = milk_eu_eda[milk_eu_eda['Area'] == country][str(df)]\n",
    "        sns.boxplot(data)\n",
    "        plt.title(f'{country} {df} Distribution', fontsize = 25)\n",
    "        plt.xlabel(f'{df}', fontsize = 20)\n",
    "        plt.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist('Germany', 'Export Value per litre(USD)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Confidence interval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on population proportion of % pasture usage above 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe that has all the % pasture land per country in 2020\n",
    "pasture_2020 = milk_eu_eda[milk_eu_eda['Year'] == 2020][['Area', '% Pasture']]\n",
    "pasture_2020.head()\n",
    "\n",
    "#test distribution of % pasture land\n",
    "dist = stats.shapiro(pasture_2020['% Pasture'])\n",
    "print(dist)\n",
    "#sample proportion\n",
    "meet_cond = pasture_2020[pasture_2020['% Pasture'] > 0.15].shape[0]\n",
    "print(meet_cond)\n",
    "total = pasture_2020.shape[0]\n",
    "\n",
    "#confidence interval for population proportion\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "proportion_confint(meet_cond, total, alpha = 0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses a normal approximation to the binomial distribution to calculate the CI, even if the data isnt normally distributed, so is nonparametric. I have chosen a larger level of confidence here as the sample size is rather small and this methods accuracy scales with sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Head_2020 = milk_eu_eda[milk_eu_eda['Year'] == 2020][['Area', 'Head']]\n",
    "\n",
    "#check if normal\n",
    "dist = stats.shapiro(Head_2020['Head'])\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is NOT normal, so we will be looking at calculating a CI on the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample median \n",
    "\n",
    "#sort the data\n",
    "Head_2020.sort_values(by = 'Head', inplace = True)\n",
    "\n",
    "#confidence interval for population median\n",
    "def CI_median(data):\n",
    "    n = data.shape[0]\n",
    "    z = 1.96\n",
    "    q = 0.5\n",
    "    #ci given by index +/1 z*sqrt(q*(1-q))*n\n",
    "    ci = z*np.sqrt(q*(1-q)*n)\n",
    "    lower = np.ceil(data.shape[0]/2 - ci)-1 #subtracting one since the index starts from 0\n",
    "    upper = np.ceil(data.shape[0]/2 + ci)-1\n",
    "    return int(lower), int(upper)\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_head = CI_median(Head_2020)\n",
    "ci_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find corresponding values\n",
    "print(Head_2020.iloc[ci_head[0]][1], ',', Head_2020.iloc[ci_head[1]][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a cI on the amount of milk produced in litre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Volume_2020 = milk_eu_eda[milk_eu_eda['Year'] == 2020][['Area', 'volume(litre)']]\n",
    "\n",
    "#check if normal\n",
    "dist = stats.shapiro(Volume_2020['volume(litre)'])\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not normal again, so we will use the ci on the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_volume = CI_median(Volume_2020)\n",
    "ci_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the corresponding values\n",
    "print(Volume_2020.iloc[ci_volume[0]][1], ',', Volume_2020.iloc[ci_volume[1]][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the total milk production over the years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group the volumes by year\n",
    "Volume_year = milk_eu_eda.groupby('Year')['volume(litre)'].sum()\n",
    "Volume_year.plot(xticks=Volume_year.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comapring means of ireland, france and netherlands herd count over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anova test on ireland, netherlands, and france using two wat anova\n",
    "ireland = milk_eu_eda[(milk_eu_eda['Area'] == 'Ireland')]['Head']\n",
    "netherlands = milk_eu_eda[(milk_eu_eda['Area'] == 'Netherlands')]['Head']\n",
    "france = milk_eu_eda[(milk_eu_eda['Area'] == 'France')]['Head']\n",
    "#check correlation between the three countries\n",
    "plt.plot(france, netherlands, 'o')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if equal variance\n",
    "from scipy import stats\n",
    "stats.levene(ireland, netherlands, france)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variances are unequal so we may not use anova. instead we will use a non parametric test, such as the Friedman test or friedman two way anova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "null-hypothesis: there is no significant differences between the mean\n",
    "\n",
    "Alt-hypothesis: there is a significant diffewrence between the means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running a friedman test\n",
    "head_friedman = stats.friedmanchisquare(ireland, netherlands, france)\n",
    "print(head_friedman)\n",
    "\n",
    "\n",
    "if head_friedman[1] < 0.05:\n",
    "    print('Reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing distributions in volume of milk produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Volume.sort_values(by = 'Median', inplace = True)\n",
    "Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon visual inspection ireland, spain, and denmark seem to have somewhat similar medians, on the same order of magnitude at the very least. since there are 3 non normal samples of data we will use kruskal wallis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ireland = milk_eu_eda[(milk_eu_eda['Area'] == 'Ireland')]['volume(litre)']\n",
    "france = milk_eu_eda[(milk_eu_eda['Area'] == 'France')]['volume(litre)']\n",
    "netherlands = milk_eu_eda[(milk_eu_eda['Area'] == 'Netherlands')]['volume(litre)']\n",
    "\n",
    "#run a kruskal wallis test\n",
    "volume_kruskal = stats.kruskal(ireland, france, netherlands)\n",
    "print(volume_kruskal)\n",
    "if volume_kruskal[1] < 0.05:\n",
    "    print('Reject the null hypothesis')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though our visual inspection was wrong, and there is infact a rather significant difference between irelands median and the two closest other values, denmark and spain. Even if we were to carry out a non-parametric test including the normal data, there is unlinkey to be a different outcome as in a normal distrinution the mean and median should be similar(or identifical if perfectly symmetrical) and even the closest means are likely not close enough to irelands median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the distribution in precentage of land that is used for pasture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pasture.sort_values(by = 'Mean', inplace = True)\n",
    "Pasture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three we will look at are ireland, austria and belgium. It appears as though irelands mean is significantly diffewrent to other countries, so we will check this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ireland = milk_eu_eda[(milk_eu_eda['Area'] == 'Ireland')]['% Pasture']\n",
    "france = milk_eu_eda[(milk_eu_eda['Area'] == 'France')]['% Pasture']\n",
    "netherlands = milk_eu_eda[(milk_eu_eda['Area'] == 'Netherlands')]['% Pasture']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of each distribution is not equal. This means that the Anova test may not be ideal for this scenario. Instead we will use the the Friedman test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kruskal wallis test\n",
    "pasture_mann = stats.mannwhitneyu(ireland, france, netherlands)\n",
    "print(pasture_mann)\n",
    "if pasture_mann[1] < 0.05:\n",
    "    print('Reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Revenue.sort_values(by = 'Median', inplace = True)\n",
    "Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ireland = milk_eu_eda[(milk_eu_eda['Area'] == 'Ireland')]['Revenue(usd)']\n",
    "france = milk_eu_eda[(milk_eu_eda['Area'] == 'France')]['Revenue(usd)']\n",
    "#mann whitney test\n",
    "revenue_mann = stats.mannwhitneyu(ireland, france, alternative = 'two-sided')\n",
    "print(revenue_mann)\n",
    "if revenue_mann[1] > 0.05:\n",
    "    print('Accept the null hypothesis')\n",
    "else:\n",
    "    print('Reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Export.sort_values(by = 'Median', inplace = True)\n",
    "Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ireland = milk_eu_eda[(milk_eu_eda['Area'] == 'Ireland')]['Export Value per litre(USD)'].tolist()\n",
    "denmark = milk_eu_eda[(milk_eu_eda['Area'] == 'Denmark')]['Export Value per litre(USD)'].tolist()\n",
    "spain = milk_eu_eda[(milk_eu_eda['Area'] == 'Spain')]['Export Value per litre(USD)'].tolist()\n",
    "netherlands = milk_eu_eda[(milk_eu_eda['Area'] == 'Netherlands')]['Export Value per litre(USD)'].tolist()\n",
    "france = milk_eu_eda[(milk_eu_eda['Area'] == 'France')]['Export Value per litre(USD)'].tolist()\n",
    "germany = milk_eu_eda[(milk_eu_eda['Area'] == 'Germany')]['Export Value per litre(USD)'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run friedman test\n",
    "export_friedman = stats.friedmanchisquare(ireland, denmark, spain, netherlands, france, germany)\n",
    "print(export_friedman)\n",
    "if export_friedman[1] < 0.05:\n",
    "    print('Reject the null hypothesis')\n",
    "else:\n",
    "    print('Accept the null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ks test\n",
    "from scipy.stats import kstest\n",
    "ireland = ireland\n",
    "germany = germany\n",
    "ireland_germany = stats.kstest(ireland, germany, alternative = 'two-sided')\n",
    "print(ireland_germany)\n",
    "if ireland_germany[1] > 0.05:\n",
    "    print('Accept the null hypothesis')\n",
    "else:\n",
    "    print('Reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ireland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "irelands export value is not comparable to that of the top 6 countries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_6_countries = milk_eu_eda[milk_eu_eda.Year == 2020].nlargest(6, 'Tonne')['Area']\n",
    "top_6_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_stats = Tonne[Tonne['Country'].isin(top_6_countries)]\n",
    "table_stats\n",
    "#adding interquartile range\n",
    "table_stats['IQR'] = table_stats['Q3'] - table_stats['Q1']\n",
    "table_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert table_stats to a table in microsoft word\n",
    "table_stats.to_excel('table_stats.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot box plot for top 6 countries Tonnes\n",
    "sns.boxplot(x = 'Area', y = 'Tonne', data = milk_eu_eda[milk_eu_eda['Area'].isin(top_6_countries)])\n",
    "plt.title('Distribution of milk quantity in top 6 countries', fontsize = 25)\n",
    "plt.ylabel('Tonne', fontsize = 20)\n",
    "plt.xlabel('Country', fontsize = 20)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print year that corresponds to the outlier in ireland\n",
    "da"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "890563eb1401dd7c5eac482b2070a231034cb0eabe59bf1a3eb86f9e36919f52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
