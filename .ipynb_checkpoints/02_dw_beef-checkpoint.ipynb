{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents (Clickable in sidebar)<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#The-revised-research-question\" data-toc-modified-id=\"The-revised-research-question-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>The revised research question</a></span></li><li><span><a href=\"#Libraries,---modules-and-orientation\" data-toc-modified-id=\"Libraries,---modules-and-orientation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Libraries,   modules and orientation</a></span></li><li><span><a href=\"#Functions-Section\" data-toc-modified-id=\"Functions-Section-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Functions Section</a></span><ul class=\"toc-item\"><li><span><a href=\"#Title-function\" data-toc-modified-id=\"Title-function-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Title function</a></span></li><li><span><a href=\"#The-clean-function-creates-multiple-single-variable--files-acts-onexpansive-source-files\" data-toc-modified-id=\"The-clean-function-creates-multiple-single-variable--files-acts-onexpansive-source-files-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>The clean function creates multiple single variable  files acts onexpansive source files</a></span></li></ul></li><li><span><a href=\"#Combine-CCKP--Timeseries-Files-and-Add-a-CountryYear-Key\" data-toc-modified-id=\"Combine-CCKP--Timeseries-Files-and-Add-a-CountryYear-Key-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Combine CCKP  Timeseries Files and Add a CountryYear Key</a></span></li><li><span><a href=\"#The-Climate-Change-Knowledge-Portal.-(n.d.).-Retrieved-February-20,-2023,-from-https://climateknowledgeportal.worldbank.org/\" data-toc-modified-id=\"The-Climate-Change-Knowledge-Portal.-(n.d.).-Retrieved-February-20,-2023,-from-https://climateknowledgeportal.worldbank.org/-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>The Climate Change Knowledge Portal. (n.d.). Retrieved February 20, 2023, from <a href=\"https://climateknowledgeportal.worldbank.org/\" rel=\"nofollow\" target=\"_blank\">https://climateknowledgeportal.worldbank.org/</a></a></span></li><li><span><a href=\"#The-FAOSTAT-and-CCKP-data-become-one!\" data-toc-modified-id=\"The-FAOSTAT-and-CCKP-data-become-one!-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>The FAOSTAT and CCKP data become one!</a></span></li><li><span><a href=\"#Balancing-Statistical-confidence--with-model-simplicity\" data-toc-modified-id=\"Balancing-Statistical-confidence--with-model-simplicity-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Balancing Statistical confidence  with model simplicity</a></span></li><li><span><a href=\"#Processing-predictor-data\" data-toc-modified-id=\"Processing-predictor-data-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Processing predictor data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Image-below-shows-download-control-panel\" data-toc-modified-id=\"Image-below-shows-download-control-panel-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Image below shows download control panel</a></span></li><li><span><a href=\"#Basic-code-is-explained-line-by-line-below\" data-toc-modified-id=\"Basic-code-is-explained-line-by-line-below-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Basic code is explained line by line below</a></span></li><li><span><a href=\"#Rationale-for-data-frame-reductions--and-other-EDA-decisions\" data-toc-modified-id=\"Rationale-for-data-frame-reductions--and-other-EDA-decisions-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Rationale for data frame reductions  and other EDA decisions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Post-2002-missing-data-beahavior\" data-toc-modified-id=\"Post-2002-missing-data-beahavior-8.3.1\"><span class=\"toc-item-num\">8.3.1&nbsp;&nbsp;</span>Post 2002 missing data beahavior</a></span></li><li><span><a href=\"#Top-EU-Cattle-Stock-Countries\" data-toc-modified-id=\"Top-EU-Cattle-Stock-Countries-8.3.2\"><span class=\"toc-item-num\">8.3.2&nbsp;&nbsp;</span>Top EU Cattle Stock Countries</a></span></li><li><span><a href=\"#Further-variables\" data-toc-modified-id=\"Further-variables-8.3.3\"><span class=\"toc-item-num\">8.3.3&nbsp;&nbsp;</span>Further variables</a></span></li></ul></li></ul></li><li><span><a href=\"#Investigation-of-Several-Techniques-of-Visualising-the-Missing-Data\" data-toc-modified-id=\"Investigation-of-Several-Techniques-of-Visualising-the-Missing-Data-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Investigation of Several Techniques of Visualising the Missing Data</a></span></li><li><span><a href=\"#Nutrient-Data-Set-is-Clean\" data-toc-modified-id=\"Nutrient-Data-Set-is-Clean-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Nutrient Data Set is Clean</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling   for Irish Beef \n",
    "\n",
    "## The revised research question\n",
    "How has Ireland's beef sector performed compared to the EU 27 countries since 2000, and can we forecast future prices using this historical data? Additionally, what can we learn from sentiment analysis of the beef industry during this time period? By focusing on data from 2000 onwards, we can better capture the current state of the beef industry and make more relevant predictions about future trends. \n",
    "## Libraries,   modules and orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Data Manipulation and Analysis\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fancyimpute\n",
    "import missingno as msno\n",
    "from functools import partial, reduce\n",
    "### Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.image as mpimg\n",
    "### Statistical Analysis\n",
    "from scipy.stats import ks_2samp, shapiro\n",
    "### Machine Learning\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "### Text Processing\n",
    "import html\n",
    "import re\n",
    "import inflection\n",
    "### Country Information\n",
    "from countryinfo import CountryInfo\n",
    "import pycountry\n",
    "from countrygroups import EUROPEAN_UNION\n",
    "### File System and OS\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "### Date and Time\n",
    "import datetime\n",
    "import time\n",
    "### Data Presentation\n",
    "from tabulate import tabulate\n",
    "from IPython.display import HTML, Image, display\n",
    "\n",
    "### Data Types\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "\n",
    "# Filter out the FutureWarning with the level keyword\n",
    "# warnings.filterwarnings('ignore', message='Using the level keyword in DataFrame and Series aggregations is deprecated')\n",
    "\n",
    "# Reset the warning filter to default\n",
    "# warnings.filterwarnings('default')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Section\n",
    "\n",
    "In order to tidy up the layout and reading and due to the logical nature of functions we collect them here rather than leave them scattered throughout the notepad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "\n",
    "### Title function\n",
    "The 'title' naming convention is where each word starts\n",
    "with a capital letter, except for prepositions and conjunctions, which start with a lowercase letter. \n",
    " The function takes a string as input and converts it to title case, \n",
    "where the first letter of each non-conjunction/preposition word is capitalized, \n",
    "and all other letters are lowercase.\n",
    "It achieves this by splitting the input string into a list of words, -->\n",
    "identifying which words are prepositions or conjunctions based on a predefined list, \n",
    "nd then capitalizing the first letter of all other words while converting prepositions \n",
    "nd conjunctions to lowercase. The resulting list of processed words is then joined back into a\n",
    "single string with proper spacing and returned.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Rename the 'Area' column to 'Country'\n",
    "    df = df.rename(columns={'Area': 'Country'})\n",
    "\n",
    "    # Filter out specified countries and keep required columns\n",
    "    countries_to_drop = ['Belgium-Luxembourg','Austria', 'Malta', 'Bulgaria', 'Slovakia', 'Netherlands', 'Latvia']\n",
    "    df = df[~df['Country'].isin(countries_to_drop)]\n",
    "    df = df[['Country', 'Year', 'Element', 'Item', 'Value']]\n",
    "\n",
    "    # Filter for year >= 2002\n",
    "    df = df[df['Year'] >= 2002]\n",
    "\n",
    "    # Create a 'Key' column by concatenating 'Country' and 'Year'\n",
    "    df['Key'] = df['Country'] + df['Year'].astype(str)\n",
    "\n",
    "    # Convert values in the Item and Element columns to clean snake\n",
    "    df['Item'] = df['Item'].apply(snake)\n",
    "    df['Element'] = df['Element'].apply(snake)\n",
    "\n",
    "    # Generate item and element lists\n",
    "    item_df = df['Item'].unique()\n",
    "    element_df = df['Element'].unique()\n",
    "\n",
    "    # Create separate dataframes for each combination of Element and Item\n",
    "    df_names = []\n",
    "    combinations = df[['Element', 'Item']].drop_duplicates()\n",
    "    for i, row in combinations.iterrows():\n",
    "        # Get the current combination of Element and Item\n",
    "        e = row['Element']\n",
    "        it = row['Item']    \n",
    "        # Create a DataFrame for the current combination\n",
    "        df_name = f\"{e}_{it}_df\"\n",
    "        globals()[df_name] = df[(df['Element'] == e) & (df['Item'] == it)].reset_index(drop=True)\n",
    "        df_names.append(df_name)\n",
    "\n",
    "    # Filter for year >= 2002\n",
    "    df = df[df['Year'] >= 2002]\n",
    "\n",
    "    # Return the list of dataframe names\n",
    "    return df_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def title(sentence):\n",
    "    \"\"\"\n",
    "    Takes a string and converts it to title case, where the first letter of each\n",
    "    non-conjunction/preposition word is capitalized, and all other letters are lowercase.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): The string to convert to title case.\n",
    "        \n",
    "    Returns:\n",
    "        str: The input string converted to title case.\n",
    "    \"\"\"\n",
    "    # Define a list of common prepositions and conjunctions\n",
    "    prepositions_conjunctions = ['a', 'this', 'an', 'the', 'and', 'but', 'or', 'for', 'has', 'nor', 'on', 'at', 'to', 'from', 'by', 'over', 'under', 'in', 'out', 'of']\n",
    "    # Split the input string into a list of words\n",
    "    words = sentence.split()\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        # If the word is not a preposition or conjunction, capitalize the first letter and lowercase the rest\n",
    "        if word.lower() not in prepositions_conjunctions:\n",
    "            processed_words.append(word.capitalize())\n",
    "        # If the word is a preposition or conjunction, convert to lowercase\n",
    "        else:\n",
    "            processed_words.append(word.lower())\n",
    "    # Join the list of processed words into a single string, with proper spacing\n",
    "    output = \" \".join(processed_words)\n",
    "    # Remove any leading/trailing whitespace and add some padding\n",
    "    return \"     \" + re.sub('\\s+', ' ', output.strip()) + \"     \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this_is_a_text_with_pe_riods_and_other_characters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def snake(text, default='default'):\n",
    "    \"\"\"\n",
    "    Converts a given string to snake_case by replacing any whitespace characters with underscores,\n",
    "    converting to all lowercase, and removing any non-alphanumeric characters from the beginning and end.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The string to convert to snake_case.\n",
    "        default (str): The default value to return if the input text is empty.\n",
    "\n",
    "    Returns:\n",
    "        str: The resulting string in snake_case format, or the default value if the input text is empty.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return default\n",
    "    # Convert to string and replace any non-alphanumeric characters at the beginning and end with an empty string\n",
    "    text = re.sub(r'^\\W+|\\W+$', '', str(text))\n",
    "    # Replace any period symbols with underscores\n",
    "    text = text.replace('.', '')\n",
    "    # Replace any other non-alphanumeric characters with empty strings\n",
    "    text = re.sub(r'\\W+', '_', text)\n",
    "    # Convert to all lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "text = \"This is a text with pe,  ,  ,  ,riods. (And other characters   .)\"\n",
    "result = snake(text)\n",
    "print(result)  # Output: \"this_is_a_text_with_periods_and_other_characters\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "### The clean function creates multiple single variable  files acts onexpansive source files\n",
    "-   Reads a CSV file located at file_path into a pandas DataFrame\n",
    "-   Renames the column named 'Area' to 'Country' in the DataFrame\n",
    "-   Filters out specified countries and retains only required columns\n",
    "-   Filters for years greater than or equal to 2002\n",
    "-   Creates a new column named 'Key' by concatenating the 'Country' and 'Year' columns\n",
    "-   Converts values in the 'Item' and 'Element' columns to snake case\n",
    "-   Generates unique lists of items and elements in the DataFrame\n",
    "-   Creates separate DataFrames for each combination of element and item\n",
    "-   Returns the final DataFrame\n",
    "-   Specifically, the function performs the following steps\n",
    "\n",
    "Regenerate response\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Rename the 'Area' column to 'Country'\n",
    "    df = df.rename(columns={'Area': 'Country'})\n",
    "    # Filter out specified countries and keep required columns\n",
    "    countries_to_drop = ['Belgium-Luxembourg','Austria', 'Malta', 'Bulgaria', 'Slovakia', 'Netherlands', 'Latvia']\n",
    "    df = df[~df['Country'].isin(countries_to_drop)]\n",
    "    df = df[['Country', 'Year', 'Element', 'Item', 'Value']]\n",
    "    # Filter for year >= 2000\n",
    "    df = df[df['Year'] >= 2002]\n",
    "    # Create a 'Key' column by concatenating 'Country' and 'Year'\n",
    "    df['Key'] = df['Country'] + df['Year'].astype(str)\n",
    "    # Convert values in the Item and Element columns to clean snake\n",
    "    df['Item'] = df['Item'].apply(snake)\n",
    "    df['Element'] = df['Element'].apply(snake)\n",
    "    # Generate item and element lists\n",
    "    item_df = df['Item'].unique()\n",
    "    element_df = df['Element'].unique()\n",
    "    # Create separate dataframes for each combination of Element and Item\n",
    "    combinations = df[['Element', 'Item']].drop_duplicates()\n",
    "    for i, row in combinations.iterrows():\n",
    "        # Get the current combination of Element and Item\n",
    "        e = row['Element']\n",
    "        it = row['Item']    \n",
    "        # Create a DataFrame for the current combination\n",
    "        df_name = f\"{e}_{it}_df\"\n",
    "        globals()[df_name] = df[(df['Element'] == e) & (df['Item'] == it)].reset_index(drop=True)\n",
    "    # Return the final dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Rename the 'Area' column to 'Country'\n",
    "    df = df.rename(columns={'Area': 'Country'})\n",
    "\n",
    "    # Filter out specified countries and keep required columns\n",
    "    countries_to_drop = ['Belgium-Luxembourg','Austria', 'Malta', 'Bulgaria', 'Slovakia', 'Netherlands', 'Latvia']\n",
    "    df = df[~df['Country'].isin(countries_to_drop)]\n",
    "    df = df[['Country', 'Year', 'Element', 'Item', 'Value']]\n",
    "\n",
    "\n",
    "    # Filter for year >= 2002\n",
    "    df = df[df['Year'] >= 2002]\n",
    "\n",
    "    # Create a 'Key' column by concatenating 'Country' and 'Year'\n",
    "    df['Key'] = df['Country'] + df['Year'].astype(str)\n",
    "\n",
    "    # Convert values in the Item and Element columns to clean snake\n",
    "    df['Item'] = df['Item'].apply(snake)\n",
    "    df['Element'] = df['Element'].apply(snake)\n",
    "\n",
    "    # Generate item and element lists\n",
    "    item_df = df['Item'].unique()\n",
    "    element_df = df['Element'].unique()\n",
    "\n",
    "    # Create separate dataframes for each combination of Element and Item\n",
    "    combinations = df[['Element', 'Item']].drop_duplicates()\n",
    "    for i, row in combinations.iterrows():\n",
    "        # Get the current combination of Element and Item\n",
    "        e = row['Element']\n",
    "        it = row['Item']    \n",
    "        # Create a DataFrame for the current combination\n",
    "        df_name = f\"{e}_{it}_df\"\n",
    "        globals()[df_name] = df[(df['Element'] == e) & (df['Item'] == it)].reset_index(drop=True)\n",
    "\n",
    "    # Print the list of DataFrames for each combination of Element and Item\n",
    "    for combination in combinations.itertuples(index=False):\n",
    "        e, it = combination\n",
    "        df_name = f\"{e}_{it}_df\"\n",
    "        print(f\"DataFrames for {e} - {it}:\\n{globals()[df_name]}\")\n",
    "\n",
    "    # Return the final dataframe\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean('raw/nutrient.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanraw(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[df['Year'] >= 2002]\n",
    "    df = df[df['Area'] != '']\n",
    "    # remove excluded countries\n",
    "    excluded_countries = ['Austria', 'Malta', 'Bulgaria', 'Slovakia', 'Netherlands', 'Latvia','Belgium-Luxembourg']\n",
    "    df = df[~df['Area'].isin(excluded_countries)]\n",
    "    df = df.rename(columns={'Area': 'Country'})\n",
    "    df['Key'] = df['Country'] + df['Year'].astype(str)\n",
    "    df = df[['Country','Year','Element','Item', 'Value','Key']]\n",
    "    df['Item'] = df['Item'].apply(snake)\n",
    "    df['Element'] = df['Element'].apply(snake)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Combine CCKP  Timeseries Files and Add a CountryYear Key\n",
    "\n",
    "The combine_timeseries_files function reads multiple CSV files from the rain or temperature subfolders.Referencing the first five rows of Rain_IRL_df  below  the first unnamed column gives the year and based on position the name of the country is extracted. The function then forms a Key by concatrenating the Country and Year values together. This 'Key' column  will be  used to merge with our Cattle Stocks data   to single dataframe. It filters to only include the first two columns, which are Year and corresponding weather measurements and as such, it drops all redundant regional readings. The resulting dataframe contains the weather data of 27 European countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the first two rows of data, with headers in the second row\n",
    "Rain_IRL_df = pd.read_csv('rain/pr_timeseries_annual_cru_1901-2021_IRL.csv', header=1, nrows=2)\n",
    "\n",
    "# Show the dataframe\n",
    "Rain_IRL_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The Climate Change Knowledge Portal. (n.d.). Retrieved February 20, 2023, from https://climateknowledgeportal.worldbank.org/\n",
    "\n",
    "The Climate Change Knowledge Portal. (n.d.). Retrieved February 20, 2023, from https://climateknowledgeportal.worldbank.org/\n",
    "To maintain consistency with the FAO data, annual and not monthly time series  aggregates were taken from the Climatic Research Unit (CRU) dataset for **precipitation** and   **mean-temperature**. These datasets are provided by the CRU TS 4.04 dataset, a gridded climate dataset produced by the Climatic Research Unit (CRU) at the University of East Anglia in the United Kingdom. In the statistics section, range, variance, and standard deviation of monthly data may be revisited for insights.\n",
    "\n",
    "The file names and folder names of the CCKP data used in this project are tabulated below.\n",
    "\n",
    "<span style=\"font-size: 24px;\">     </span>\n",
    "        \n",
    "All datasets from the CCKP are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO (CC BY-NC-SA 3.0 IGO). \n",
    "Source: CCKP (2023). Time Series datasets. Retrieved from [https://climateknowledgeportal.worldbank.org/download-data]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_timeseries_files(path: str, subfolder: str, col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads in all CSV files in the given subfolder of the directory, renames the first unnamed column based on its position,\n",
    "    and combines the resulting dataframes together into a single dataframe.\n",
    "    Args:\n",
    "        path (str): The relative path of the directory containing the data.\n",
    "        subfolder (str): The name of the subfolder within the directory to read the CSV files from.\n",
    "        col_name (str): The name to assign to the specified column in the resulting dataframe.\n",
    "    Returns:\n",
    "        pandas.DataFrame: The resulting dataframe after combining data from all CSV files in the specified subfolder.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        folder_path = os.path.join(path, subfolder)\n",
    "        csv_filenames = glob.glob(folder_path + \"/*.csv\")\n",
    "        # Read in all CSV files, rename the first unnamed column based on position, and filter to only include the 'key' and specified column\n",
    "        processed_dfs = []\n",
    "        for filename in csv_filenames:\n",
    "            file_path = os.path.join(path, subfolder, os.path.basename(filename))\n",
    "            df = pd.read_csv(file_path, on_bad_lines='skip', skiprows=1)\n",
    "            if df.columns[0].startswith(\"Unnamed\"):\n",
    "                df.rename(columns={df.columns[0]: \"Year\"}, inplace=True)\n",
    "            df['Key'] = df.columns[1] + df['Year'].astype(str)\n",
    "            df.rename(columns={df.columns[1]: col_name}, inplace=True)\n",
    "            df = df.filter(['Key', col_name])\n",
    "            processed_dfs.append(df)\n",
    "        # Concatenate all dataframes into a single dataframe\n",
    "        df = pd.concat(processed_dfs, ignore_index=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining CSV files in folder {folder_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FAOSTAT and CCKP data become one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rmmy stands for rain in mm/year\n",
    "rain_df = combine_timeseries_files('', 'rain', 'Rmmy')\n",
    "# save the rain DataFrame to a CSV file\n",
    "rain_df.to_csv('clean/rain.csv', index=False)\n",
    "rain_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_df = combine_timeseries_files('', 'temperature', 'T\\u00b0C')\n",
    "# save the rain DataFrame to a CSV file\n",
    "temperature_df.to_csv('clean/temperature.csv', index=False)\n",
    "temperature_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned file saved in 'beef/clean folder' in first notebook\n",
    "main_df = pd.read_csv('clean/stock.csv')# loads the cleaned cattle stock  CSV file to pandas DataFrame n df\n",
    "main_df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming the dataframe is called 'df'\n",
    "main_df['Key'] = main_df['Country'].str.cat(main_df['Year'].astype(str), sep='_')\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adds a unique key column to our dataframe \n",
    "based on the \"Country\" and \"Year\" columns by casting year to string type.\n",
    "\"\"\"\n",
    "main_df['Key'] = main_df['Country'] + main_df['Year'].astype(str)# adds a unique key column \n",
    "main_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.merge(main_df, rain_df, on='Key')\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.merge(main_df, temperature_df, on='Key')\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Balancing Statistical confidence  with model simplicity\n",
    "\n",
    "- The extent of the data we use in terms of Countries, Time, Variables can be narrrowed while solving missing values issues \n",
    "- Sample size: The larger the sample size, the smaller the confidence interval will be.\n",
    "- At the same time the sample size across variables , time and counries must be sufficient to provide reliable results.But we will still further cut countries and years in the interest of simplicity and processing times.\n",
    "- Margin of error: The margin of error is the range of values within which the realised future  predictions of production nd price will likely lie.\n",
    "-  By reducing the margin of error, the confidence interval becomes smaller.\n",
    "- Significance level: The significance level is the probability of rejecting the null hypothesis when it is actually true. By using a smaller significance level (e.g., 0.01 instead of 0.05), the confidence interval will be narrower.\n",
    "- Confidence level: The confidence level is the degree of certainty that the population parameter falls within the confidence interval. By increasing the confidence level (e.g., from 90% to 95%), the confidence interval will become wider, but the predictions will be more reliable.\n",
    "- Model complexity: By using simpler models with fewer parameters, the predictions may be more focused and easier to interpret. \n",
    "- We will prepare a wide range of data but agressively jettison most of it during pre-processing.\n",
    "- Variable selection: Choosing the most relevant variables will be based on the data not hunches.\n",
    "- Data quality: Ensuring that the data is accurate, complete, and representative of the population of interest can help to reduce uncertainty and increase the precision of the predictions.\n",
    "- Using the 'nutrient.csv' data to  narrow  range of data\n",
    "- Overall, a combination of all of these  factors can help to ensure that statistical research results are reliable, actionable, and focused while keeping processing loads and project comlpexity under control.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing predictor data\n",
    "### Image below shows download control panel\n",
    "- db [https://www.fao.org/faostat/en/#data/domains_table](https://www.fao.org/faostat/en/#data/domains_table) /Land, Inputs and Sustainability/Inputs/ Fertilizers by Nutrient\n",
    "- The  image below demonstrates retrieval settings for 'Fertilizers by Nutrient' database was \n",
    "- European Union (27) > (List) \n",
    "- >1999 as per revised research question\n",
    "- All 'Items' and All 'Elemement' selected for later grouping\n",
    "![image1](images/beef002.png)\n",
    "\n",
    "### Basic code is explained line by line below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the original/source/raw DataFrame\n",
    "df = pd.read_csv('raw/nutrient.csv')\n",
    "# filter for time\n",
    "df = df[df['Year'] >= 2000]\n",
    "# remove  \"Belgium-Luxembourg\" based on 01...ijpnb decision\n",
    "df = df[df['Area'] != 'Belgium-Luxembourg']\n",
    "# rename the 'Area' column to 'Country' \n",
    "df = df.rename(columns={'Area': 'Country'})\n",
    "# create a 'Key' column by concatenating 'Country' and 'Year'\n",
    "df['Key'] = df['Country'] + df['Year'].astype(str)\n",
    "# keep required varient columns\n",
    "df = df[['Country','Year','Element','Item', 'Value','Key']]\n",
    "# Convert  values in the Item  and Element columns to clean snake\n",
    "df['Item'] = df['Item'].apply(snake)\n",
    "df['Element'] = df['Element'].apply(snake)\n",
    "# Generate item and element lists as per image above\n",
    "item_df=df['Item'].unique()\n",
    "element_df = df['Element'].unique()\n",
    "# Print several objects for critical thinking and EDA\n",
    "print(df.columns)\n",
    "print(df['Element'].unique())\n",
    "print(df['Item'].unique())\n",
    "# print the final dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the unique combinations of Element and Item\n",
    "combinations = df[['Element', 'Item']].drop_duplicates()\n",
    "# loop through all combinations of Element and Item\n",
    "for i, row in combinations.iterrows():\n",
    "    # get the current combination of Element and Item\n",
    "    e = row['Element']\n",
    "    it = row['Item']    \n",
    "    # create a DataFrame for the current combination\n",
    "    df_name = f\"{e}_{it}_df\"\n",
    "    globals()[df_name] = df[(df['Element'] == e) & (df['Item'] == it)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the number of missing values for each country and year\n",
    "missing_values = df.groupby(['Country', 'Year'])['Value'].apply(lambda x: x.isna().sum())\n",
    "# Reshape the data to a pivot table for plotting\n",
    "missing_values = missing_values.unstack('Country')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rationale for data frame reductions  and other EDA decisions\n",
    "\n",
    "\n",
    "- Decision to drop 2000, 2001 and 2002 is based on heatmap below\n",
    "- A visual inspection of missing data shows almost all countries in our sample have disproportionate levels of missing data in 2000 and 2001\n",
    "- That is why 2000 and 2001 are cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Verification  and Culling countries\n",
    "\n",
    "A visual inspection now suggest we drop the \n",
    "\n",
    "#### Post 2002 missing data beahavior\n",
    "- Post 2002 missing data beahavior is exclusive to these 5 countries\n",
    "- This is an ordered list\n",
    "- Austria had the most missing data\n",
    "- The Netherlands the least\n",
    "\n",
    "| Index | Country     |\n",
    "|-------|-------------|\n",
    "| 1     | Austria     |\n",
    "| 2     | Malta       |\n",
    "| 3     | Bulgaria    |\n",
    "| 4     | Slovakia    |\n",
    "| 5     | Netherlands |\n",
    "| 6     | Latvia      |\n",
    "\n",
    "- Strategy solves a missing data problem \n",
    "- While meeting project aim to jettison data\n",
    "- We see the Netherlands is 7th in the average stock league table \n",
    "- We could simply remove it \n",
    "- or we could simplify the project greatly and narrow our research question to the top 6 producers?\n",
    "- The ML stage will infor this\n",
    "\n",
    "#### Top EU Cattle Stock Countries\n",
    "\n",
    "| Index | Country      |\n",
    "|-------|--------------|\n",
    "| 1     | France       |\n",
    "| 2     | Germany      |\n",
    "| 3     | Ireland      |\n",
    "| 4     | Spain        |\n",
    "| 5     | Italy        |\n",
    "| 6     | Poland       |\n",
    "| 7     | Netherlands  |\n",
    "| 8     | Belgium      |\n",
    "\n",
    "####  Further variables\n",
    "\n",
    "- Here we only looked at the Soil Nutrient data \n",
    "- From it we decided to remove 2000 and 2001 \n",
    "- Furthermore we removed 6 countries including the 7th in the stock league tables the Netherlands\n",
    "- We may narrow our zone of interest further but will import and grow our main data file based on the YearCountry merging key and unique Element Item combos\n",
    "- Finally we will keep the headers painfully abreviated but generate a data dictionary for clarity of communifiaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation of Several Techniques of Visualising the Missing Data     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Heatmap of missing values\n",
    "sns.heatmap(missing_values, cmap='Blues')\n",
    "plt.title('Missing Values by Country and Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value count by year\n",
    "missing_by_year = missing_values.sum(axis=1)\n",
    "plt.plot(missing_by_year.index, missing_by_year.values)\n",
    "plt.title('Missing Values by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Missing Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the number of missing values for each country and year\n",
    "missing_values = df.groupby(['Country', 'Year'])['Value'].apply(lambda x: x.isna().sum())\n",
    "# Reshape the data to a pivot table for plotting\n",
    "missing_values = missing_values.unstack('Country')\n",
    "# Get the top 8 countries with the most missing values\n",
    "top_countries = missing_values.sum().sort_values(ascending=False)[:8].index\n",
    "# Stack the missing values and reset the index\n",
    "missing_values_stacked = missing_values[top_countries].stack().reset_index()\n",
    "missing_values_stacked.columns = ['Year', 'Country', 'Missing Values']\n",
    "# Line plot of missing values\n",
    "sns.lineplot(data=missing_values_stacked, x='Year', y='Missing Values', hue='Country')\n",
    "plt.title('Distribution of Missing Values by Country and Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Density plot of missing values by year\n",
    "sns.kdeplot(missing_by_year, shade=True)\n",
    "plt.title('Density Plot of Missing Values by Year')\n",
    "plt.xlabel('Number of Missing Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the CSV file as a Pandas DataFrame\n",
    "df = pd.read_csv('clean/nutrient2002.csv')\n",
    "print(df.shape)\n",
    "# List of excluded countries\n",
    "excluded_countries = ['Austria', 'Malta', 'Bulgaria', 'Slovakia', 'Netherlands','Latvia']\n",
    "# Keep only the excluded countries in the DataFrame\n",
    "df = df[df['Country'].isin(excluded_countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=cleanraw('raw/nutrient.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nutrient Data Set is Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a heatmap to visualize missing values\n",
    "sns.heatmap(df.isnull(), cmap='viridis')\n",
    "plt.title('Missing Values in Nutrient Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('raw/manure.csv')\n",
    "rawfiles = os.listdir('raw')\n",
    "rawfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the clean() function with a file path and assign the results to several df variables\n",
    "df = clean('raw/nutrient.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of global variables in your script \n",
    "globals_vars = list(globals().keys())\n",
    "\n",
    "# Filter the list to include only the dataframes created by the clean() function\n",
    "df_names = [var for var in globals_vars if isinstance(globals()[var], pd.DataFrame) and var.endswith('_df')]\n",
    "\n",
    "# Print the list of dataframe names\n",
    "print(df_names)\n",
    "df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a list of all the global variables in your script\n",
    "# global_vars = list(globals())\n",
    "\n",
    "# # Delete all dataframes created by the clean() function\n",
    "# for var in global_vars:\n",
    "#     if isinstance(globals()[var], pd.DataFrame) and var.endswith('_df'):\n",
    "#         del globals()[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [del globals()[var] for var in list(globals()) if isinstance(globals()[var], pd.DataFrame) and var.endswith('_df')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   df = pd.read_csv('raw/LivestockPatterns.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_nutrient_phosphate_p2o5_total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean('raw/LivestockPatterns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prep(file_path):\n",
    "#     # Read in the CSV file as a Pandas DataFrame\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     # Filter for years 2002 and later\n",
    "#     df = df[df['Year'] >= 2002]\n",
    "    \n",
    "#     # Remove excluded countries\n",
    "#     excluded_countries = ['Austria', 'Malta', 'Bulgaria', 'Slovakia', 'Netherlands', 'Latvia','Belgium-Luxembourg']\n",
    "#     df = df[~df['Area'].isin(excluded_countries)]\n",
    "    \n",
    "#     # Rename the 'Area' column to 'Country'\n",
    "#     df = df.rename(columns={'Area': 'Country'})\n",
    "    \n",
    "#     # Create a 'Key' column by concatenating 'Country' and 'Year'\n",
    "#     df['Key'] = df['Country'] + df['Year'].astype(str)\n",
    "    \n",
    "#     # Keep required columns\n",
    "#     df = df[['Country', 'Year', 'Element', 'Item', 'Value', 'Key']]\n",
    "    \n",
    "#     # Convert Item and Element columns to snake case\n",
    "#     df['Item'] = df['Item'].apply(snake)\n",
    "#     df['Element'] = df['Element'].apply(snake)\n",
    "    \n",
    "#     # Create dictionary of dataframes for each combination of Element and Item\n",
    "#     df_dict = {}\n",
    "#     for element, item in zip(df['Element'], df['Item']):\n",
    "#         element_item_key = f\"{element}||{item}\"\n",
    "#         if element_item_key not in df_dict:\n",
    "#             df_dict[element_item_key] = df[(df['Element'] == element) & (df['Item'] == item)].reset_index(drop=True)\n",
    "    \n",
    "#     return df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep('raw/manure.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# get the unique combinations of Element and Item\n",
    "combinations = df[['Element', 'Item']].drop_duplicates()\n",
    "\n",
    "# loop through all combinations of Element and Item\n",
    "for i, row in combinations.iterrows():\n",
    "    # get the current combination of Element and Item\n",
    "    e = row['Element']\n",
    "    it = row['Item']\n",
    "    \n",
    "    # create a DataFrame for the current combination\n",
    "    df_name = f\"{e}_{it}_df\"\n",
    "    globals()[df_name] = df[(df['Element'] == e) & (df['Item'] == it)].reset_index(drop=True)\n",
    "\n",
    "# print a list of the DataFrames created\n",
    "df_list = [var for var in globals() if var.endswith('_df')]\n",
    "df_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframes(file_path):\n",
    "    # Read in the CSV file as a Pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Get the unique combinations of Element and Item\n",
    "    combinations = df[['Element', 'Item']].drop_duplicates()\n",
    "\n",
    "    # Create a dictionary of dataframes for each combination of Element and Item\n",
    "    df_dict = {}\n",
    "    for i, row in combinations.iterrows():\n",
    "        e = row['Element']\n",
    "        it = row['Item']\n",
    "        key = f\"{e}_{it}\"\n",
    "        df_dict[key] = df[(df['Element'] == e) & (df['Item'] == it)]\n",
    "\n",
    "    return df_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes('raw/land_pasture.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap to visualize missing values\n",
    "sns.heatmap(df.isnull(), cmap='viridis')\n",
    "plt.title('Missing Values in Nutrient Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_nutrient_nitrogen_n_total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Define a function to create a new DataFrame for a given group\n",
    "# def create_df(group):\n",
    "#     element_name = group['Element'].iloc[0]\n",
    "#     item_name = group['Item'].iloc[0]\n",
    "#     new_df = group.copy()\n",
    "#     new_df['Element'] = element_name\n",
    "#     new_df['Item'] = item_name\n",
    "#     new_df.columns = [inflection.underscore(col) for col in new_df.columns] # rename columns to snake_case\n",
    "#     df_name = f'{inflection.underscore(element_name)}_{inflection.underscore(item_name)}'\n",
    "#     return df_name, new_df\n",
    "\n",
    "# # Group the original dataframe by Element and Item, and loop through the resulting groups to create new DataFrames\n",
    "# dfs = {}\n",
    "# for (element, item), group in df.groupby(['Element', 'Item']):\n",
    "#     df_name, new_df = create_df(group)\n",
    "#     dfs[df_name] = new_df\n",
    "\n",
    "# # Print the resulting DataFrames\n",
    "# for name, df in dfs.items():\n",
    "#     print(name)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by 'Element' and 'Item'\n",
    "groups = df.groupby(['Element', 'Item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by Element and Item\n",
    "grouped = nutrient_df.groupby(['Element', 'Item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in groups:\n",
    "    element, item = name\n",
    "    filename = f\"{element}_{item}.csv\"\n",
    "    group.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each group and create a new DataFrame for each\n",
    "for name, group in grouped:\n",
    "    # Create a new DataFrame using the group\n",
    "    new_df = pd.DataFrame(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the new DataFrames\n",
    "dfs = {}\n",
    "\n",
    "# Loop over each group and create a new DataFrame for each\n",
    "for name, group in grouped:\n",
    "    # Create a new DataFrame using the group\n",
    "    new_df = pd.DataFrame(group)\n",
    "    \n",
    "    # Get the names of the Element and Item for this group\n",
    "    element_name = name[0]\n",
    "    item_name = name[1]\n",
    "    \n",
    "    # Create a unique name for this new DataFrame\n",
    "    df_name = f'{element_name}_{item_name}_df'\n",
    "    \n",
    "    # Store the new DataFrame in the dictionary\n",
    "    dfs[df_name] = new_df\n",
    "\n",
    "dfs['Nitrogen_Fertilizers_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # Do EDA on the new DataFrame as desired\n",
    "    # ...\n",
    "    \n",
    "    # Store the new DataFrame however you like (e.g. in a dictionary)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by country and item and calculate the mean value\n",
    "df_grouped = dfs['Production_Crops_Economic'].groupby(['Area', 'Item']).mean()\n",
    "\n",
    "# Print the first 5 rows\n",
    "print(df_grouped.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We will bring in another predictor manually before automating\n",
    "The unwanted area and years are filtered out by the process Next  LandCover \"\"\"\n",
    "os.listdir('./raw') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictor variables to include in the analysis.\n",
    "-  production volume\n",
    "- imports\n",
    "- exports and \n",
    "- slaughter numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_data(file_path):\n",
    "    # Read in the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def clean_column_names(df):\n",
    "    # Replace forward slashes, backslashes, and commas in column names with underscores\n",
    "    df.columns = df.columns.str.replace('[/\\\\\\\\, (,) ,=  ]', '_')\n",
    "    return df\n",
    "\n",
    "def clean_element_item_values(df):\n",
    "    # Replace forward slashes, backslashes, commas, periods, and hyphens in Element and Item values with underscores\n",
    "    df['Element'] = df['Element'].str.replace('[/\\\\\\\\, (,) , =  ]', '_')\n",
    "    df['Item'] = df['Item'].str.replace('[/\\\\\\\\, (,), =   -\\.]', '_')\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_rows(df):\n",
    "    # Drop rows where Area is 'Belgium-Luxembourg'\n",
    "    df = df[df['Area'] != 'Belgium-Luxembourg']\n",
    "    return df\n",
    "\n",
    "def generate_key_column(df):\n",
    "    # Merge Area and Year columns into a new Key column\n",
    "    df['Key'] = df['Area'] + '_' + df['Year'].astype(str)\n",
    "    return df\n",
    "\n",
    "def filter_dataframes(df):\n",
    "    # Generate a list of dataframes, with each dataframe filtered by Element and Item\n",
    "    dfs = []\n",
    "    for element in df['Element'].unique():\n",
    "        for item in df['Item'].unique():\n",
    "            element_item_df = df[(df['Element'] == element) & (df['Item'] == item)]\n",
    "            if not element_item_df.empty:\n",
    "                # Replace spaces and decimal points with underscores in dataframe names\n",
    "                element_item_name = element.replace(' ', '_').replace('.', '_') + '_' + item.replace(' ', '_').replace('.', '_') + '_df'\n",
    "                dfs.append(element_item_name)\n",
    "                # Replace spaces and decimal points with underscores in dataframe variable names\n",
    "                exec(element_item_name.replace(' ', '_').replace('.', '_') + ' = element_item_df')\n",
    "\n",
    "    # Debugging line: print out the list of dataframe names\n",
    "    print(dfs)\n",
    "    \n",
    "    # Return a dictionary of the dataframes\n",
    "    return {name: globals()[name] for name in dfs}\n",
    "\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    df = read_data(file_path)\n",
    "    df = drop_rows(df)\n",
    "    df = clean_column_names(df)\n",
    "    df = clean_element_item_values(df)\n",
    "    return filter_dataframes(df)\n",
    "\n",
    "#     df = generate_key_column(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Group by element and item and calculate the mean value\n",
    "df_grouped = dfs['Production_Crops_Economic'].groupby(['Area', 'Item']).mean()\n",
    "\n",
    "# Print the first 5 rows\n",
    "print(df_grouped.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = preprocess_data('raw/nutrient.csv')\n",
    "print(dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yield_Carcass_Weight_Beef_and_Buffalo_Meat__primary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = preprocess_data('raw/beefandbuffalo.csv')\n",
    "print(dfs.keys())  # prints a list of the names of the processed dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yield_Carcass_Weight_Meat_of_cattle_with_the_bone__fresh_or_chilled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(file_path):\n",
    "    # Read in the data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Replace forward slashes, backslashes, and commas in column names with underscores\n",
    "    df.columns = df.columns.str.replace('[/\\\\\\\\, ]', '_')\n",
    "\n",
    "    # Replace forward slashes, backslashes, and commas in Element and Item values with underscores\n",
    "    df['Element'] = df['Element'].str.replace('[/\\\\\\\\, ]', '_')\n",
    "    df['Item'] = df['Item'].str.replace('[/\\\\\\\\, ]', '_')\n",
    "\n",
    "    # Drop rows where Area is 'Belgium-Luxembourg'\n",
    "    df = df[df['Area'] != 'Belgium-Luxembourg']\n",
    "\n",
    "    # Merge Area and Year columns into a new Key column\n",
    "    df['Key'] = df['Area'] + '_' + df['Year'].astype(str)\n",
    "\n",
    "    # Generate a list of dataframes, with each dataframe filtered by Element and Item\n",
    "    dfs = []\n",
    "    for element in df['Element'].unique():\n",
    "        for item in df['Item'].unique():\n",
    "            element_item_df = df[(df['Element'] == element) & (df['Item'] == item)]\n",
    "            if not element_item_df.empty:\n",
    "                # Replace spaces with underscores in dataframe names\n",
    "                element_item_name = element + '_' + item.replace(' ', '_') + '_df'\n",
    "                dfs.append(element_item_name)\n",
    "                # Replace spaces with underscores in dataframe variable names\n",
    "                exec(element_item_name.replace(' ', '_') + ' = element_item_df')\n",
    "\n",
    "    # Return a dictionary of the dataframes\n",
    "    return {name: globals()[name] for name in dfs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YiCaMe_df=Yield_Carcass_Weight_Meat_of_cattle_with_the_bone__fresh_or_chilled_df\n",
    "\n",
    "YiCaMe_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beefandbuffalo_df = pd.read_csv( 'raw/beefandbuffalo.csv', on_bad_lines='skip')   # loads Land USe Domain\n",
    "beefandbuffalo_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = create_dfs(beefandbuffalo_df)\n",
    "df_names = [name for name, _ in dfs]\n",
    "df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Producing_Animals_Slaughtered_Beef_and_Buffalo_Meat__primary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = group_by_element_item(beefandbuffalo_df)\n",
    "\n",
    "# loop over the list and print the name of each dataframe and its first few rows\n",
    "for i, df in enumerate(df_list):\n",
    "    print(f\"Dataframe {i}: {df.columns[0]} - {df.columns[1]}\")\n",
    "    print(df.head())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataframe 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate over the groups and create new dataframes\n",
    "for group_name, group_data in grouped_df:\n",
    "    # create a new dataframe with the group data\n",
    "    new_df = group_data.copy()\n",
    "\n",
    "    # append the new dataframe to the list\n",
    "    df_list.append(new_df)\n",
    "\n",
    "    # print the first few rows of the new dataframe\n",
    "    print(\"New dataframe created: \", group_name)\n",
    "    new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Production=df_list[1]\n",
    "Production\n",
    "# assuming the dataframe is called 'Production'\n",
    "unique_items = Production['Item'].unique()\n",
    "print(unique_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "grouped_df = beefandbuffalo_df.groupby('Element')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the groups and create new dataframes\n",
    "for group_name, group_data in grouped_df:\n",
    "    # create a new dataframe with the group data\n",
    "    new_df_name = group_name + \"_df\"\n",
    "    locals()[new_df_name] = group_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming the original dataframe is called 'df'\n",
    "grouped_df = df.groupby('Element')\n",
    "df_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # print the first few rows of the new dataframe\n",
    "    print(\"New dataframe created: \", new_df_name)\n",
    "    print(locals()[new_df_name].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filters(df):\n",
    "    # Delete the \"Belgium-Luxembourg\" column\n",
    "    if \"Belgium-Luxembourg\" in df.columns:\n",
    "        df = df.drop(\"Belgium-Luxembourg\", axis=1)\n",
    "\n",
    "    # Filter the \"Year\" column to only include years greater than or equal to 2000\n",
    "    df = df[df[\"Year\"] >= 2000]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of unique countries from the \"Area\" column\n",
    "countries = df['Area'].unique()\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# count the number of missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# print the result\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of areas with missing values\n",
    "num_missing_areas = df['Area'][df.isna().any(axis=1)].nunique()\n",
    "\n",
    "# Print the result\n",
    "print(\"The number of areas with missing values is:\", num_missing_areas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "31900/1708 # 20% data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake('Distribution of Missing Values by Area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake('msno.matrix(df)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a bar chart of the missing value counts by year\n",
    "plt.figure(figsize=(20,8))  # increase the figure size for better readability\n",
    "ax = year_counts.plot(kind='bar')\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Number of Missing Values', fontsize=14)\n",
    "plt.title('Distribution of Missing Values by Year', fontsize=18)\n",
    "plt.xticks(rotation=0, fontsize=12)  # rotate x-axis labels to 0 degrees\n",
    "ax.tick_params(axis='y', labelsize=12)  # adjust y-axis label size\n",
    "ax.tick_params(axis='x', pad=10)  # adjust x-axis tick padding\n",
    "\n",
    "# add value labels to the bars\n",
    "for i, v in enumerate(year_counts):\n",
    "    ax.text(i, v, str(v), ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "# set year labels vertically\n",
    "ax.set_xticklabels(year_counts.index, rotation=90)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "A new dataframe with counts of missing values \n",
    "for each country was sorted in descending order\n",
    "revealing  the top 10 countries\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create a new dataframe to hold the counts of missing values by country\n",
    "country_counts = df.isnull().sum(axis=1).groupby(df.Area).sum().sort_values(ascending=False)\n",
    "\n",
    "# get the top 10 countries with the most missing values\n",
    "top_10_countries = country_counts.head(10)\n",
    "\n",
    "# print the list of top 10 countries\n",
    "print(top_10_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe to hold the counts of missing values by country\n",
    "country_counts = df.isnull().sum(axis=1).groupby(df.Area).sum().sort_values(ascending=True)\n",
    "\n",
    "# get the list of countries with any missing values\n",
    "missing_countries = country_counts[country_counts > 0].index\n",
    "\n",
    "# print the list of countries with missing values\n",
    "print(\"Countries with missing values:\\n\", missing_countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe to hold the counts of missing values by country\n",
    "country_counts = df.isnull().sum(axis=1).groupby(df.Area).sum().sort_values(ascending=True)\n",
    "\n",
    "# get the list of countries with any missing values\n",
    "missing_countries = country_counts[country_counts > 0].index\n",
    "\n",
    "# print the list of countries with missing values\n",
    "print(\"Countries with missing values:\\n\", missing_countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe to hold the counts of missing values by country\n",
    "country_counts = df.isnull().sum(axis=1).groupby(df.Area).sum().sort_values(ascending=True)\n",
    "\n",
    "# get the list of countries with any missing values\n",
    "missing_countries = country_counts[country_counts > 0]\n",
    "\n",
    "# create a new dataframe with the missing value counts and the total number of observations for each country\n",
    "mv_counts = pd.concat([missing_countries, df.groupby('Area').size()], axis=1)\n",
    "mv_counts.columns = ['Missing Values', 'Total Observations']\n",
    "\n",
    "# calculate the proportion of missing values for each country\n",
    "mv_counts['% Missing'] = mv_counts['Missing Values'] / mv_counts['Total Observations'] * 100\n",
    "\n",
    "# sort the dataframe by the proportion of missing values in descending order\n",
    "mv_counts = mv_counts.sort_values('% Missing', ascending=False)\n",
    "\n",
    "# display the table\n",
    "print(mv_counts.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# group the data by Area and compute the total count of missing values for each group\n",
    "area_counts = df.isnull().sum(axis=1).groupby(df.Area).sum()\n",
    "\n",
    "# create a bar chart of the missing value counts by Area\n",
    "plt.figure(figsize=(10,5))\n",
    "ax = area_counts.plot(kind='bar')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Number of Missing Values')\n",
    "plt.title('Distribution of Missing Values by Area')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# add value labels to the bars\n",
    "for i, v in enumerate(area_counts):\n",
    "    ax.text(i, v, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_describtions = df['Flag Description'].unique()\n",
    "print(unique_describtions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[df['Year'] > 1999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Key'] = df['Area'] + '_' + df['Year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# count the frequency of NaN and 'Official figure' values in the 'Flag Description' column\n",
    "flag_counts = df['Flag Description'].value_counts(dropna=False)\n",
    "\n",
    "# plot a pie chart of the flag counts\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(flag_counts, labels=flag_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Flag Description Frequencies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This can be md in final version\n",
    "####  Pascal\n",
    "The 'pascal' naming function takes in a string as\n",
    "input and converts it to a PascalCase format. \n",
    "PascalCase is a naming convention where \n",
    "the first letter of each word is capitalized, and there \n",
    "are no spaces or separators between the words.\n",
    "\"\"\"\n",
    "\n",
    "def pascal(string):\n",
    "    \"\"\"\n",
    "    Convert a space- or snake-separated string to PascalCase.\n",
    "\n",
    "    Parameters:\n",
    "        string (str): The input string to convert to PascalCase.\n",
    "\n",
    "    Returns:\n",
    "        str: The input string in PascalCase format.\n",
    "\n",
    "    \"\"\"\n",
    "    # Replace any underscores with spaces\n",
    "    string = string.replace(\"_\", \" \")\n",
    "    # Capitalize the first letter of each word\n",
    "    words = string.title()\n",
    "    # Remove any remaining spaces\n",
    "    words = words.replace(\" \", \"\")\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#### Camel\n",
    "\n",
    "Camel case is a naming convention in which each word in a compound word is capitalized, except for the first word which is in lower case. It is commonly used in programming languages for naming variables and functions.\n",
    "\n",
    "handles both snake_case and space-separated strings\n",
    "\"\"\"\n",
    "def camel(string):\n",
    "    \"\"\"\n",
    "    Convert a space-separated or snake_case string to camelCase.\n",
    "\n",
    "    Parameters:\n",
    "        string (str): The string to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: The converted string in camelCase.\n",
    "    \"\"\"\n",
    "    # Replace underscores with spaces and split the string into a list of words\n",
    "    words = string.replace(\"_\", \" \").split()\n",
    "    # Convert the first word to lowercase and capitalize all subsequent words\n",
    "    camel_cased = [words[0].lower()] + [word.capitalize() for word in words[1:]]\n",
    "    # Concatenate the words together and return the resulting string\n",
    "    return \"    \" + ''.join(camel_cased) + \"    \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  tabler\n",
    "\n",
    "\"\"\"The code defines a function tabler that generates an HTML table with information on the files in a given folder and\n",
    "adds a section header that contains the directory path \n",
    "\"\"\"\n",
    "\n",
    "def tabler(folder_path):\n",
    "    \"\"\"\n",
    "    Generate an HTML table with information on files in a given folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path to the folder containing the files.\n",
    "\n",
    "    Returns:\n",
    "    - str: The HTML code for the table.\n",
    "    \"\"\"\n",
    "    # Get the contents of the folder\n",
    "    try:\n",
    "        contents = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        return \"Directory not found\"\n",
    "    except OSError:\n",
    "        return \"Invalid folder path\"\n",
    "\n",
    "    # Create the section header\n",
    "    header = f\"### {folder_path}\\n\\n\"\n",
    "\n",
    "    # Create the table header\n",
    "    table = '<table style=\"font-size:100%\"><thead><tr><th>File Name</th><th>Size</th><th>Modified Time</th></tr></thead><tbody>'\n",
    "\n",
    "    # Add a row for each file\n",
    "    for item in contents:\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            # Get the file size and modified time\n",
    "            size_bytes = os.path.getsize(item_path)\n",
    "            size_kb = size_bytes / 1024\n",
    "            size_str = '{:,.2f} KB'.format(size_kb)\n",
    "            modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path)).strftime('%Y-%m-%d %H:%M')\n",
    "            # Add a row to the table\n",
    "            table += '<tr><td>{}</td><td>{}</td><td>{}</td></tr>'.format(item, size_str, modified_time)\n",
    "\n",
    "    # Close the table\n",
    "    table += '</tbody></table>'\n",
    "\n",
    "    return header + table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "1012.78px",
    "width": "507.775px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents (Clickable in sidebar)",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "1166.7px",
    "left": "1576px",
    "top": "1389.19px",
    "width": "497px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "890563eb1401dd7c5eac482b2070a231034cb0eabe59bf1a3eb86f9e36919f52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
